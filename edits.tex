\usepackage[table]{xcolor}
\definecolor{mygreen}{RGB}{0, 90, 0}
% \definecolor{purple}{RGB}{128,0,128}
\newcommand\original[1]{\textcolor{red}{#1}}
\newcommand\TODO[1]{\textcolor{red}{[TODO: #1]}}

%%%%%% EDITING MACROS %%%%%%%%%

\newcommand\new[1]{\textcolor{blue}{#1}}
\newcommand\old[1]{\sout{#1}}
% orange for EI
\definecolor{orange}{rgb}{1,0.5,0}
\newcommand\ei[2]{\sout{#1} \textcolor{orange}{#2}}
\newcommand\eic[1]{\textcolor{orange}{[#1]}}
% green for JW
% \definecolor{green}{rgb}{0,0.5,0}
\newcommand\jw[2]{\sout{#1} \textcolor{purple}{#2}}
\newcommand\jwc[1]{\textcolor{purple}{[#1]}}
% purple for JJ
\definecolor{purple}{rgb}{0.5,0,1}
\newcommand\jj[2]{\sout{#1} \textcolor{purple}{#2}}
\newcommand\jjc[1]{\textcolor{purple}{[Josh: #1]}}
% cyan for AR
\definecolor{cyan}{rgb}{0,.5,.5}
\newcommand\ar[2]{\sout{#1} \textcolor{cyan}{#2}}
\newcommand\arc[1]{\textcolor{cyan}{#1}}
% light brown for KT
\definecolor{lightbrown}{rgb}{0.5,0.5,0}
\newcommand\kt[2]{\sout{#1} \textcolor{lightbrown}{#2}}
\newcommand\ktc[1]{\textcolor{lightbrown}{#1}}

\newcommand\editModelingGuidanceEd{Many guidelines have been proposed regarding the use of mechanistic models to inform policy, reviewed by Behrend et al.~\cite{behrend20}. The recommendations of \cite{behrend20} generally align with other manifestos on the proper use of models for informing policy \cite{saltelli20,donnelly18}, each emphasizing the importance of involving stakeholders in the modeling process, transparency, and communication of uncertainty. Other recommendations focus on issues of model calibration and assessment \cite{dahabreh17,egger17,penaloza15}.  Based on our literature review, \cite{lee20} was among the studies that best followed these general recommendations for policy-driven modeling. Concretly, \cite{lee20} follows four of the five principles outlined in \cite{behrend20}: \emph{complete model documentation}, \emph{complete description of data used}, \emph{communicating uncertainty}, and \emph{testable model outcomes}. Determining the level of adherence to the first principle, \emph{stakeholder engagement}, is difficult based solely on the article. Despite this, the inconsistency between their forecasts and the cholera incidence from 2019 to 2022 suggests that existing recommendations and standards related to policy-driven modeling may be insufficient. \cite{saltelli19} suggests that improvements in model model-based outputs may be obtained by developing structures and standards based on statistical principles. Our general recommendations therefore complement and extend existing guidelines by focusing on the methodological tasks of calibrating and evaluating dynamic models in a rigorous statistical framework. We specifically emphasize principles that proved relevant to our case study; complementary methodological suggestions arising from a spatio-temporal analysis of COVID-19 are given in \cite{li23}.}

\newcommand\editModelingGuidance{Numerous guidelines have been proposed for using mechanistic models to inform policy, reviewed in \cite{behrend20}. Behrend et al.~\cite{behrend20} identify the importance of stakeholder engagement, transparency, reproducibility, uncertainty communication, and testable model outcomes. These and related principles are echoed by other influential articles \cite{saltelli20,donnelly18}. Additional literature emphasizes model calibration and evaluation techniques \cite{dahabreh17,egger17,penaloza15}. These guidelines often lack implementation specifics. As an example, \cite{lee20} largely adhere to the principles of \cite{behrend20}---though assessing the extent of stake-holder engagement is challenging---yet their projections are inconsistent with actual cholera incidence data from 2019 to 2022, demonstrating the limitations of current standards. We provide methodology for rigorous statistical calibration and evaluation of dynamic models (as advocated by \cite{saltelli19}), thereby expanding on the prevailing guidance. We specifically emphasize principles that prove essential in our case study. Complementary methodological suggestions arising from a spatio-temporal analysis of COVID-19 are detailed in \cite{li23}.}

\newcommand\editMechModels{Mechanistic models representing biological phenomena are valuable for epidemiology and consequently for public health policy \cite{lofgren14,mccabe21}. More broadly, they have useful roles throughout biology, expecially when combined with statistical methods that properly account for stochasticity and nonlinearity \cite{may04}. In some situations, modern machine learning methods can outperform mechanistic models on epidemiological forecasting tasks \cite{lau22,baker18}. The predictive skill of non-mechansitic models can reveal limitations in mechanistic models, but cannot readily replace the scientific understanding obtained by describing the biological dynamics of the system in a mathematical model \cite{baker18,prosperi20}.}

% \newcommand\editDiscussionOne{Model diagnostics are instrumental in enhancing model fits and identifying limitations of models that cannot readily be fixed. In our case study, we compared the Models~1--3 of \cite{lee20} to simple statistical benchmarks, enabling us to identify deficiencies in the models that could be improved. In the case of Model~3, comparing a fitted model to a benchmark and then following up by questioning why the model did not describe the data as efficiently as a simple statistical model helped us discern that the model fell short in explaining the surge in cholera transmission following Hurricane Matthew. This led us to suggest a model modification that enchanced model fit. When a mechanistic model beats a statistical benchmark, we can begin to critically evalutate the causal implications of the fitted model, carefully acknowledging the fact that beating a simple benchmark alone does not imply that the model is both a useful quantitative and qualitative description of the data. In Model~1, we found that the flexible seasonality term closely mirrors the seasonal rainfall patterns in Haiti, reaffirming alternative evidence of the importance of rainfall as a driver of cholera infection and increasing our confidence in Model~1's ability to capture the critical dynamics of the cholera outbreak.}
% 
\newcommand\editDiscussionOne{Model diagnostics are instrumental for improving model fit and exposing unresolvable limitations. In our case study, we compared the three models from Lee et al. (2020) to statistical benchmarks, revealing areas for enhancement. For example, comparisons of Model~3 to a benchmark revealed its inadequacy in accounting for the post-hurricane increase in transmission, leading to a beneficial model refinement. When a mechanistic model beats a statistical benchmark, we can begin to critically evalutate causal implications, carefully acknowledging that beating a simple benchmark alone does not imply the model is a useful qualitative description of the data. The recalibrated version of Model~1 outperforms its benchmark, enabling evaluation of causal implications. When doing so, we find that the fitted model provides a causal description of the dynamic system that is consitent with known features of the system, such as the importance of rainfall as a driver of cholera infection. The congruency between causal implications of the model and our belief about the dynamic system, coupled with a strong quantitative description of obsered data relative to a benchmark, increases our confidence that the model adequately captures critical dynamics of the system.}

% \newcommand\editDiscussionTwo{Likelihood based inference enables researchers to choose between any model that is evaluated on the same dataset using a metric such as AIC. Nested model variations are particularly useful as they enable formal statistical testing of the nested features. The cholera case study demonstrated the advantage of such a model variation in all three models. For instance, we tested the inclusion of a trend in transmission rate over time in Model~1; in Model~2, we considered adding an additional phase parameter, which enables a shift in the seasonal peaks of cholera infections; in Model~3, we included additional parameters such as those associated with Hurricane Matthew. In all these cases, we found strong statistical evidence in favor of these model variations over their simpler counterparts.}


% \newcommand\editDiscussionThree{Another important consideration when fitting a mechanistic model to a dynamic system is the complexity of the model. Mathematical models necessarily simplify complex dynamical systems in order to make inference possible. Decisions about which features of a system to incorporate in a model and which should be excluded can significantly influence the model-based conclusions. However, these decisions can be tested using a likelihood-based framework. Standard model decisions cover deterministic versus stochastic modeling and the decision to include or exclude a meta-population structure. Model benchmarks can assist in determining the level of complexity to be used in the model, enabling researchers to gauge a model's relative performance against a better-understood statistical model.}
% 
\newcommand\editDiscussionTwo{When fitting a mechanistic model to a dynamic system, the complexity of the model warrants careful attention. Mathematical models simplify complex dynamical systems, highlighting essential features that both facilitate scientific understanding and enable inference. In this simplification process, careful consideration must be given to which facets of the system are incorporated into the model and which are omitted, as these choices can shape the conclusions drawn from the model. These modeling decisions are driven both by the need for tractable inference and by the pursuit of scientific parsimony. Standard modeling choices—such as adopting a deterministic or stochastic approach, or incorporating a meta-population structure—reflect the balance between realism and simplicity.} %Benchmarking models against established and simpler statistical models in a likelihood based framework can help researchers determine an appropriate level of complexity, ensuring that models are both manageable and meaningful in their portrayal of the underlying dynamics.}

\newcommand\editDiscussionThree{Along with enabling comparisons to statistical benchmarks, likelihood-based methods aid in determining an appropriate level of model complexity. Models fit to the same data can be compared using a criteria such as AIC. Nested model variations are particularly useful as they enable formal statistical testing of the nested features. We illustrated the benefits of examining nested model features, enhancing all three models. Model~1 included a time-varying transmission rate; Model~2 assessed a phase-shift parameter in seasonal cholera peaks; Model~3 incorporated hurricane-related parameters. Each addition received strong statistical support over their simpler predecessors.}

\newcommand\editDiscussionFour{Unmodeled features of a dynamic system can lead to spurious or misleading parameter estimates if the feature greatly impacts observed data. This is because the model must compensate for these unaccounted features using its inherent flexibility. In deterministic models, parameter estimation is the sole source of such flexibility. When the influence of unmodeled dynamics is substantial, it can skew parameter estimates toward scientifically implausible values, a phenomenon observed in the estimated parameter values of Model~2. Incorporating demographic and environmental stochasticity into models can mitigate the impact of these unmodeled features. These stochastic phenomena are not only arguably present in biological systems, but their inclusion in a model also allows observed data variations to be attributed to inherent randomness rather than to distorted parameter values. Incorporating extra-demographic stochasticity in a model has also been shown to be beneficial \cite{he10,stocks20,li23}, which is consistent with our results: Models~1 and 3 suggest the presence of extra-demographic stochsticity, as evidenced by the confidence intervals provided in Suplement~S8.}

\newcommand\editDiscussionFive{The three models in this case study consider a range of possible model complexity. Of these models, Model~3 is distinct in that it is both stochastic and has a meta-population structure, making it challenging to draw likelihood-based inferences. In this paper, we demonstrated how this model class could be calibrated to incidence data using the innovative IBPF algorithm. One of only a few examples of fitting a model with this level of complexity via maximum likelihood, this case study exemplifies the algorithm's potential benefits and provides an example for future researchers on a possible approach to fitting a high-dimensional non-linear model.}

\newcommand\editDiscussionSix{If forecasts are an important component of a modeling task, the forecasts should be consistent with the available data, particularly at the most recently available time points. In our case study, we do this by simulating forward from the filtering distribution, as this proceedure conditions latent variables on the available data. This type of forecasting, however, is not possible using a deterministic model, where only initial conditions and parameter values can influence forecasts. Deterministic simulations may also result in over-confidence in model forecasts, as they can only account for uncertainty due to the parameter estimation proceedure \cite{king15}. Despite their limitations, deterministic models can offer valuable insights into dynamic systems. As with all models, researchers should consider and acknoweledge these limitations when applying deterministic models and interpreting their outcomes.}

\newcommand\editModVacc{Because Model~1 only accounts for national level disease dynamics, the pre-determined department-specific vaccination campaigns are carried out by assuming the vaccines are administered in one week to the same number of individuals that would have obtained vaccines if explicitly administered to the specific departments. We refer readers to \cite{lee20} and the accompanying supplement material for more details.}

\newcommand\editForecastOne{Forecasts of a dynamic system should should be consistent with the available data. It is particularly important that forecasts are consistent with the most recent information available, as recent data is likely to be more relevant than older data. While this assertion may seem self-evident, it is not the case for deterministic models, for which the initial conditions together with the parameters are sufficient for forecasting, and so recent data may not be consistent with model trajectories. Epidemiological forecasts based on deterministic models are not uncommon in practice, despite their limitations \cite{king15}. Lee et al.~\cite{lee20} chose to obtain forecasts from all of their models by simulating forward from initial conditions, rather than conditioning forecasts based on the available data.
This decision is possibly as a result of using a deterministic model, as forecasts from different models may only be considered comparable if they are obtained in the same way, which must be by simulating from initial conditions because Model~2 is deterministic.}

\newcommand\editForecastTwo{In contrast, for non-deterministic Models~1 and 3, we obtain forecasts by simulating future values using latent states that are harmonious with the most recent data. This is done by drawing latent states at the last observation time ($t_N$) from the filtering distribution $f_{\bm{X}_N|\bm{Y}_{1:N}}(\bm{x}_{N} | \bm{y}^*_{1:N} ; \hat\paramVec)$. The decision to obtain model forecasts from initial conditions partially explains the unsuccessful forecasts of Lee et al.~\cite{lee20}. Table~S7 in their supplement material, which contains results that were not discussed in their main article, shows that the subset of their simulations which were consistent with observing zero cases from 2019-2020 were also more consistent with the disappearance of cholera from Haiti from 2019-2022. These results support our argument that forecasts should be made by ensuring the starting point for the forecast is consistent with available data.}

\newcommand\editMetaPop{In our literature review, 17 articles considered dynamic models that incorporate spatial heterogeneity \cite{lee20,tuite11,pasetto18,fitzgibbon20,eisenberg13,rinaldo12,chao11,abrams13,trevisin22,sallah17,collins14,kelly16,azman12,leung22,kuhn14,mari15,gatto12}. All but four \cite{lee20,pasetto18,sallah17,azman12} of these studies used deterministic dynamic models: this greatly simplifies the process of calibrating model parameters to incidence data, though deterministic models can struggle to describe complex stochastic dynamics. The model in \cite{pasetto18} was fit using an Ensemble Kalman Filter (EnKF) \cite{evensen09}; though EnKF scales favorably with the number of spatial units, it relies on linearization of latent states which can be problematic for highly nonlinear systems \cite{evensen22,ionides21}. Alternative approaches used to fit stochastic models included making additional simplifying assumptions to aid in the fitting process \cite{lee20}, and using MCMC algorithms \cite{sallah17,azman12} which require specific structures in the latent dynamics, making these algorithms non plug-and-play. In this subsection, we present how the recently developed iterated block particle filter (IBPF) algorithm \cite{ning21ibpf,ionides22} can be used to fit a spatially explicit stochastic dynamic model to incidence data.}

\newcommand\editSpatBench{To obtain a benchmark for models with a meta-population structure, we fit independent auto-regressive negative binomial models to each spatial unit. Under the assumption of independence, the log-likelihood of the benchmark on the entire collection of data can be obtained by summing up the log-likelihood for each independent model. In general, a spatially explicit model may not have well-defined individual log-likelihoods, and, in this case, comparisons to benchmarks must be made at the level of the joint model.}

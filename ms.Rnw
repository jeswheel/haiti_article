%% Template for the submission to:
%%   The Annals of Applied Statistics [AOAS]
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aoas]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb,graphicx,enumerate,url,xr,lmodern}
\RequirePackage[authoryear]{natbib}
\usepackage{url}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage[normalem]{ulem}% to use \sout in feedback commands
\usepackage{bm}
\usepackage[mathscr]{euscript}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{remark}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% parameters %%%%%%%%%%%
\newcommand\Wsat{W_{\mathrm{sat}}}
\newcommand\muIR{\mu_{IR}}
\newcommand\muEI{\mu_{EI}}
\newcommand\transmission{\beta}
\newcommand\seasAmplitude{a}
\newcommand\rainfallExponent{r}
\newcommand\muRS{\mu_{RS}}
\newcommand\vaccineEfficacy{\theta}
\newcommand\muBirth{\mu_S}
\newcommand\muDeath{\delta}
\newcommand\choleraDeath{\delta_{C}}
\newcommand\symptomFrac{f}
\newcommand\asymptomRelativeInfect{\epsilon}
\newcommand\asymptomRelativeShed{\epsilon_{W}}
\newcommand\Wbeta[1]{\beta_{W#1}}
\newcommand\Wremoval{\delta_W}
\newcommand\Wshed{\mu_W}
\newcommand\mixExponent{\nu}
\newcommand\sigmaProc{\sigma_{\mathrm{proc}}}
\newcommand\reportRate{\rho}
\newcommand\obsOverdispersion{\psi}
\newcommand\phaseParm{\phi}
\newcommand\transmissionTrend{\zeta}
\newcommand\vaccClass{Z}
\newcommand\vaccCounter{z}
\newcommand\modelCounter{m}
\newcommand\missing{---}
\newcommand\fixed{\color{blue}}
\newcommand\demography{D}
\newcommand\code[1]{\texttt{#1}}
\newcommand\paramVec{\theta}

\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\varv}{\mathord}{matha}{118}
\newcommand\myeqref[1]{(\ref{#1})}
\newcommand{\blind}{1}

%% customized math macros
\newcommand\seq[2]{{#1}\!:\!{#2}}
\newcommand\R{\mathbb{R}}
\newcommand\Var{\mathrm{Var}}
\newcommand\var{\Var}
\newcommand\Cov{\mathrm{Cov}}
\newcommand\cov{\Cov}
\newcommand\iid{\mathrm{iid}}
\newcommand\dist{d}
\def\lik{L}
\def\loglik{\ell}

%%%%%% EDITING MACROS %%%%%%%%%
% orange for EI
\definecolor{orange}{rgb}{1,0.5,0}
\newcommand\ei[2]{\sout{#1} \textcolor{orange}{#2}}
\newcommand\eic[1]{\textcolor{orange}{[#1]}}
% green for JW
\definecolor{green}{rgb}{0,0.5,0}
\newcommand\jw[2]{\sout{#1} \textcolor{green}{#2}}
\newcommand\jwc[1]{\textcolor{green}{[#1]}}
% purple for JJ
\definecolor{purple}{rgb}{0.5,0,1}
\newcommand\jj[2]{\sout{#1} \textcolor{purple}{#2}}
\newcommand\jjc[1]{\textcolor{purple}{[#1]}}
% cyan for AR
\definecolor{cyan}{rgb}{0,.5,.5}
\newcommand\ar[2]{\sout{#1} \textcolor{cyan}{#2}}
\newcommand\arc[1]{\textcolor{cyan}{#1}}
% light brown for KT
\definecolor{lightbrown}{rgb}{0.5,0.5,0}
\newcommand\kt[2]{\sout{#1} \textcolor{lightbrown}{#2}}
\newcommand\ktc[1]{\textcolor{lightbrown}{#1}}

\endlocaldefs

\begin{document}

<<Setup, include=FALSE,echo=FALSE,results='hide'>>=
library(knitr)
library(pomp)
library(panelPomp)
library(spatPomp)
library(doParallel)
library(doRNG)
library(haitipkg)
library(tidyverse)

RUN_LEVEL <- 1
rl_dir <- paste0("run_level_", RUN_LEVEL, "/")

for (i in 1:3) {
  if (!dir.exists(paste0("model", i, '/', rl_dir))) {
    dir.create(paste0("model", i, '/', rl_dir), recursive = TRUE)
  }
}

opts_knit$set(concordance=TRUE)
opts_chunk$set(
    progress = TRUE,
    concordance = TRUE,
    prompt = TRUE,
    highlight = FALSE,
    tidy = TRUE,
    tidy.opts = list(
        keep.blank.line = FALSE
    ),
    comment = "",
    warning = FALSE,
    message = FALSE,
    error = TRUE,
    echo = FALSE,
    cache = FALSE,
    strip.white = TRUE,
    # results="markup",
    background = "#FFFFFF00",
    size = "normalsize",
    fig.path = "figure/",
    fig.lp = "fig:",
    fig.align = "left",
    fig.show = "asis",
    dev = "pdf",
    dev.args = list(
        bg = "transparent",
        pointsize = 9
    )
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5)))

# 40 cores for doob, 8 for ito
doob_cores <- 40
gl_cores <- 36

cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)

dep_labeller <- as_labeller(
  c(
    'Artibonite' = 'Artibonite',
    'Sud_Est' = 'Sud-Est',
    'Sud.Est' = 'Sud-Est',
    'Nippes' = 'Nippes',
    'Nord_Est' = 'Nord-Est',
    'Nord.Est' = 'Nord-Est',
    'Ouest' = 'Ouest',
    'Centre' = 'Centre',
    'Nord' = 'Nord',
    'Sud' = 'Sud',
    'Nord_Ouest' = 'Nord-Ouest',
    'Nord.Ouest' = 'Nord-Ouest',
    'Grande_Anse' = 'Grand\'Anse',
    'Grand.Anse' = 'Grand\'Anse'
  )
)

options(
  scipen = 0,
  help_type = "html",
  stringsAsFactors = FALSE,
  # prompt="R> ",
  continue = "+  ",
  width = 70,
  useFancyQuotes = FALSE,
  reindent.spaces = 2,
  xtable.comment = FALSE
)

dep_plot_df <- haitiCholera %>%
  select(-report) %>%
  pivot_longer(
    data = .,
    cols = -date_saturday,
    values_to = 'cases',
    names_to = 'dep'
  ) %>%
  mutate(
    date = as.Date(date_saturday),
    dep = gsub("\\.", "_", dep)
  ) %>%
  mutate(
    dep = case_when(dep == "Grand_Anse" ~ "Grande_Anse", TRUE ~ dep)
  )

true_agg_cases <- dep_plot_df %>%
  tidyr::pivot_wider(
    data = .,
    id_cols = c(date),
    names_from = dep,
    values_from = cases,
    names_prefix = 'cases_'
  ) %>%
  mutate(
    ReportedAll = cases_Artibonite + cases_Centre +
      cases_Grande_Anse + cases_Nippes + cases_Nord +
      cases_Nord_Est + cases_Ouest + cases_Sud +
      cases_Sud_Est + cases_Nord_Ouest
  )


# TODO: Remove these functions and replace with lubridate functions (also in the package).
dateToYears <- function(date, origin = as.Date("2014-01-01"), yr_offset = 2014) {
  # This function converts a date to a decimal representation
  #
  # ex: "1976-03-01" -> 1976.163

  julian(date, origin = origin) / 365.25 + yr_offset
}

yearsToDate <- function(year_frac, origin = as.Date("2014-01-01"), yr_offset = 2014.0) {
  # This function is the inverse function of dateToYears; it takes
  # a decimal representation of a date and converts it into a Date.
  #
  # ex: 1976.163 -> "1976-03-01"

  as.Date((year_frac - yr_offset) * 365.25, origin = origin)
}

yearsToDateTime <- function(year_frac, origin = as.Date("2014-01-01"), yr_offset = 2014.0) {
  # Same as the function above, but a DateTime object rather than a Date
  # object.
  #
  # ex: 1976.163 -> "1976-03-01"
  as.POSIXct((year_frac - yr_offset) * 365.25 * 3600 * 24, origin = origin)
}

@

<<ReplicateModel1, echo=FALSE, message=FALSE, include=FALSE>>=
# In this chunk, we replicate the results in Lee et al for model 1.
# We do this replication here in the ms.Rnw file because we need to get
# estimates for the likelihood of the models used by lee et al. The chunk has
# the following steps:
#   (1) Initialize some starting values close to those used in lee et al
#   (2) do small MIF, similar to what was done in lee et al, to get a fit for
#       epidemic period.
#   (3) Simulate from model
#   (4) Use simulations to initialize endemic phase of the model, and
#       perform MIF, similar to what lee et al did for this phase
#   (5) Simulate from model
#   (6) Calculate the likelihood of each possible set of parameters.

N_PARAMS   <- switch(RUN_LEVEL,  10,  20,  300)
N_LEE_SIMS <- switch(RUN_LEVEL,  10,  20,   20)
NMIF       <- switch(RUN_LEVEL,   5,  20,   50)
NP_MIF     <- switch(RUN_LEVEL,  20,  50,  100)  # Too few, but matches Lee et al
NP_EVAL    <- switch(RUN_LEVEL,  50, 200, 2000)
NREPS_EVAL <- switch(RUN_LEVEL,   3,   5,   36)

#-------------------------- Step 1 ----------------------

set.seed(141837)
sample_beta_nu <- function(mu_beta = 5.8, mu_nu = 0.96, n_params = N_PARAMS) {
  # This function is used to sample starting values for beta and nu.
  #
  # params:
  #   mu_beta: Average of the marginal distribution for beta1. Value was fit visually.
  #   mu_nu: Average of the marginal distribution for nu. Value was fit visually.
  # returns:
  #   data.frame of size 300x2, values of beta1 and nu with truncated bivariate
  #   normal relation.

  # Determine a good value for the marginal standard deviations
  sd_beta <- (12.1-mu_beta) / qnorm(0.995)
  sd_nu <- (0.9-mu_nu) / qnorm(0.005)

  # Create covariance matrix for the bivariate-normal distribution
  cov_mat <- rbind(
    c(sd_beta^2, -0.7 * sd_beta * sd_nu),
    c(-0.7 * sd_beta * sd_nu, sd_nu^2)
  )

  # Sample from the bivariate normal distribution
  X <- MASS::mvrnorm(n = n_params, mu = c(mu_beta, mu_nu), Sigma = cov_mat)
  colnames(X) <- c("beta1", "nu")

  # Identify values that fell outside of range, so they can be resampled
  bad_beta <- which(X[, 'beta1'] > 12.1 | X[, 'beta1'] < 0.04)
  bad_nu <- which(X[, 'nu'] > 1 | X[, 'nu'] < 0.9)
  n_resamp <- length(unique(c(bad_beta, bad_nu)))

  # Keep resampling values until all values fall within the range.
  while (n_resamp > 0) {
    to_resamp <- unique(c(bad_beta, bad_nu))
    X[to_resamp, ] <- MASS::mvrnorm(n = n_resamp, mu = c(mu_beta, mu_nu), Sigma = cov_mat)

    bad_beta <- which(X[, 'beta1'] > 12.1 | X[, 'beta1'] < 0.04)
    bad_nu <- which(X[, 'nu'] > 1 | X[, 'nu'] < 0.9)
    n_resamp <- length(unique(c(bad_beta, bad_nu)))
  }

  # Return results as a data.frame
  as.data.frame(X)
}
# Run the function to get values of beta1 and nu
starting_params <- sample_beta_nu()

sample_rho <- function(beta, n_params = N_PARAMS) {
  # This function is used to get starting values for the parameter rho, based
  # on it's strong correlation with beta1 and nu. Here, we first sample beta1
  # and nu, and then sample rho based on a regression on beta1. Additionally,
  # it is noted that rho has a bi-modal distribution, with lower values of rho
  # corresponding with high probability to higher values of beta1.
  #
  # params:
  #   beta: previously sampled values of beta1 that will be used to get values
  #         of rho.
  # returns:
  #   vector of length 300 that have an appropriate relationship with the parameter
  #   beta1.

  rhos <- rep(0, N_PARAMS)

  # Get slope and intercept for the regression
  slope <- (0.4/(5.1 - 6.75))
  intercpt <- -6.75 * slope

  # Determine whice mode should the value of rho belong to, based on regression
  case_when(
    beta > 6.75 ~ 0,  # Large beta --> group "0"
    # Small beta --> more likely to be group "1", with increasing probability as beta gets smaller
    beta <= 6.75 & beta > (0.95 - intercpt) / slope ~ slope * beta + intercpt,
    TRUE ~ 0.95
  ) -> probs

  # Sample the groups that rho belongs to
  rho_group1 <- rbernoulli(N_PARAMS, probs)
  rho_group2 <- !rho_group1

  # Keep track of how many values are in each group, used later for resampling
  n_group1 <- sum(rho_group1)
  n_group2 <- sum(rho_group2)

  # Resample values of rho that are larger than 1
  group1 <- rnorm(n_group1, mean = 1, sd = 0.05)
  n_bad1 <- sum(group1 > 1)
  while(n_bad1 > 0) {
    group1[group1 > 1] <- rnorm(n_bad1, mean = 1, sd = 0.175)
    n_bad1 <- sum(group1 > 1)
  }

  # get standard deviation for regression
  std_beta1 <- (beta[rho_group2] - mean(beta[rho_group2])) / sd(beta[rho_group2])

  group2 <- 0.25 - 0.025 * std_beta1 - (rlnorm(n_group2, sdlog = 0.2) - 1)

  # Resample values of rho in group2 as needed
  which_bad2 <- group2 < .13
  while(sum(which_bad2) > 0) {
    group2[group2 < 0.13] <- 0.25 - 0.1 * std_beta1[which_bad2] - (rlnorm(sum(which_bad2), sdlog = 0.2) - 1)

    which_bad2 <- group2 < .13
  }

  rhos[rho_group1] <- group1
  rhos[rho_group2] <- group2

  # Return the values of rho
  rhos
}
# Sample rho, using already sampled beta1.
starting_params$rho <- sample_rho(starting_params$beta1)

sample_tau <- function(mu = 4.1, n_params = N_PARAMS) {
  # Function used to sample tau. Tau is approximately independent of the other
  # parameters.
  #
  # params:
  #   mu: mean of the truncated normal distribution
  # returns:
  #   vector of length 300, sampled values of tau from truncated normal distribution

  # Sample from normal distribution
  X <- rnorm(n_params, mean = mu, sd = (1.7-mu) / qnorm(0.001))

  # Get how many resamples are needed
  n_resamp <- sum(X > 5.9 | X < 1.7)

  # Keep resampling until all values fall within desired range
  while (n_resamp > 0) {
    X[X > 5.9 | X < 1.7] <- rnorm(n_resamp, mean = mu, sd = (1.7-mu) / qnorm(0.001))

    n_resamp <- sum(X > 5.9 | X < 1.7)
  }

  # Return values of tau
  X
}
# Use function to sample tau
starting_params$tau <- sample_tau()

# Here we note that there are additional parameters that were fit, but their
# relationship between the remaining parameters is not given in Lee et al. (2020).
# Because of this, we simply sample these parameters using a truncated normal distribution:

POP <- 10911819
E_0_probs <- dnorm(seq(1, 1752, 1), mean = 400, sd = (1752-400) / qnorm(0.99))
I_0_probs <- dnorm(seq(1, 2271, 1), mean = 500, sd = (2271-500) / qnorm(0.99))
starting_params$E_0 <- sample(1:1752, size = N_PARAMS, replace = TRUE, prob = E_0_probs) / POP
starting_params$I_0 <- sample(1:2271, size = N_PARAMS, replace = TRUE, prob = I_0_probs) / POP
starting_params$S_0 <- 1 - starting_params$E_0 - starting_params$I_0

rm(sample_beta_nu, sample_rho, sample_tau, POP, I_0_probs, E_0_probs)
gc()

#-------------------------- Step 2 ----------------------

# Create epi model for haiti1
h1_epi <- haiti1()

bounds <- tribble(
  ~param, ~lower, ~upper,
  "beta2", 1e-08, 10,
  "beta3", 1e-08, 10,
  "beta4", 1e-08, 10,
  "beta5", 1e-08, 10,
  "beta6", 1e-08, 10
)

lower <- bounds$lower
names(lower) <- bounds$param

upper <- bounds$upper
names(upper) <- bounds$param

betas <- sobol_design(
  lower = lower,
  upper = upper,
  nseq  = N_PARAMS
)

epi_params <- cbind(starting_params, betas)

not_in_start_params <- names(coef(h1_epi))[!names(coef(h1_epi)) %in% colnames(epi_params)]

for (i in 1:length(not_in_start_params)) {
  epi_params[, not_in_start_params[i]] <- coef(h1_epi)[not_in_start_params[i]]
}

rm(i)

epi_params$sig_sq <- 0

# simply re-ordering to match model order
epi_params <- epi_params[, names(coef(h1_epi))]

epi_rw <- rw.sd(
  # Parameters close to desired values
  beta1 = 0.001,
  tau   = 0.001,
  nu    = 0.001,
  rho   = 0.001,
  # Parameters not close to desired values
  beta2 = 0.02,
  beta3 = 0.02,
  beta4 = 0.02,
  beta5 = 0.02,
  beta6 = 0.02,
  E_0   = ivp(0.1),
  I_0   = ivp(0.1)
)

registerDoRNG(48963587)

lee1_epi_mif <- bake(
  file = paste0("model1/", rl_dir, "lee1_epi_fit.rds"),
  {
    foreach(
      i = 1:nrow(epi_params),
      .packages = c('pomp'),
      .combine = c
    ) %dopar% {
      r_params <- unlist(epi_params[i, ])
      coef(h1_epi) <- r_params
      mif2(
        h1_epi,
        Np = NP_MIF,
        Nmif = NMIF,
        cooling.fraction.50 = 0.5,  # Cooling set so that we will reach smaller rw.sd at the end than the end of the unit3 search.
        rw.sd = epi_rw
      )
    }
  }
)

rm(epi_rw, epi_params, not_in_start_params,
   lower, upper, betas, bounds, starting_params)
gc()

# epi_complete <- as.data.frame(t(coef(lee1_epi_mif)))
# epi_complete$logLik <- logLik(lee1_epi_mif)

# GGally::ggpairs(
#   filter(epi_results, logLik > -2200),
#   columns = c("rho", "tau", "beta1", "nu", "logLik")
# )

# epi_complete <- filter(epi_complete, logLik > -2200)

lee_epi_ll <- bake(
  file = paste0("model1/", rl_dir, "lee1_epi_evals.rds"), {

    # Create data.frame to save results
    lee_epi_ll <- data.frame(
      "pfLL" = rep(0, length(lee1_epi_mif)),
      "pfse" = rep(0, length(lee1_epi_mif))
    )

    for (j in 1:nrow(lee_epi_ll)) {
      # Get parameters of interest
      pf_params <- coef(lee1_epi_mif[[j]])

      # make results reproducible
      registerDoRNG((j * 687383921) %% 7919)

      # Calculate log likelihoods
      ll_evals <- foreach(i=1:NREPS_EVAL, .combine = c) %dopar% {
        logLik(pfilter(h1_epi, params = pf_params, Np = NP_EVAL))
      }
      lee_epi_ll[j, 'pfLL'] <- logmeanexp(ll_evals, se = TRUE)[1]
      lee_epi_ll[j, 'pfse'] <- logmeanexp(ll_evals, se = TRUE)[2]
    }

    lee_epi_ll
  }
)

# First, we need to simulate the epidemic model using the fitted parameters.
registerDoRNG(18599687)
foreach(
  i = 1:length(lee1_epi_mif),
  .combine = rbind
) %dopar% {
  guess <- coef(lee1_epi_mif)
  sims <- simulate(h1_epi, nsim = N_LEE_SIMS, format = 'data.frame',
                   params = guess)  # 20 sims for each set of parameters
  sims$param_set <- i
  sims
} -> all_sims

#-------------------------- Step 3 ----------------------

# quants <- all_sims %>%
#   select(.id, week, cases) %>%
#   group_by(week) %>%
#   summarize(
#     q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
#     q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
#     q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
#   ) %>%
#   ungroup() %>%
#   mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

#-------------------------- Step 4 ----------------------

h1_end <- haiti1(period = "endemic")
start_states <- all_sims %>% filter(week == max(week))

end_rw <- rw.sd(
  beta1 = 0.05,
  tau   = 0.005,
  nu    = 0.04,
  rho   = 0.01,
  beta2 = 0.05,
  beta3 = 0.05,
  beta4 = 0.05,
  beta5 = 0.05,
  beta6 = 0.05
)

registerDoParallel(20)
registerDoRNG(21489587)

end_start <- t(coef(lee1_epi_mif))

# end_start <- epi_complete %>%
#   select(-logLik)

lee1_end_mif <- bake(
  file = paste0("model1/", rl_dir, "lee1_end_fit.rds"),
  {
    foreach(
      i = 1:nrow(end_start),
      .packages = c('pomp'),
      .combine = c
    ) %dopar% {

      output <- list()
      r_params <- end_start[i, ]

      start_states %>%
        filter(param_set == i) %>%
        select(-.id, -week, -param_set) %>%
        sapply(., median, na.rm = TRUE) -> rinit_parms

      # Set the inital state
      new_rinit <- Csnippet(
        sprintf(
          "
          S = nearbyint(%f);
          E = nearbyint(%f);
          I = nearbyint(%f);
          A = nearbyint(%f);
          R = nearbyint(%f);
          incid = nearbyint(%f);
          foival = %f;
          Str0 = nearbyint(%f);
          Sout = nearbyint(%f);
          Sin = nearbyint(%f);
          ",
          rinit_parms['S'],
          rinit_parms['E'],
          rinit_parms['I'],
          rinit_parms['A'],
          rinit_parms['R'],
          rinit_parms['incid'],
          rinit_parms['foival'],
          rinit_parms['Str0'],
          rinit_parms['Sout'],
          rinit_parms['Sin']
        )
      )

      h1_end <- h1_end %>% pomp(
        rinit = new_rinit,
        statenames = c(
          "S", "E", "I", "A", "R", "incid",
          "foival", "Str0", "Sout", "Sin"
        )
      )

      coef(h1_end) <- r_params
      mif2(
        h1_end,
        Np = NP_MIF,
        Nmif = NMIF,
        cooling.type = "hyperbolic",
        cooling.fraction.50 = 0.05,  # Cooling set so that we will reach smaller rw.sd at the end than the end of the unit3 search.
        rw.sd = end_rw
      )
      # output[['m2']] <- m2
      #
      # coef(h1_end) <- coef(m2)
      # sims <- simulate(  # Step 5
      #   h1_end, nsim = N_LEE_SIMS, format = 'data.frame',
      #   params = coef(m2)
      # )
      # sims$param_set <- i
      #
      # output[['sims']] <- sims
      # output
    }
  }
)

rm(end_rw, start_states)
gc()

lee_end_ll <- bake(
  file = paste0("model1/", rl_dir, "lee1_end_evals.rds"), {

    # Create data.frame to save results
    lee_end_ll <- data.frame(
      "pfLL" = rep(0, length(lee1_end_mif)),
      "pfse" = rep(0, length(lee1_end_mif))
    )

    # Perform particle filter to get likelihood estimates
    for (j in 1:nrow(lee_end_ll)) {
      # Get parameters of interest
      pf_params <- coef(lee1_end_mif[[j]])

      # make results reproducible
      registerDoRNG((j * 687383921) %% 7919)

      # Calculate log likelihoods
      ll_evals <- foreach(i=1:NREPS_EVAL, .combine = c) %dopar% {
        logLik(pfilter(h1_end, params = pf_params, Np = NP_EVAL))
      }

      lee_end_ll[j, 'pfLL'] <- logmeanexp(ll_evals, se = TRUE)[1]
      lee_end_ll[j, 'pfse'] <- logmeanexp(ll_evals, se = TRUE)[2]
    }

    lee_end_ll
  }
)

rm(lee1_end_mif, lee1_epi_mif)
gc()

# MIF_params <- matrix(nrow = length(lee1_end_mif$m2) / 2, ncol = length(coef(lee1_end_mif[[1]])))
# all_sims_end <- data.frame()
# liks <- rep(0, nrow(end_start))
# for (i in 1:nrow(end_start)) {
#   MIF_params[i, ] <- coef(lee1_end_mif[[(2 * i) - 1]])
#   all_sims_end <- rbind(all_sims_end, lee1_end_mif[[(2 * i)]])
#   liks[i] <- logLik(lee1_end_mif[[(2 * i) - 1]])
# }
#
# colnames(MIF_params) <- names(coef(lee1_end_mif[[1]]))
# end_results <- as.data.frame(MIF_params)
# end_results$logLik <- liks

# GGally::ggpairs(
#   filter(end_results, logLik > -2200),
#   columns = c("rho", "tau", "beta1", "nu", "logLik")
# )

#-------------------------- Step 6 ----------------------

mod1_lee_ll <- max(lee_end_ll$pfLL, na.rm = TRUE) + max(lee_epi_ll$pfLL, na.rm = TRUE)

rm(
  lee_end_ll, lee_epi_ll, h1_end, h1_epi, all_sims, end_start,
  N_LEE_SIMS, N_PARAMS, NMIF, NP_EVAL, NP_MIF, NREPS_EVAL
)
gc()
@



\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Informing policy via dynamic models: Eliminating cholera in Haiti}
\runtitle{Eliminating Cholera in Haiti}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Jesse}~\snm{Wheeler}\ead[label=e1]{jeswheel@umich.edu}\orcid{0000-0003-3941-3884}},
\author[A]{\fnms{AnnaElaine}~\snm{Rosengart}\ead[label=e2]{aelr@umich.edu}}
\author[A]{\fnms{Zhuoxun}~\snm{Jiang}\ead[label=e3]{zhuoxunj@umich.edu}},
\author[A]{\fnms{Kevin}~\snm{Hao En Tan}\ead[label=e4]{kevtan@umich.edu}},
\author[A]{\fnms{Noah}~\snm{Treutle}\ead[label=e5]{ntreutle@umich.edu}}
\and
\author[A]{\fnms{Edward}~\snm{Ionides}\ead[label=e6]{ionides@umich.edu}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Statistics Department, University of Michigan\printead[presep={,\ }]{e1,e2,e3,e4,e5,e6}}
\end{aug}

\begin{abstract}
Public health decisions must be made about when and how to implement interventions to control an infectious disease epidemic.
These decisions should be informed by data on the epidemic as well as current understanding about the transmission dynamics.
Such decisions can be posed as statistical questions about scientifically motivated dynamic models.
Thus, we encounter the methodological task of building credible, data-informed decisions based on stochastic, partially observed, nonlinear dynamic models.
This necessitates addressing the tradeoff between biological fidelity and model simplicity, and the reality of misspecification for models at all levels of complexity.
As a case study, we consider a cholera epidemic in Haiti.
The 2010 introduction of cholera to Haiti led to an extensive outbreak and sustained transmission until it was eliminated in 2019.
We study three models developed by expert teams to advise on vaccination policies.
We assess methods used for fitting and evaluating these models, leading to recommendations for future studies.
Diagnosis of model misspecification and development of alternative models can lead to improved statistical fit, but caution is nevertheless required in drawing policy conclusions based on causal interpretations of the models.
\end{abstract}

\begin{keyword}
  \kwd{Partially observed Markov process}
  \kwd{Hidden Markov model}
  \kwd{infectious disease}
  \kwd{cholera}
  \kwd{sequential Monte Carlo}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:

\section{Introduction}

Quantitative models for dynamic systems offer potential for designing effective control measures.
Regulation of biological populations is a fundamental topic in epidemiology, ecology, fisheries and agriculture.
Quantitative models for these population dynamics may be nonlinear and stochastic, with the resulting complexities compounded by incomplete understanding of the underlying biological mechanisms and by partial observability of the system variables.
Developing and testing such models, and assessing their fitness for guiding policy, is a challenging statistical task.
Questions of interest include: What indications should we look for in the data to assess whether the model-based inferences are trustworthy?
What diagnostic tests and model variations can and should be considered in the course of the data analysis?
What are the possible trade-offs of increasing model complexity, such as the inclusion of interactions across spatial units?

This case study investigates the use of dynamic models and spatiotemporal data to inform a policy decision in the context of the cholera outbreak in Haiti, which started in 2010.
We build on a multi-group modeling exercise by \citet{lee20} in which four expert modeling teams developed models to the same dataset with the goal of comparing conclusions on the feasibility of eliminating cholera by a vaccination campaign.
Model~1 is stochastic and describes cholera at the national level;
Model~2 is deterministic with spatial structure, and includes transmission via contaminated water;
Model~3 is stochastic with spatial structure, and accounts for measured rainfall.
Model~4 has an agent-based construction, featuring considerable mechanistic detail but limited ability to calibrate these details to data.
The strengths and weaknesses of the agent-based modeling approach  \citep{tracy18} are outside the scope of this article, and we focus on Models~1--3.

The four independent teams were given the task of estimating the
potential effect of prospective oral cholera vaccine (OCV) programs.
OCV is accepted as a safe and effective tool for controlling the spread of cholera, however the available stockpile and production of OCV doses is insufficient to meet global needs \citep{lee20, pezzoli20} \eic{NEW CHOLERA VACCINES, NEW STUDIES OF EXISTING VACCINES, AND INCREASED PRODUCTION VOLUMES ALL AROSE DURING 2010-2019. MAYBE SOMETHING LIKE:
Advances in OCV technology and vaccine availability raised the possibility of planning a national vaccination program \citep{lee20, pezzoli20}}
In the study, certain data were shared between the groups, including demography and vaccination history; vaccine efficacy was also fixed at a shared value between groups.
Beyond this, the groups made autonomous decisions on what to include and exclude from their models; this autonomy reduced the possible effect that assumptions about the dynamic system may have on the final conclusion of the study.
Despite this autonomy, and largely adhering to existing guidelines on creating models to inform policy \citep{behrend20,saltelli20}, the consensus across the four models was that an extensive nationwide vaccination campaign would be necessary to eliminate cholera from Haiti.
This conclusion is inconsistent with the fact that there have been no confirmed cases since February, 2019 without additional vaccination efforts \citep{ferguson22}.

The failure of \cite{lee20} to correctly predict the elimination of cholera has been debated \citep{francois20,rebaudetComment20,henrys20,leeReply20}.
\citet{rebaudetComment20} suggested that the models proposed by \citet{lee20} were too unrealistic.
We find a more nuanced conclusion: attention to methodological details in model fitting, diagnosis and forecasting can improve each of the proposed model's ability to quantitatively describe observed data.
These improvements results in forecasts that are more consistent with the observed outcome, without requiring major changes to the model structures.
Based on this retrospective analysis, we offer suggestions on fitting mechanistic models to dynamic systems for future studies.

We proceed by introducing Models~1--3 in Sec.~\ref{sec:models};
in Sec.~\ref{sec:methods}, we present a methodological approach to examining and refining these models.
In Sec.~\ref{sec:results}, we use improved model fits to project cholera incidence in Haiti under various vaccination campaigns.
We then conclude with a discussion on the use of mechanistic models to inform policy decisions in Sec.~\ref{sec:discussion}.

\begin{figure}[ht]
<<Plot_Reported_Cases, echo=FALSE, fig.height=3.5>>=
plot_df <- haitiCholera %>%
  select(-report) %>%
  mutate(date = as.Date(date_saturday)) %>%
  select(-date_saturday) %>%
  pivot_longer(
    data = .,
    cols = -c(date),
    names_to = 'Departement',
    values_to = "Cases",
  )

ggplot(plot_df, aes(x = date, y = Cases + 1)) +
  facet_wrap(~Departement, nrow = 2, labeller = dep_labeller) +
  geom_line() +
  theme(
    axis.title.x = element_blank()
  ) +
  ylab('Reported Cases') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years'))

@
\caption{\label{CholeraData}
\small
Reported Cholera cases in the outbreak in Haiti from 2010-2019.
}
\end{figure}

\section{Mechanistic models for cholera in Haiti}\label{sec:models}
Models that focus on learning relationships between variables in a dataset are called {\it associative}, whereas models that incorporate a known scientific property of the system are called {\it causal} or {\it mechanistic}.
The danger in using forecasting techniques which rely on associative models to predict the consequence of interventions is called the Lucas critique in an econometric context.
\citet{lucas76} pointed out that it is naive to predict the effects of an intervention on a given system based entirely on historical associations.
To successfully predict the effect of an intervention, a model should therefore both provide a quantitative explanation of existing data and should have a causal interpretation: a manipulation of the system should correspond quantitatively with the corresponding change to the model.
This motivates the development of mechanistic statistical models, which provides a statistical fit to the available data while also supporting a causal intepretation.

The deliberate limitation of coordination between the groups of \citet{lee20} allows us to treat the models as fairly independently developed expert approaches to understanding cholera transmission.
However, it led to differences in notation, and in subsets of the data chosen for analysis, that hinder direct comparison.
Here, we have put all three models into a common notational framework.
Translations back to the original notation of \citet{lee20} are given in Table~S-1.

Each model describes the cholera dynamics using a latent state vector $\bm{X}^{(\modelCounter)}(t)$ for each continuous time-point $t \in \mathcal{T}$, where $\mathcal{T}$ is the set of all time-points and $\modelCounter \in \{1, 2, 3\}$ indexes the model.
The observation at time $t_n$ is modeled by a response vector $\bm{Y}^{(\modelCounter)}_n$.
The latent state vector $\bm{X}^{(\modelCounter)}(t)$ consists of individuals labeled as susceptible (S), infected (I), asymptomatically infected (A), vaccinated (V), and recovered (R), with various sub-divisions sometimes considered in each model for $\modelCounter \in \{1, 2, 3\}$.
Models~2 and~3 have metapopulation structure, meaning that each individual is a member of a spatial unit, denoted by a subscript $u\in \seq{1}{U}$.
Here, the spatial units are the $U=10$ Haitian administrative d\'{e}partements (henceforth anglicized as departments).

In the following subsections, complete descriptions of Models~1--3 are provided.
While the model description is scientifically critical, as well as being necessary for transparency and reproducibility, the model details are not essential to our methodological discussions of how to diagnose and address model misspecification with the purpose of informing policy.
A first-time reader may choose to skim through the rest of this section, and return later.

%%%%%%%%% 11111111111 %%%%%%%%%%

\subsection{Model~1}
\label{sec:model1}
$\bm{X}^{(1)}(t) = \big(S_{\vaccCounter}(t),  E_{\vaccCounter}(t), I_{\vaccCounter}(t), A_{\vaccCounter}(t), R_{\vaccCounter}(t), \vaccCounter \in 0:\vaccClass\big)$ describes susceptible, latent (exposed), infected (and symptomatic), asymptomatic, and recovered individuals in vaccine cohort $\vaccCounter$.
Here, $\vaccCounter=0$ corresponds to unvaccinated individuals, and $\vaccCounter \in \seq{1}{\vaccClass}$ describes hypothetical vaccination programs.
The force of infection is
\begin{equation}
\label{model1:lambda}
\lambda(t) = \Big(\sum_{\vaccCounter=0}^{\vaccClass}  I_{\vaccCounter}(t) + \asymptomRelativeInfect \sum_{\vaccCounter=0}^{\vaccClass} A_{\vaccCounter}(t) \Big)^\nu \frac{d\Gamma(t)}{dt} \transmission(t)/N
\end{equation}
where $\transmission(t)$ is a periodic cubic spline representation of seasonality, given in terms of a B-spline basis $\{ s_j(t), j\in \seq{1}{6}\}$ and parameters $\transmission_{1:6}$ as
\begin{equation}
\label{model1:beta}
\log\transmission(t) = \sum_{j=1}^6 \transmission_j s_j(t).
\end{equation}
The process noise $d\Gamma(t)/dt$ is multiplicative Gamma-distributed white noise, with infinitesimal variance parameter $\sigmaProc^2$.
\cite{lee20} included process noise in Model~3 but not in Model~1, i.e., they fixed $\sigmaProc^2 = 0$.
Gamma white noise in the transmission rate gives rise to an over-dispersed latent Markov process \citep{breto11} which has been found to improve the statistical fit of disease transmission models \citep{stocks20,he10}.

Per-capita transition rates are given in Equations~\ref{model1:SE}-\ref{model1:birth}:
\begin{eqnarray}
\label{model1:SE}
\mu_{S_{\vaccCounter}E_{\vaccCounter}} &=& \lambda(t)
\\[-3pt]
\label{model1:EI}
\mu_{E_{\vaccCounter}I_{\vaccCounter}} &=& \muEI\big(1-\symptomFrac_{\vaccCounter}(t)\big)
\\[-3pt]
\label{model1:EA}
\mu_{E_{\vaccCounter}A_{\vaccCounter}} &=& \muEI\, \symptomFrac_{\vaccCounter}(t)
\\[-3pt]
\label{model1:toR}
\mu_{I_{\vaccCounter}R_{\vaccCounter}} &=& \mu_{A_{\vaccCounter}R_{\vaccCounter}} = \muIR
\\[-3pt]
\label{model1:RS}
\mu_{R_{\vaccCounter}S_{\vaccCounter}} &=& \muRS
\\[-3pt]
\label{model1:vacc}
\mu_{S_0S_{\vaccCounter}} &=& \mu_{E_0E_{\vaccCounter}} = \mu_{I_0I_{\vaccCounter}} = \mu_{A_0A_{\vaccCounter}} = \mu_{R_0R_{\vaccCounter}} = \eta_{\vaccCounter}(t)
\\[-3pt]
\label{model1:death}
\mu_{S_{\vaccCounter}\demography} &=& \mu_{E_{\vaccCounter}\demography} = \mu_{I_{\vaccCounter}\demography} = \mu_{A_{\vaccCounter}\demography}=\mu_{R_{\vaccCounter}\demography} = \delta
\\
\label{model1:birth}
\mu_{\demography S_0} &=& \muBirth
\end{eqnarray}
where $\vaccCounter\in \seq{0}{\vaccClass}$.
Here, $\mu_{AB}$ is a transition rate from compartment $A$ to $B$.
We have an additional demographic source and sink compartment $\demography$ modeling entry into the study population due to birth or immigration, and exit from the study population due to death or immigration.
Thus, $\mu_{A\demography}$ is a rate of exiting the study population from compartment $A$ and $\mu_{\demography B}$ is a rate of entering the study population into compartment $B$.

In Model~1, the advantage afforded to vaccinated individuals is an increased probability that an infection is asymptomatic.
Conditional on infection status, vaccinated individuals are also less infectious than their non-vaccinated counterparts by a rate of $\asymptomRelativeInfect = 0.05$ in Eq.~\eqref{model1:lambda}.
In \eqref{model1:EA} and~\eqref{model1:EI} the asymptomatic ratio for non-vaccinated individuals is set $\symptomFrac_0(t)=0$, so that the asymptomatic route is reserved for vaccinated individuals.
For $\vaccCounter\in\seq{1}{\vaccClass}$, the vaccination cohort $\vaccCounter$ is assigned a time $\tau_{\vaccCounter}$, and we take $\symptomFrac_{\vaccCounter}(t) = c \, \theta^*(t-\tau_{\vaccCounter})$
% \begin{equation}
% \label{model1:theta_{\vaccCounter}}
% \symptomFrac_{\vaccCounter}(t) = c \, \theta^*(t-\tau_{\vaccCounter})
% \end{equation}
% \arc{I think we could put this in Table 2 and replace this chunk with a sentence or two defining $\symptomFrac_0(t)$ in words as the proportion of exposed individuals who develop into symptomatic cases and adding a reference to Table 2. This would save a good quarter of a page or so.}
where $\theta^*(t)$ is efficacy at time $t$ since vaccination for adults, taken from \citet{lee20}, Table~S4, and $c=\big(1-(1-0.4688)\times 0.11\big)$ is a correction to allow for reduced efficacy in the 11\% of the population aged under 5 years.
Single and double vaccine doses were modeled by changing the waning of protection; protection was assumed to be equal between single and double dose until 52 weeks after vaccination, at which point the single dose becomes ineffective.

%%%%%%% 222222222222 %%%%%%%%%%

\subsection{Model~2}
\label{sec:model2}
Susceptible individuals are in compartments $S_{u\vaccCounter}(t)$, where $u\in\seq{1}{U}$ corresponds to the $U=10$ departments, and $\vaccCounter\in\seq{0}{4}$ describes vaccination status:
\begin{itemize}
  \item[$\vaccCounter=0$:] Unvaccinated or waned vaccination protection.
  \item[$\vaccCounter=1$:] One dose at age under five years.
  \item[$\vaccCounter=2$:] Two doses at age under five years.
  \item[$\vaccCounter=3$:] One dose at age over five years.
  \item[$\vaccCounter=4$:] Two doses at age over five years.
\end{itemize}

Individuals can progress to a latent infection $E_{u\vaccCounter}$ followed by symptomatic infection $I_{u\vaccCounter}$ with recovery to $R_{u\vaccCounter}$ or asymptomatic infection $A_{u\vaccCounter}$ with recovery to $R^A_{u\vaccCounter}$.
The force of infection depends on both direct transmission and an aquatic reservoir, $W_u(t)$, and is given by
\begin{equation}
\label{model2:lambda}
\lambda_{u}(t) = 0.5\big(1+\seasAmplitude \cos(2\pi t)\big)
\frac{\beta_W\, W_u(t)}{ \Wsat  + W_u(t)} +
\transmission \left\{\sum_{\vaccCounter=0}^4 I_{u\vaccCounter}(t) + \asymptomRelativeInfect \sum_{\vaccCounter=0}^4 A_{u\vaccCounter}(t) \right\}
\end{equation}
The latent state is therefore described by the vector $\bm{X}^{(2)}(t) = \big(S_{u\vaccCounter}(t),\allowbreak E_{u\vaccCounter}(t),\allowbreak I_{u\vaccCounter}(t),\allowbreak A_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter}^A(t),\allowbreak W_u,\allowbreak u \in \seq{1}{U},\allowbreak \vaccCounter \in \seq{0}{4}\big)$.
The cosine term in Eq.~\myeqref{model2:lambda} accounts for annual seasonality, with a phase parameter $\phaseParm$.
The original implementation of Model~2 in \cite{lee20} fixes the phase at $\phaseParm = 0$.

Individuals move from department $u$ to $v$ at rate $T_{uv}$, and aquatic cholera moves at rate $T^W_{uv}$.
The nonzero transition rates are
\begin{eqnarray}
\label{model2:mu_SE}
\mu_{S_{u\vaccCounter}E_{u\vaccCounter}} &=& \vaccineEfficacy_\vaccCounter \, \lambda
\\[-3pt]
\label{model2:mu_EI}
\mu_{E_{u\vaccCounter}I_{u\vaccCounter}} &=& \symptomFrac\muEI, \quad \mu_{E_{u\vaccCounter}A_{u\vaccCounter}} = (1-\symptomFrac)\muEI
\\[-3pt]
\label{model2:mu_IR}
\mu_{I_{u\vaccCounter}R_{u\vaccCounter}} &=& \mu_{A_{u\vaccCounter}R^A_{u\vaccCounter}} = \muIR
\\[-3pt]
\label{model2:RS}
\mu_{R_{u\vaccCounter}S_{u\vaccCounter}} &=& \mu_{R^A_{u\vaccCounter}S_{u\vaccCounter}} = \muRS
\\[-3pt]
\label{model2:transport}
\mu_{S_{u\vaccCounter}S_{{\varv}\vaccCounter}} &=& \mu_{E_{u\vaccCounter}E_{{\varv}\vaccCounter}} = \mu_{I_{u\vaccCounter} I_{{\varv}\vaccCounter}} = \mu_{A_{u\vaccCounter}A_{{\varv}\vaccCounter}} = \mu_{R_{u\vaccCounter}R_{{\varv}\vaccCounter}} = \mu_{R^A_{u\vaccCounter} R^A_{{\varv}\vaccCounter}} = T_{u\varv}
\\[-3pt]
\label{model2:omega1}
\mu_{S_{u1}S_{u0}} &=& \mu_{S_{u3}S_{u0}} = \omega_1
\\[-3pt]
\label{model2:omega2}
\mu_{S_{u2}S_{u0}} &=& \mu_{S_{u4}S_{u0}} = \omega_2
\\[-3pt]
\label{model2:to_W}
\mu_{\demography W_u} &=& \Wshed \left\{ \sum_{\vaccCounter=0}^4 I_{u\vaccCounter}(t) + \asymptomRelativeShed \sum_{\vaccCounter=0}^4 A_{u\vaccCounter}(t) \right\}
\\[-3pt]
\label{model2:from_W}
\mu_{W_u\demography} &=& \Wremoval
\\[-3pt]
\label{model2:water_transport}
\mu_{W_uW_{\varv}} &=& w_r T^W_{u\varv}
\end{eqnarray}
In \eqref{model2:transport} the spatial coupling is specified by a gravity model,
\begin{equation}
\label{model2:gravity}
T_{u\varv} = v_{\mathrm{rate}} \times \frac{\mathrm{Pop}_u \mathrm{Pop}_{\varv}}{D_{u\varv}^2},
\end{equation}
where $\mathrm{Pop}_u$ is the mean population for department $u$,
$D_{u\varv}$ is a distance measure estimating average road distance between rancomly chosen members of each population, and $v_{\mathrm{rate}}= 10^{-12}$ was treated as a fixed constant.
In \eqref{model2:water_transport}, $T^W_{u\varv}$ is a measure of river flow between departments.
The unit of $W_u(t)$ is cells per ml, with dose response modeled via a saturation constant of $\Wsat$ in \eqref{model2:lambda}.


%%%%%%% 333333333 %%%%%%%%%%

\subsection{Model~3}
\label{sec:model3}

The latent state is described as $\bm{X}^{(3)}(t) = \big(S_{u\vaccCounter}(t),\allowbreak I_{u\vaccCounter}(t),\allowbreak A_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter k}(t),\allowbreak W_u(t),\allowbreak u \in \seq{0}{U},\allowbreak \vaccCounter \in \seq{0}{4},\allowbreak k \in \seq{1}{3}\big)$.
Here, $\vaccCounter=0$ corresponds to unvaccinated, $\vaccCounter=2j-1$ corresponds to a single dose on the $j$th vaccination campaign in unit $u$ and $\vaccCounter=2j$ corresponds to receiving two doses on the $j$th vaccination campaign.
$k\in\seq{1}{3}$ models non-exponential duration in the recovered class before waning of immunity.
The force of infection is
\begin{eqnarray}
\label{eq:model3:foi}
\lambda_u(t) &=& \Wbeta{_u} \frac{W_u(t)}{1+W_u(t)} + \transmission_u \sum_{{\varv}\neq u}\big(I_{\varv 0}(t)+ \asymptomRelativeInfect A_{\varv 0}(t)\big)
\\[-3pt]
\label{eq:model3:SI}
\mu_{S_{u\vaccCounter}I_{u\vaccCounter}} &=& \symptomFrac \,  \lambda_u \big(1-\eta_{u\vaccCounter}(t)\big) \, d\Gamma/dt
\\[-3pt]
\label{eq:model3:SA}
\mu_{S_{u\vaccCounter}A_{u\vaccCounter}} &=& (1-\symptomFrac) \,  \lambda_u \big(1-\eta_{u\vaccCounter}(t)\big) \,  d\Gamma/dt
\\[-3pt]
\label{eq:model3:IR}
\mu_{I_{u\vaccCounter}R_{u\vaccCounter 1}} &=& \mu_{A_{u\vaccCounter}R_{u\vaccCounter 1}} = \muIR
\\[-3pt]
\label{eq:model3:IS}
\mu_{I_{u\vaccCounter}S_{u0}} &=& \muDeath + \choleraDeath
\\[-3pt]
\label{eq:model3:AS}
\mu_{A_{u\vaccCounter}S_{u0}} &=& \muDeath
\\[-3pt]
\label{eq:model3:RRnext}
\mu_{R_{u\vaccCounter 1}R_{u\vaccCounter 2}} &=& \mu_{R_{u\vaccCounter 2}R_{u\vaccCounter 3}} = 3\muRS
\\[-3pt]
\label{eq:model3:RS}
\mu_{R_{u\vaccCounter k}S_{u0}} &=& \muDeath + 3\muRS \, \mathbf{1}_{\{k=3\}}
\\[-3pt]
\label{eq:model3:water}
\mu_{{\demography}W_u} &=& \big[1 + \seasAmplitude \big(J(t))^r \big] D_i \, \Wshed \big[ I_{u0}(t)+ \asymptomRelativeShed A_{u0}(t) \big]
\\[-3pt]
\label{eq:model3:Decay}
\mu_{W_u\demography} &=& \Wremoval
\end{eqnarray}

As with Model~1, $d\Gamma_u(t)/dt$ is multiplicative Gamma-distributed white noise in \myeqref{eq:model3:SI} and \myeqref{eq:model3:SA}.
In \eqref{eq:model3:water}, $J_u(t)$ is a dimensionless measurement of precipitation that has been standardized by dividing the observed rainfall at time $t$ by the maximum recorded rainfall in department $u$ during the epidemic, and $D_u$ is the average population density.
Demographic stochasticity is accounted for by modeling non-cholera related death rate $\muDeath$ in each compartment, along with an additional death rate $\choleraDeath$ in \myeqref{eq:model3:IS} to account for cholera induced deaths among infected individuals.
We note that all deaths are balanced by births into the susceptible compartment in \myeqref{eq:model3:AS} and \myeqref{eq:model3:RS}, thereby maintaining constant population in each department.

\section{Statistical Analysis}\label{sec:methods}

We consider model fitting (Sec.~\ref{model_fitting}) followed by diagnostic investigations (Sec.~\ref{model_diagnostics}) and forecasting (Sec.~\ref{sec:filter}).

<<Load Model Parameters, echo=FALSE, results='hide'>>=
load('model3/output/PanelLocalAll_PF.rda')  # Final model 3 evaluations
load('model3/output/PanelLocalAll.rda')  # Final model 3 parameters (MIF search)
ll_mod3 <- mif_logLik
MIF_mod3 <- local_MIF2_search
rm(mif_logLik, local_MIF2_search, tLocal)

# load('model1/output/trendResultsJoint.rda')  # Parameters and evaluations
load('model1/output/recal_mcap_results.rda')

p1 <- best_params
rm(best_params)

gc()

# Fit model 2. Note that this takes the same ammount of time at all run
# levels.
h2_fit <- bake(
  file = paste0("model2/", rl_dir, "model2_fit.rds"), {
    fit_haiti2()
  },
  timing = FALSE
)

@

<<table-input,echo=FALSE,eval=T, include=FALSE, message=FALSE>>=

best_m3 <- ll_mod3 %>%
  arrange(-logLik) %>%
  slice_head(n = 1) %>%
  pull(which)

p3 <- coef(MIF_mod3[[best_m3]])
rm(MIF_mod3)
gc()


stew(file = "models.rda", {
  h1 <- haiti1_joint()
  coef(h1) <- p1

  h2_epi <- haiti2(region = 'before', cutoff = 10000, measure = 'log')

  coef(h2_epi) <- h2_fit$h2_params

  h3 <- haiti3_panel(start_time = "2010-10-23", B0 = TRUE)
  coef(h3) <- p3
  pp3 <- pparams(h3)
  p3u <- pp3$specific
  p3s <- pp3$shared
})
@


\subsection{Model Fitting}\label{model_fitting}

Proposed mechanistic structures form a family of statistical models indexed by a parameter vector $\paramVec$.
Different values of $\paramVec$ can result in qualitative differences in the predicted behavior of the system.
The complex nature of biological systems necessitates a search for modeling assumptions that combine insightful simplicity with fidelity to biological reality.
For example, many models commonly used in epidemiology are motivated by reasoning about a homogeneous mixing population \citep{bansal07} which is simultaneously an avenue for powerful simplification and a source of model misspecification.
Other common considerations include whether the proposed model should be stochastic or deterministic; whether the model should have change points in parameter values or should otherwise make adjustments for changes through time in the dynamic system; and whether the proposed model should include any spatial heterogeneity at a scale permissible by the observed data.
In addition, elements of $\paramVec$ can either be chosen as constants, based on scientific reasoning and previous knowledge, or calibrated to observed data.
Suitable methodology for calibrating model parameters may depend on other modeling decisions.
While this section is focused on the elements of $\paramVec$ that are calibrated to data, we note that the decision of which elements to fix and which to estimate has consequences for model interpretability, as discussed in Sec.~\ref{sec:results}.

All three model considered in this study describes cholera dynamics via unobservable states that evolve dynamically with time.
Despite their similarities, these models represent a diverse selection of possible modeling assumptions: namely, the use of stochastic (Models~1 and 2) or deterministic (Model~2) equations; spatially-heterogeneous meta-population (Models~2 and 3) or spatially-aggregated (Model~1) structure; and the use of covariates (Model~3) versus mathematical equations (Models~1 and 2) to describe a seasonal mechanism.
All these structures can be described in the framework of partially observed Markov process (POMP) models, with the understanding that the deterministic Model~2 is a degenerate case of a stochastic model.
In the following subsections we describe our approach to fitting these mechanistic models.

\subsubsection{Model~1}

One approach to modeling a dynamic system is through probabilistic models.
With a probabilistic model, we suppose the existence of a joint density $f_{\bm{X}^{(\modelCounter)}_{\seq{0}{N}}, \bm{Y}^{(\modelCounter)}_{\seq{1}{N}}}$, with $\bm{X}^{(\modelCounter)}_{\seq{0}{N}}$ denoting the unobservable Markov process of model $\modelCounter$ at times $\seq{0}{N} = \{0, 1, \ldots, N\}$, and $\bm{Y}^{(\modelCounter)}_{\seq{1}{N}}$ denoting the observable process of the system at times $\seq{1}{N}$.
Under this framework, the observed data $y_{\seq{1}{n}}^*$ are assumed to be a single realization of the model $y_{\seq{1}{n}}^* \sim f_{\bm{X}^{(\modelCounter)}_{\seq{0}{N}}, \bm{Y}^{(\modelCounter)}_{\seq{1}{N}}}(x_{\seq{0}{N}}, y_{\seq{1}{N}}; \paramVec)$, where $\paramVec$ is a parameter vector that indexes the model.
Using a probabilistic model results in several advantages, including the ability to account for variability present in the system.
% which is of great interest to scientists and policy makers.
Furthermore, because each draw from the joint distribution represents a potential outcome of the dynamic system, best/worst case scenarios under the assumptions of the model can be easily obtained via simulation.

Once a model for the system has been proposed, the parameter vector $\paramVec$ needs to be estimated using the observed data.
There exist several algorithms, both frequentist and Bayesian, that can be used to obtain estimates of the parameters in stochastic dynamic models.
% including the EM algorithm, Kalman Filter (and extensions) \citep{evensen09}, and iterated filtering algorithms \citep{ionides15}.
In order to retain the ability to propose models that are scientifically meaningful rather than only those that are simply statistically convenient, we restrict ourselves to parameter estimation techniques that have the plug-and-play property, which is that the fitting procedure only requires the ability to simulate the latent process instead of evaluating transition densities \citep{breto09,he10}.
Plug-and-play algorithms include Bayesian approaches like ABC and PMCMC \citep{toni09,andrieu10}, but here we use frequentist methods to maximize model likelihoods.
To our knowledge, the only plug-and-play frequentist methods that can maximize the complete model likelihood are iterated filtering algorithms, which modify the well-known particle filter \citep{arulampalam02} by performing a random walk for each parameter and particle.
These perturbations are carried out iteratively over multiple filtering operations, using the collection of parameters from the previous filtering pass as the parameter initialization for the next iteration, and decreasing the random walk variance at each step.

The ability to maximize the likelihood allows for likelihood-based inference, like performing statistical tests for potential improvements to the model.
% TODO: We should address somewhere why we included the process noise in Model 1. Anna suggested this below:
% We first demonstrate this capability by proposing the inclusion of additional stochasticity in the latent process through the force of infection, similar to that in Model~3. Eq.~\myeqref{model1:lambda} becomes
% \begin{equation}
% \label{model1:lambda_wn}
% \lambda(t) = \Big(\sum_{k=0}^{K}  I_{k}(t) + \asymptomRelativeInfect \sum_{k=0}^{K} A_{k}(t) \Big)^\nu d\Gamma_u(t)/dt \transmission(t)/N
% \end{equation}
% where $d\Gamma_u(t)/dt$ is multiplicative Gamma-distributed white noise.
%
% Part of the appeal of mechanistic models lies in their ability to illustrate a dynamical system.
% Inherent is dynamical systems is randomness, and, in the context of epidemiology, this randomness is a crucial component to the system itself.
% Useful and statistically sound incidence forecasting and prediction of vaccine efficacy are not only dependent upon a reasonable reflection of the natural system in the model; it is necessary for the model to also reflect the quantitative characteristics of the processes in question.
% This need, itself, is dependent upon inclusion of sufficient stochasticity in the model \citep{breto09}.
% For now, we motivate this decision by its presence in Model~3 and leave the more rigorous assessment of this adjustment to the supplement.
% The above can no doubt be improved.
% \arc{I think that evaluating the inclusion of both the noise and the trend parameter in the main text may distract from the main point of this manuscript, which is why I mentioned putting it in the supplement.}
% \eic{The dynamic noise issue is important, but is well described elsewhere so can perhaps be handled mostly by references. Maybe we don't need to do more than mention that Lee et al constrained the noise parameter to be zero, but we found considerable advantage from including the parameter (XXX log units of likelihood) consistent with previous investigations (refs).}
% Transition phrase here?
We demonstrate this capability by proposing a linear trend $\transmissionTrend$ in transmission in Eq.~\myeqref{model1:beta}:
\begin{equation}
\label{model1:betat}
\log\transmission(t) = \sum_{j=1}^6 \transmission_s s_j(t) + \transmissionTrend\bar{t}
\end{equation}
Where $\bar{t}$ is the linear mapping $\bar{t}: [0, N] \rightarrow [-1, 1]$ of the time $t$.
The proposal of a linear trend in transmission is a result of observing an apparent decrease in reported cholera infections from 2012-2019 in Fig.~\ref{CholeraData}.
% The inclusion of a possible trend in transmission is due to observing an apparent continual decrease in reported cholera infections from 2012-2019 can be seen in Fig.~\ref{CholeraData}.
While several factors may contribute to this decrease, one explanation is that case-area targeted interventions (CATIs), which included education sessions, increased monitoring, household decontamination, soap distribution, and water chlorination in infected areas \citep{rebaudet19CATI}, may have greatly reduced cholera transmission \citep{rebaudet21}.

We perform a statistical test to determine whether or not the data indicate the presence of a linear trend in transmissibility.
To do this, we perform a profile-likelihood search on the parameter $\transmissionTrend$ and obtain a confidence interval via a Monte Carlo Adjusted Profile (MCAP) \citep{ionides17}.
Similar to Model~2, this model was originally implemented in two parts: an epidemic phase from October 2010 through March 2015, and an endemic phase from March 2015 onward.
As before, we continue to allow the re-estimation of some model parameters at the start of the endemic phase ($\reportRate, \sigmaProc^2$ and $\obsOverdispersion$) but require that the latent Markov process $X(t)$ carry over from one phase into the next.
The resulting confidence interval for $\transmissionTrend$ is $(\Sexpr{myround(mcap_results$ci[1], digits = 3)}, \Sexpr{myround(mcap_results$ci[2], digits = 3)})$, with the full results displayed in Fig.~\ref{fig:betat}.
These results are suggestive that the inclusion of a trend in transmission rate improves the quantitative ability of Model~1 to describe the observed data.
The reported results for Model~1 in the remainder of this article were obtained with the inclusion of the parameter $\transmissionTrend$.

\begin{figure}[ht]
\centering
<<Beta_trend_Figure, fig.height=2.5, fig.width=3.7, fig.align='center'>>=
ggplot() +
  geom_point(data = maxresults, aes(x = betat, y = logLik)) +
  geom_line(data = mcap_results$fit, aes(x = parameter, y = smoothed), col = 'blue') +
  # geom_line(data = mcap_results$fit, aes(x = parameter, y = quadratic), col = 'red') +
  geom_vline(xintercept = mcap_results$ci[1], linetype = 'dashed') +
  geom_vline(xintercept = mcap_results$ci[2], linetype = 'dashed') +
  # geom_vline(xintercept = mcap_results$quadratic_max, col = 'red') +
  geom_vline(xintercept = mcap_results$mle, col = 'blue') +
  labs(x = "Linear Trend in Transmission", y = 'Log Likelihood') +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 10))
@
\caption{\label{fig:betat}
Monte Carlo adjusted profile of $\transmissionTrend$.
The blue curve is the profile, the blue line indicates the MLE, and the dashed lines indicate the confidence interval.
}
\end{figure}

Model~1 is implemented using the \code{pomp} package \cite{pomp2}, which contains an implementation of the IF2 algorithm that was used to compute the likelihood profile.
% Performing a profile confidence search of a parameter also results in a MLE calculation, as the value that maximizes the profile likelihood also maximizes the entire likelihood surface \citep{murphy2000}.
Simulations from the fitted model compared to the observed data are given in Fig~\ref{fig:mod1fit}.

\begin{figure}[ht]
\centering
<<Model_1_Sims_Figure, fig.height=2.4, fig.width=4.7, fig.align='center'>>=
sims <- simulate(h1, nsim = 500, format = 'data.frame', seed = 3448931)

quants <- sims %>%
  mutate(is_data = .id == 'data') %>%
  filter(!is_data) %>%
  select(.id, week, cases) %>%
  group_by(week) %>%
  summarize(
    q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
    q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
    q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

ggplot() +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  geom_line(data = quants, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = quants, aes(x = date, ymin = q025 + 1, ymax = q975 + 1), alpha = 0.5) +
  theme(axis.title.x = element_blank(),
        axis.text = element_text(size = 8),
        axis.title.y = element_text(size = 10))+
  ylab('Reported cholera cases') +
  geom_vline(xintercept = lubridate::weeks(232) + lubridate::ymd("2010-10-16"), linetype = 'dashed') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '1 year'))
@
\caption{\label{fig:mod1fit}
Simulations from Model~1 compared to reported cholera cases.
The black curve is observed data, the blue curve is median of 500 simulations from the fitted model, and the vertical dashed line represents break-point when parameters are refit.
}
\end{figure}

<<Clean-up Model 1 Simulations, echo=FALSE, include=FALSE>>=
rm(sims, quants)
gc()
@

\subsubsection{Model~2}

Model~2 is a deterministic model, which can be viewed as a special case of a POMP (or SpatPOMP) with no randomness in the dynamic process or measurement.
By combining a deterministic process model with a simple Gaussian measurement model, Model~2 reduces model fitting to a least squares calculation over parameters in a set of differential equations.
Deterministic compartment models have a long history in the field of infectious disease epidemiology \citep{kermack1927,brauer2017,varghese21},
and can be justified by asymptotic considerations in a large-population limit \citep{dadlani2020,ndii17}.

\citet{lee20} originally fit two versions of model~2 based on a presupposed change in cholera transmission from a epidemic phase to endemic phase that occurred in March, 2014.
We follow their decision to re-estimate model parameters at this break-point, but we include a requirement that the latent state $\bm{X}^{(2)}(t)$ at the start of the endemic period must be the same as the state at the end of the epidemic period.
This additional constraint is sensible for a mechanistic interpretation of the latent state, though it adds difficulty to the task of obtaining a model fit that closely resembles the observed data.
To combat this added difficulty, and to allow for a possible shift in the seasonality component in the force of infection, we introduce an additional phase parameter $\phaseParm$ in Eq.~\ref{model2:lambda}.
% \begin{equation}
% \lambda_{u}(t) = 0.5\big(1+\seasAmplitude \cos(2\pi t + \phaseParm)\big)
% \frac{\beta_W\, W_u(t)}{ \Wsat  + W_u(t)} +
% \beta \left\{\sum_{d=0}^4 I_{ud}(t) + \asymptomRelativeInfect \sum_{d=0}^4 A_{ud}(t) \right\}\label{eq:model2:phase}
% \end{equation}
% \eic{WE COULD SAVE AN EQUATION BY WRITING THE MORE GENERAL ONE INITIALLY AND SAYING THAT THEY ASSUMED $\phaseParm=0$}

% We also note that, without the inclusion of a phase parameter, there is an implicit assumption is that force of infection is greatest around January and weakest around June, and the addition of this parameter allows for the testing of this assumption.
We further increase model flexibility by fitting the parameter $\Wshed$ rather than treating it as fixed.
We implemented this model using the \code{spatPomp} \code{R} package \citep{asfaw21github}.
The model was then fit using the subplex algorithm, implemented in the  \code{subplex} package \citep{king2020Subplex}.
A comparison of the trajectory of the fitted model to the data is given in Fig~\ref{fig:mod2Traj}.

<<Model 2 Trajectory, echo=FALSE>>=
h2_epi_traj <- trajectory(h2_epi, params = h2_fit$h2_params, format = 'data.frame')
h2_epi_traj$Ctotal <- rowSums(h2_epi_traj[, paste0("C", 1:10)]) * h2_fit$h2_params['Rho']


h2_traj <- h2_epi_traj %>%
  select(year, Ctotal) %>%
  mutate(
    date = yearsToDate(year)
  )
@


\subsubsection{Model~3}
Model~3 is also of a probabilistic model.
In this model, both the latent and observable processes can be factored into department specific processes which interact with each other.
We denote the latent and measurement processes for Model~3 as $\bm{X}^{(3)}(t_{0:N}) = X^{(3)}_{1:U, 0:N}$, and $\bm{Y}^{(3)}(t_{1:N}) = Y_{1:U, 1:N}$ \eic{IS THIS NEEDED HERE?}.
The decision to model the system via metapopulation models versus a model aggregated to a larger spatial scale is one of great scientific interest, and evidence for the former approach has been provided in previous studies \citep{king15}.
Note that this evidence alone does not automatically discredit conclusions drawn via nationally aggregated models, as there are also good reasons to prefer a simple model over a complex one \citep{saltelli20,green15}.
Still, researchers that intend to use a mechanistic model to describe a dynamic system should design the model to be as realistic as their scientific understanding of the system and their computational abilities permit \eic{THIS SENTENCE SEEMS TO DISMISS THE ARGUMENTS IN THE PREVIOUS SENTENCE. PERHAPS THIS SENTENCE CAN JUST BE DELETED}.
Fitting scientifically flexible metapopulation models, however, remains a challenging statistical problem;
this is due to the fact that the approximation error of particle filters grows exponentially in the dimension of the model \citep{rebeschini15,park20}.
Algorithms that are based on the particle filter therefore become computationally intractable as the number of spatial units increase.

Parameters that must be fit in Model~3 are primarily shared between each department, the exception to this being the parameters $\beta_{W_u}$, and $\beta_u$, which are unique for each department $u \in \seq{1}{10}$.
To fit this model, \citet{lee20} simplified the parameter estimation problem by fitting independent department-level models to the data.
The shared parameters were calibrated using the cholera incidence data from Artibonite, and the department-specific parameters ($\beta_{W_u}$ and $\beta_u$) were fit using the data from their respective department.
Reducing a spatially coupled \eic{not spatially heterogeneous, as previously written} model to individual units in this fashion requires special treatment of any interactive mechanisms between spatial units, such as found in Eq.~\myeqref{eq:model3:foi}.
In particular, when considering a model for department $u$, the values $I_\nu(t)$ and $A_\nu(t)$, are unknown for $u \neq v \in \seq{1}{10}$.
A first order approximation of Eq.~\myeqref{eq:model3:foi} for each department $v \in \seq{1}{10}$ can be obtained using the weekly number of observed cholera cases in each department:
\begin{equation}
  I_v(t) + A_v(t) \approx \frac{365}{7\reportRate}y^*_v(t)\left(\frac{1}{\muDeath + \choleraDeath + \muIR} + \frac{1 - \symptomFrac}{\symptomFrac(\muDeath + \muIR)}\right)\label{eq:model3:approx}
\end{equation}
This approximation leads to department-specific models that are conditionally independent given the reported number of cholera infections in the remaining departments.
Here, we refer to a collection of POMP models that are independent across units as a PanelPOMP.

In the case of a PanelPOMP, an extension of the IF2 algorithm, known as Panel Iterated Filtering (PIF) \citep{breto20}, can be used to obtain the MLE, which solves the curse of dimensionality for this class of models.
A major advantage of this algorithm over fitting each unit-specific model separately is that PIF can be used to fit both unit-specific parameters and shared parameters; in this way, the calibration of shared parameters involves all of the available data instead of just an arbitrarily chosen subset.

We then fit the panel version of Model~3 using a slight modification of the PIF algorithm, which we call the Block Panel Iterated Filter (BPIF).
The BPIF algorithm is implemented in the \code{panelPomp} package \citep{breto20panelPomp} in the R programming language, and can be used by adding the argument \code{block = TRUE} in the \code{mif2} function.
Pseudo-code for this algorithm is provided in Algorithm~S1 in the supplementary material.
% Even with the simplification of Eq.~\myeqref{eq:model3:approx}, a significant amount of computational effort is needed to obtain a proper model fit.
The reported parameters were obtained using the PanelPOMP version of Model~3, and simulations for the various vaccination campaigns (Sec.~\ref{sec:results}) were obtained using these same parameters in the SpatPOMP version of the model.
% Simulations from the fitted version of the PanelPOMP model are shown in Fig.~\ref{h3panelsims}.

<<Model 3 PanelSims,echo=FALSE>>=
h3_panel_sims <- switch(RUN_LEVEL, 20, 100, 500)

departements <-
  c(
    'Artibonite',
    'Centre',
    'Grande_Anse',
    'Nippes',
    'Nord',
    'Nord-Est',
    'Nord-Ouest',
    'Ouest',
    'Sud',
    'Sud-Est'
  )

registerDoRNG(5317865)

h3_panel_sims <- bake(
  file = paste0('model3/', rl_dir, 'panelPompSims.rds'), {
    foreach(dep = departements, .combine = rbind) %dopar% {
      SIRB <- unitobjects(h3)[[dep]]
      shared <- h3@shared
      specific <- h3@specific[, dep]

      pomp::simulate(
        SIRB,
        params = c(shared, specific),
        nsim = h3_panel_sims,
        format = 'data.frame'
      ) -> sims

      sims$dep <- dep
      sims
    }
  }
)

h3_panel_quantiles <- h3_panel_sims %>%
  mutate(
    date = as.Date(
      lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
    )
  ) %>%
  group_by(dep, date) %>%
  summarise(
    q05 = quantile(cases, 0.025, na.rm = T),
    mean = mean(cases, na.rm = T),
    q50 = quantile(cases, 0.5, na.rm = T),
    q95 = quantile(cases, 0.975, na.rm = T)
  ) %>%
  ungroup() %>%
  mutate(
    dep = gsub("-", "_", dep)
  ) %>%
  filter(date >= yearsToDate(h3@unit.objects$Artibonite@t0))

plot_order <- c(
  'Artibonite',
  'Sud_Est',
  'Nippes',
  'Nord_Est',
  'Ouest',
  'Centre',
  'Nord',
  'Sud',
  'Nord_Ouest',
  'Grande_Anse'
)

@

\begin{figure}[ht]
<<Model3_PanelSims_Figure, echo=FALSE, fig.height=3>>=
ggplot() +
  geom_line(data = h3_panel_quantiles, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_line(data = dep_plot_df, aes(x = date, y = cases + 1), col = 'black') +
  geom_ribbon(data = h3_panel_quantiles, aes(x = date, ymin = q05 + 1, ymax = q95 + 1),
              alpha = 0.4) +
  facet_wrap(~factor(dep, levels = plot_order), nrow = 2,
             labeller = dep_labeller) +
  ylab('Reported cholera cases') +
  theme(axis.title.x = element_blank()) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years')) +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

@
\caption{\label{h3panelsims}
Simulations from initial conditions using panelPOMP version of Model~3.
The black curve represents true case count, the blue curve the median of 500 simulations from the model, and the grey ribbons representing $95\%$ confidence interval.
}
\end{figure}

<<Remove Model3 panelSims, cache=FALSE, include=FALSE, message=FALSE, echo=FALSE>>=
rm(h3_panel_quantiles, h3_panel_sims)
gc()
@


Fitting Model~3 as a PanelPOMP simplifies the problem of parameter estimation, but it also introduces additional technicalities that must be addressed.
One concern is that of obtaining model forecasts, which was the primary goal of \citet{lee20}.
% "primary output" -> "primary goal", based on Anna's suggestion.
The simplified PanelPOMP version of Model~3 relies on the observed cholera cases as a covariate, which are unavailable for use in forecasts.
To address this, we use the MLE obtained for the PanelPOMP approximation of Model~3 as an estimate for the parameters in fully coupled version of the model, which was implemented using the \code{spatPomp} package.
Simulations from the SpatPOMP version of the model, using the parameters that were fit with the PanelPOMP version of the model, are displayed in Fig.~\ref{h3spatsims}.

<<Model 3 SpatPOMP sims, echo=FALSE>>=
# load('output/FinalPanelAsSpatPomp_PF.rda')

h3_spat_nsim <- switch(RUN_LEVEL, 20, 100, 500)

h3Spat_quants <- bake(
  file = paste0('model3/', rl_dir, 'SpatPompSims.rds'), {
    h3Spat <- haiti3_spatPomp()
    coef(h3Spat) <- haitipkg:::panelParms_toSpatParms3(p3)

    h3Spat_sims <- simulate(
      h3Spat, nsim = h3_spat_nsim, format = 'data.frame'
    )

    h3Spat_sims %>%
      rename(dep = unitname) %>%
      group_by(dep, time) %>%
      summarise(
        q05 = quantile(cases, 0.025, na.rm = T),
        mean = mean(cases, na.rm = T),
        q50 = quantile(cases, 0.5, na.rm = T),
        q95 = quantile(cases, 0.975, na.rm = T)
      ) %>%
      ungroup() %>%
      mutate(
        date = lubridate::date_decimal(time)
      ) %>%
      filter(date >= yearsToDate(h3Spat@t0)) %>%
      mutate(date = as.Date(lubridate::round_date(date, unit = 'day')))
  },
  timing = FALSE
)

@

\begin{figure}[ht]
<<Model3_SpatPOMP_sims_Figure, fig.height=3>>=
ggplot() +
  geom_line(data = h3Spat_quants, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_line(data = dep_plot_df, aes(x = date, y = cases + 1), col = 'black') +
  geom_ribbon(data = h3Spat_quants,
              aes(x = date, ymin = q05 + 1, ymax = q95 + 1),
              alpha = 0.4) +
  facet_wrap(~factor(dep, levels = plot_order), nrow = 2,
             labeller = dep_labeller) +
  labs(y = 'Log Reported Cholera Cases') +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years')) +
  theme(axis.title.x = element_blank()) +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))
@
\caption{\label{h3spatsims}
Simulations from initial conditions using SpatPOMP version of Model~3.
The black curve represents true case count, the blue line the median of 500 simulations from the model, and the grey ribbons representing $95\%$ confidence interval.
}
\end{figure}

<<Remove Model3 spatSims, cache=FALSE, include=FALSE, message=FALSE, echo=FALSE>>=
rm(h3Spat_quants, h3_spat_nsim)
gc()
@

While simulations from the SpatPOMP version of Model~3 resemble the observed data, we note in Table~\ref{tab:likes} that the SpatPOMP version of Model~3 has a lower log-likelihood than the PanelPOMP version using the same model parameters.
A natural question to ask is if the MLE obtained for the PanelPOMP version of Model~3---obtained using the approximation in Eq.~\myeqref{eq:model3:approx}---could result in a drastically different set of parameters than those that would be obtained by estimating the MLE in the fully coupled model.
Unfortunately, the answer to this question would require the ability to accurately and reliably estimate the MLE of the fully coupled model, which is beyond the scope of this article.
% this question is unsatisfactorily left unanswered, because the answer would require knowledge of the the MLE without the simplification, which is an open statistical question and beyond the scope of this article.
% While this approach may provide a reasonable estimate of the model parameters, it is unknown by how much this estimate may be improved.
This highlights the need for advancements in statistical methodology that permit inference on models with coupled metapopulation dynamics.

% \subsection{Comparing mechanistic models with associative benchmarks} %AAAAAAAAAAAAAAA
\subsection{Model Diagnostics} \label{model_diagnostics}

Parameter calibration (whether Bayesian or frequentist) aims to find the best description of the observed data under the assumptions of the model.
Obtaining the best fitting set of parameters for a given model does not, however, guarantee that the model provides an accurate representation of the system in question.
Model misspecification, which may be thought of as the omission of a mechanism in the model that is an important feature of the dynamic system, is inevitable at all levels of model complexity.
To make progress, while accepting proper limitations, one must bear in mind the much-quoted observation of \citet{box79} that ``all models are wrong but some are useful.''
\eic{I WAS WONDERING WHETHER TO SAY SOMETHING LIKE, This is not just good practical advice for applied statistics, but matches broader appreciation of the limitations of all scientific knowledge (REF). DO YOU THINK THIS IS USEFUL, OR A DISTRACTION?}
In this section, we discuss some tools and suggestions for diagnosing mechanistic models with the goal of making the subjective assessment of model ``usefulness" more objective by relying on the quantitative and statistical ability of the model to match the observed data, which we call the model's goodness-of-fit
\eic{GOODNESS-OF-FIT SEEMED INITIALLY TO ME A MEASURE OF "RIGHT VS WRONG" RATHER THAN "USEFUL VS USELESS". I THINK NOW I SEE YOUR POINT: GOODNESS OF FIT CAN GIVE US SOME INDICATION OF HOW WRONG A MODEL IS, WITH THE GUIDING PRINCIPLE THAT A MODEL WHICH IS TOO FAR WRONG IS PROBABLY NOT RELIABLE FOR USEFUL PURPOSES. MAYBE WE HAVE TO EXPLAIN THIS TO THE READER?}
Goodness-of-fit may provide evidence supporting the causal interpretation of one model versus another, but cannot by itself rule out the possibility of alternative explanations.

One common approach to assess a mechanistic model's goodness-of-fit is to compare simulations from the fitted model to the observed data.
Visual inspection may indicate defects in the model, or may  suggest that the observed data are a plausible realization of the fitted model.
While visual comparisons can be informative, they provide only a weak and informal measure of the goodness-of-fit of a model.
The study by \citet{lee20} provides an example of this: their models and parameter estimates resulted in simulations that visually resembled the observed data, yet resulted in model likelihoods that were---in some cases---remarkably smaller than likelihoods that can be achieved via the likelihood based optimization techniques that were used (see Table~\ref{tab:likes}).
Alternative forms of model validation should therefore be used in conjunction with visual comparisons of simulations to observed data.

Another approach is to compare a quantitative measure of the model fit (such as MSE, predictive accuracy, or model likelihood) among all proposed models.
These comparisons provide insight into how each model performs relative to the others.
To calibrate relative measures of fit, it is useful to compare against a model that has well-understood statistical ability to fit data, and we call this model a {\it benchmarks}.
Standard statistical models, interpreted as associative models without requiring any mechanistic interpretation of their parameters, provide suitable benchmarks. 
Examples include linear regression, auto-regressive moving average time series models, or even independent and identically distributed measurements.
The benchmarks enable us to evaluate the goodness of fit that can be expected of a suitable mechanistic model.

Goodness-of-fit alone does not guarantee that a model provides a correct causal interpretation of the model.
Indeed, associative models are not constrained to have a causal interpretation, and typically are designed with the sole goal of providing a statistical fit to data.
Consequently, we should not require a candidate mechanistic model to beat all benchmarks.
However, a mechanistic model which falls far short against benchmarks is evidently failing to explain some substantial aspect of the data.
A convenient measure of fit should have interpretable differences that help to operationalize the meaning of far short.
Ideally, the measure should also have favorable theoretical properties.
Consequently, we focus on log-likelihood as a measure of goodness of fit, and we adjust for the degrees of freedom of the models to be compared by using the Akaike information criterion (AIC) \eic{REF}.

It should be universal practice to present measures of goodness of fit for published models, and mechanistic models should be compared against benchmarks.
This alone would assist authors and readers to confront any major statistical limitations of the proposed mechanistic models.
In addition, the published goodness of fit provides a concrete measure for subsequent research to identify and remedy limitations in the analysis, or to update the investigation based on new data or new scientific understanding.
When combined with online availability of data and code, objective measures of fit provide a powerful tool to accelerate scientific progress, following the paradigm of the {\it common task framework} \citep[][Sec.~6]{donoho17}.
In our literature review of the Haiti cholera epidemic \eic{CHECK THIS. HOW/WHERE SHOULD WE DESCRIBE THIS?} no quantitative measures of goodness of fit, and no benchmark models, were considered in any of the ?? papers which calibrated a mechanistic model to data in order to obtain scientific conclusions.


In some cases, a possible benchmark model could be a generally accepted mechanistic model, but often no such model is available.
Because of this, we use a log-linear Gaussian ARMA model as an associative benchmark, as recommended by \citet{he10}.
The theory and practice of ARMA models is well developed, but the exponential growth and decay characteristic of biological dynamics suggests applying these linear models on a log scale.


The use of a common benchmark may also be beneficial when developing models with varying spatial scale, as direct comparisons between models fit to data with different levels of spatial aggregation are meaningless (e.g. comparing Model~1 to Model~3).
\eic{I DON'T QUITE UNDERSTAND THIS YET. BENCHMARKS TEND TO APPLY TO SPECIFIC DATA SETS, SO CANNOT READILY BE USED TO COMPARE DIFFERENT LEVELS OF AGGREGATION. LIKELIHOOD BENCHMARKS CAN BE USED TO STUDY THE VALUE OF EXPLICIT SPATIAL COUPLING. HOW TO BENCHMARK ACROSS AGGREGATION LEVELS IS, I THINK, SOMETHING OF AN OPEN PROBLEM.}
Likelihoods of Models~1--3 and their respective ARMA benchmark models are provided in Table~\ref{tab:likes}.

<<Table 2 Input, echo=FALSE, eval=TRUE>>=
NREPS_EVAL <- switch(RUN_LEVEL,  3,   8,    36)
NP_EVAL    <- switch(RUN_LEVEL, 50, 500, 10000)

# Objective function of model 2, epidemic phase
epi_ofun <- traj_objfun(
  h2_epi,
  params = h2_fit$h2_params
)

mod2_epi_ll <- -epi_ofun(par = h2_fit$h2_params)
# mod2_end_ll <- -end_ofun(par = h2_fit$end_params)


mod2_ll <- mod2_epi_ll - sum(log(h2_epi@data + 1), na.rm = TRUE)

# Evaluate and sum objective functions to get total model likelihood
# mod2_ll <- -(epi_ofun(par = h2_fit$epi_params) + end_ofun(par = h2_fit$end_params))

arima_lik2 <- 0

# model 2 AIC
mod2_n_params <- 21  # TODO: adjust Number of parameters fit in Model 2
mod2_aic <- 2 * mod2_n_params - 2 * mod2_ll

# Load pre-calculated model 1 likelihood
mod1_ll <- h1_ll['ll']  # Comes from model1/output/recal_mcap_results.rda

# model 1 AIC
mod1_n_params <- 13
mod1_aic <- 2 * mod1_n_params - 2 * mod1_ll

# Load pre-calculated model 3 likelihood
mod3_ll <- ll_mod3 %>%
  arrange(-logLik) %>%
  slice_head(n = 1) %>%
  pull(logLik)

# model 3 AIC
mod3_n_params <- 39
mod3_aic <- 2 * mod3_n_params - 2 * mod3_ll

h3Spat <- haiti3_spatPomp()
coef(h3Spat) <- haitipkg:::panelParms_toSpatParms3(p3)

# Calculate likelihood via block-particle filter
mod3_spatPomp_res <- bake(
  file = paste0('model3/', rl_dir, 'SpatPomp_res.rds'), {

    # Create a list to store results
    results <- list()

    # Perform block-particle filter
    h3_bpf <- foreach(i = 1:NREPS_EVAL, .combine = c) %dopar% {
      bpfilter(h3Spat, Np = NP_EVAL, block_size = 1)
    }


    ll <- logLik(h3_bpf)
    results$ll <- logmeanexp(ll[!is.na(ll)], se = FALSE)

    ### Get likelihood for subset of the data

    # Find appropriate sub-set
    in_subset_cols <- h3_bpf[[1]]@times >= as.numeric(dateToYears(as.Date("2014-03-01")))
    in_subset_rows <- h3_bpf[[1]]@unit_names != 'Ouest'

    mod3_coupled_subset_evals <- c()
    for (i in 1:length(pf)) {
      mod3_coupled_subset_evals <- c(
        mod3_coupled_subset_evals,
        sum(h3_bpf[[i]]@block.cond.loglik[in_subset_rows, in_subset_cols])
      )
    }

    results$subset_ll <- logmeanexp(mod3_coupled_subset_evals)
    results
  },
  timing = FALSE
)

mod3_spatPomp_n_params <- 39
mod3_spatPomp_aic <- 2 * mod3_spatPomp_n_params - 2 * mod3_spatPomp_res$ll
mod3_spatPomp_res$subset_aic <- 2 * mod3_spatPomp_n_params - 2 * mod3_spatPomp_res$subset_ll

rm(NREPS_EVAL, NP_EVAL)
@

<<CalculateARMA, echo=FALSE, message=FALSE, include=FALSE, cache=TRUE>>=

ARMA_benchmarks <- list()

###
### Model 1 ###
###

# Load aggregated data
m1_agg_data <- haiti1_agg_data()

# Fit ARMA(2, 1) on log-cases
m1_log_arma <- arima(log(m1_agg_data$cases + 1), order = c(2, 0, 1))

# Fit ARMA(2, 1) on natural scale of cases
m1_arma <- arima(m1_agg_data$cases, order = c(2, 0, 1))

# Check which one is better, and save benchmark as the better one
if (m1_log_arma$loglik - sum(log(m1_agg_data$cases + 1), na.rm = TRUE) >= m1_arma$loglik) {
  ARMA_benchmarks[['m1']] <- m1_log_arma$loglik - sum(log(m1_agg_data$cases + 1), na.rm = TRUE)
} else {
  ARMA_benchmarks[['m1']] <- m1_arma$loglik
}

rm(m1_agg_data, m1_log_arma, m1_arma)
gc()

###
### Model 2 ###
###

h2_data <- haiti2_data()

# model 2 ARIMA benchmark
h2_dep_names <- h2_data$department %>% unique()
m2_benchmark <- 0
for (i in 1:10){
  cases <- h2_data[h2_data$department == h2_dep_names[i], ]$cases
  log_cases <- log(cases + 1)

  m2_arma_dep_ll <- arima(cases, order=c(2, 0, 1))$loglik
  m2_log_arma_dep_ll <- arima(log_cases, order=c(2, 0, 1))$loglik - sum(log_cases, na.rm = TRUE)

  m2_benchmark <- m2_benchmark + max(m2_arma_dep_ll, m2_log_arma_dep_ll)
}

ARMA_benchmarks[['m2']] <- m2_benchmark

rm(m2_benchmark, h2_dep_names, cases, log_cases, m2_arma_dep_ll, m2_log_arma_dep_ll, h2_data)
gc()

###
### Model 3 ###
###

MODEL3_CASES <- haitiCholera %>%
  dplyr::rename(
    date = date_saturday, Grande_Anse = Grand.Anse,
    Nord_Est = Nord.Est, Nord_Ouest = Nord.Ouest,
    Sud_Est = Sud.Est
  ) %>%
  dplyr::mutate(date = as.Date(date)) %>%
  dplyr::select(-report) %>%
  dplyr::mutate(time_diff = date - lag(date))

jumps <- which(MODEL3_CASES$time_diff > lubridate::make_difftime(day = 7))

temp <- MODEL3_CASES
for (j in 1:length(jumps)) {
  temp <- rbind(
    temp[1:(jumps[j] + j - 2), ],
    rep(NA, ncol(temp)),
    temp[(jumps[j] + j - 1):nrow(temp), ]
  )
}

mod3_cases <- temp %>%
  dplyr::select(-time_diff)

mod3_departements <-
  c(
    'Artibonite',
    'Centre',
    'Grande_Anse',
    'Nippes',
    'Nord',
    'Nord-Est',
    'Nord-Ouest',
    'Ouest',
    'Sud',
    'Sud-Est'
  )

m3_benchmark <- 0
for (dep in mod3_departements) {

  # Get department cases
  cases <- as.numeric(mod3_cases[, gsub("-", "_", dep)])

  # Fit model on natural scale of cases
  m3_a201 <- arima(cases, order = c(2, 0, 1))

  # Fit model on log-scale of cases
  m3_log_a201 <- arima(log(cases + 1), order = c(2, 0, 1))

  m3_benchmark <- m3_benchmark + max(
    m3_log_a201$loglik - sum(log(cases + 1), na.rm = TRUE),
    m3_a201$loglik
  )
}

ARMA_benchmarks[['m3']] <- m3_benchmark
rm(cases, m3_a201, m3_log_a201, m3_benchmark, mod3_departements, dep,
   mod3_cases, temp, jumps, MODEL3_CASES, i, j)
gc()
@

<<Table 2 Lee Input, echo=FALSE, include=FALSE>>=
mod1_lee_n_params <- 20
mod1_lee_aic <- 2 * mod1_lee_n_params - 2 * mod1_lee_ll

# load('model2/output/mod2_leeFit.rda')
mod2_lee_aic <- 2 * h2_fit$leeFit_n_params - 2 * h2_fit$leeFit_ll
# mod2_lee_aic <- 2 * mod2_leeFit_num_parms - 2 * mod2_leeFit_ll
@

<<Lee3 Sub Calculations, echo=FALSE, include=FALSE, message=FALSE>>=
lee3_NP   <- switch(RUN_LEVEL, 50, 250, 5000)
lee3_NREP <- switch(RUN_LEVEL,  3,   8,   36)

registerDoRNG(49637421)

# Create Lee et al (2020a) version of model 3 on sub-data
lee3_sub <- haiti3_lee_et_al()

# This model is only fit for a subset of the data, so we will set data
# that are not part of this subset as NA. This will allow us to estimate
# the likelihood for only the values that were considered during the
# parameter estimation phase.
set_as_na <- lee3_sub@times < dateToYears(as.Date('2017-06-10'))
temp_ouest_cases <- lee3_sub@data['casesOuest', ]
temp_ouest_cases[set_as_na] <- NA_real_
lee3_sub@data['casesOuest', ] <- temp_ouest_cases

lee3_all <- haiti3_lee_et_al(start_time = "2010-10-23")

rm(set_as_na, temp_ouest_cases)
gc()

lee3_results <- bake(
  file = paste0('model3/', rl_dir, 'lee3_results.rds'), {

    # Create lists to save results
    lee3_results <- list()

    # Save how long it takes for sub-data
    lee3_results[["sub_pf_time"]] <- system.time({

      # Save all likelihood evaluations
      lee3_results[["sub_ll_evals"]] <- foreach(
        i = 1:lee3_NREP, .combine = c
      ) %dopar% {
        logLik(pfilter(lee3_sub, Np = lee3_NP))  # Get likelihood evaluations
      }
    })

    registerDoRNG(2789131)

    # Save how long it takes for all data
    lee3_results[["all_pf_time"]] <- system.time({

      # Save all likelihood evaluations
      lee3_results[["all_ll_evals"]] <- foreach(
        i = 1:lee3_NREP, .combine = c
      ) %dopar% {
        logLik(pfilter(lee3_all, Np = lee3_NP))  # Get likelihood evaluations
      }
    })

    lee3_results[["n_params"]] <- 29

    lee3_results
  },
  timing = FALSE
)

rm(lee3_sub, lee3_all, lee3_NP, lee3_NREP)
gc()

mod3_lee_coupled_aic <- 2 * lee3_results$n_params - 2 * max(lee3_results$all_ll_evals)

mod3_lee_coupled_partial_aic <- 2 * lee3_results$n_params - 2 * max(lee3_results$sub_ll_evals)

@

\begin{table}[ht]
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
 & \thead{Model~1} & \thead{Model~2} & \thead{Model~3 \\ (panelPOMP)} & \thead{Model~3 \\ (SpatPOMP)}
\\
\hline
\hline
\multirow{2}{*}{Log-likelihood} &
  $\Sexpr{myround(mod1_ll, 1)}$ &
  $\Sexpr{myround(mod2_ll, 1)}$ &
  \multirow{2}{*}{$\Sexpr{myround(mod3_ll[1], 1)}$\footnotemark[2]} &
  $\Sexpr{myround(mod3_spatPomp_res$ll, 1)}$ \\
    & ($\Sexpr{myround(mod1_lee_ll, 1)}$)\footnotemark[1] &
  ($\Sexpr{myround(h2_fit$leeFit_ll, 1)}$) &
  & ($\Sexpr{myround(max(lee3_results[["all_ll_evals"]]), 1)}$)\footnotemark[3]
\\
\hline
Number of &
  $\Sexpr{as.character(mod1_n_params)}$ & $\Sexpr{as.character(mod2_n_params)}$ & $\Sexpr{as.character(mod3_n_params)}$ & $\Sexpr{mod3_spatPomp_n_params}$ \\
 Fit Parameters & (\Sexpr{as.character(mod1_lee_n_params)}) & ($\Sexpr{as.character(h2_fit$leeFit_n_params)}$) & (29) & ($\Sexpr{as.character(lee3_results$n_params, 1)}$)
\\
\hline
\multirow{2}{*}{AIC} &
  $\Sexpr{myround(mod1_aic, 1)}$ & $\Sexpr{myround(mod2_aic, 1)}$ & \multirow{2}{*}{$\Sexpr{myround(mod3_aic, 1)}$\footnotemark[2]} & $\Sexpr{myround(mod3_spatPomp_aic, 1)}$ \\
  & ($\Sexpr{myround(mod1_lee_aic, 1)}$)\footnotemark[1] & ($\Sexpr{myround(mod2_lee_aic, 1)}$) &  & ($\Sexpr{myround(mod3_lee_coupled_aic, 1)}$)\footnotemark[3]
\\
\hline
\thead{Log-ARMA(2,1) \\ Log-likelihood} &
  $\Sexpr{myround(ARMA_benchmarks$m1, 1)}$ &
  $\Sexpr{myround(ARMA_benchmarks$m2, 1)}$ &
  $\Sexpr{myround(ARMA_benchmarks$m3, 1)}$ &
  $\Sexpr{myround(ARMA_benchmarks$m3, 1)}$
\\
\hline
\end{tabular}
\caption{\label{tab:likes}Log-likelihood values for each models compared to their ARMA benchmarks.
Values in parenthesis are corresponding values using \citet{lee20} parameter estimates.
\eic{I THINK THAT THE NOTES SHOULD GO IN THE CAPTION RATHER THAN BEING FOOTNOTES.}
\eic{WE SHOULD REVIEW THIS TABLE IN A MEETING. RIGHT NOW, I'M SURPRISED BY A FEW THINGS. WHY IS THE LEE ET AL MODEL 2 LIKELIHOOD HIGHER THAN OURS? IS THE (29) IN THE PANELPOMP COLUMN A TYPO, SINCE WE DON'T COMPARE TO LEE ET AL IN THIS COLUMN? NESTING RELATIONSHIP FOR THE SPATPOMP VERSION OF MODEL 3 MAKES IT HARD TO EXPLAIN THE RATHER LARGE DECREASE IN LOGLIK. AT FACE VALUE, THIS SEEMS TO MEAN THAT THE COUPLING IS SUFFICIENTLY POORLY SPECIFIED THAT WE ARE BETTER OFF WITHOUT IT. I SEEM TO RECALL THAT OUR UNDERSTANDING INVOLVES THE WAY PANELPOMP USES A COVARIATE THAT DEPENDS ON CASES IN OTHER DEPARTMENTS. SINCE LEE ET AL ALSO DO THAT, COULD IT BE THAT THEIR ANALYSIS IS MORE CLOSELY RELATED TO THE PANELPOMP VERSION THAN THE SPATPOMP VERSION?}
}
\end{table}

\jwc{NEW PARAGRAPH: }
Similar to comparing log-likelihoods across models, an additional powerful diagnosis tool is the comparison of conditional log-likelihoods.
Conditional likelihoods, defined as the density $f_{Y_k | Y_1, \ldots, Y_{k - 1}}\big(Y_k = y_k^* | y_{1:k-1}^*\big)$, provide a basic description of how well the proposed model can describe each data point, given the previous observations.
Comparing these results across models---including benchmark models---can help researchers identify potential model deficiencies, or errors in the observed data.
Additional tools for assessing the goodness-of-fit of a model include plotting the effective sample size of each observation \jwc{REF???} and comparing any statistic of the observed data to simulations from the model, which is sometimes referred to as diagnostic probes (for example, the autocorrelation function (ACF), as was done in \cite{king15}).\footnotetext[1]{The reported likelihood is an upper bound of the likelihood of the \citet{lee20} model as it is the largest likelihood obtained using their parameter calibration regime.}
\footnotetext[2]{Parameters to department-specific models not provided by \citet{lee20}, as department fits were only used as an intermediary step to obtain parameter estimates of the coupled model.}
\footnotetext[3]{Model 3 was originally fit to only a subset of the data starting from March 2014 and did not include a large portion of data from Ouest in 2015-2016.
On this subset, the parameters provided by \cite{lee20} achieved a likelihood of $\Sexpr{myround(max(lee3_results$sub_ll_evals), 1)}$.
On this same subset of data, our model achieved a likelihood of $\Sexpr{myround(mod3_spatPomp_res$subset_ll, 1)}$.}

\subsection{Forecasts}\label{sec:filter}

The central goal of a forecast is to provide an accurate estimate of the future state of a system based on currently available data.
When a mechanistic model is used, forecasts may also provide estimates of the future effects of potential interventions.
Forecasting models are built using available scientific understanding, but forecasting can also be a way of testing new scientific hypotheses in real time \citep{lewis22}.
In order to provide trustworthy information, however, the reliability of the forecast should be undersood.
In particular, researchers should account for various forms of uncertainty present in model forecasts, and  calibrate the proposed model to observed data.


\eic{I THINK WE WILL HAVE TO EXPLAIN THIS CAREFULLY IF THE POINT IS TO APPEAR NONTRIVIAL. IT IS WORTH NOTING THAT LEE ET AL DID NOT PROPERLY FORECAST FROM THE BEST ESTIMATE OF THE CURRENT STATE. HOWEVER, IT WILL BE OBVIOUS TO MOST SATISTICAL READERS THAT THIS IS NOT THE BEST THING TO DO. IT MAY NOT BE SO OBVIOUS THAT THE DIFFERENCE IS IMPORTANT IF AND ONLY IF A DETERMINISTIC MODEL IS INADEQUATE. THE QUESTION OF WHETHER TO USE A DETERMINISTIC MODEL REMAINS OF CURRENT INTEREST...}
As an example, we note that simulations from a well-fit mechanistic model may closely resemble the observed data $y_{1:N}^*$.
In such comparisons, these simulations are random draws from the complete estimated joint distribution $f_{\bm{X}^{(\modelCounter)}_{0:N}, \bm{Y}^{(\modelCounter)}_{1:N}}\left(x_{0:N}, y_{1:N}\mid \hat{\paramVec}\right)$, where $\modelCounter$ indexes the model used for simulations.
It can therefore be tempting to use simulations from this model up to time $N+s$, $f_{\bm{X}^{(\modelCounter)}_{0:N+s}, \bm{Y}^{(\modelCounter)}_{1:N+s}}\left(x_{0:N+s}, y_{1:N+s}\mid \hat{\paramVec}\right)$, with $s\geq 0$ to project the dynamic system up to a future time $N+s$, as it has been done in previous studies \citep{lee20,???}.
This approach, however, does not take advantage of the information about the state of the system that is contained in the observed data.
Note that each of the models in question (and, more generally, all models that are described as POMPs) are Markovian, that is, the history of the process $\{\bm{X}^{(\modelCounter)}_s, s\leq t\}$ for model $\modelCounter$ is uninformative about the future of the process $\{\bm{X}^{(\modelCounter)}_s, s\geq t\}$, given the current state $\bm{X}^{(\modelCounter)}_t$.
In other words, observing the data $\bm{Y}^{(\modelCounter)}_N = y_N^*$ at time $N$ provides more information about the future state of the system than the initial conditions.
In this case, draws from the conditional density $f_{\bm{X}^{(\modelCounter)}_{N:s}, \bm{Y}^{\modelCounter}_{N:s}}\big(x_{N:s}, y_{N:s}\mid \hat{\paramVec}, \bm{X}^{(\modelCounter)}_{N} = \bm{x}^{(\modelCounter)}_N\big)$ should be preferred as forecasts, as these simulations account for the most recent known state of the system.

$\bm{X}^{(\modelCounter)}_N$, however, is unobservable and therefore draws from the desired conditional density are unobtainable.
Informed estimates of $\bm{X}^{(\modelCounter)}_N$ given the observed data $\bm{Y}^{(\modelCounter)}_{1:N} = y_{1:N}^*$ can easily be obtained, however, via the filtering distribution.
Let $\hat{\bm{X}}^{(\modelCounter), i}_N, \, i \in {1, 2, \ldots, J}$ be $\iid$ draws from the filtering distribution at time $N$, with density $\hat{\bm{X}}^{(\modelCounter), i}_N \sim f_{\bm{X}^{(\modelCounter)}_N|\bm{Y}^{(\modelCounter)}_{1:N}}(x_N \mid \hat{\paramVec}, y_{1:N}^*)$.
A single model forecast can then be obtained by simulating from the model $f_{\bm{X}^{(\modelCounter)}_{N:s}, \bm{Y}^{(\modelCounter)}_{N:s}}\big(x_{N:s}, y_{N:s}\mid \hat{\paramVec}, \bm{X}^{(\modelCounter)}_{N} = \hat{\bm{X}}^{(\modelCounter), i}_N\big)$.
Intuitively, simulating the model starting at the filtering distribution of the most recently available time point is a more appropriate way to project a stochastic dynamic system into the future, as it is not expected that each simulation from initial conditions will result in a latent state at time $N$ consistent with the model and the observation $\bm{Y}^{(\modelCounter)}_N = y_N^*$.
In this particular case study, projecting the future state of the cholera epidemic in Haiti starting from the draws from the filtering distribution allows the proposed models to benefit from the fact that very few cholera cases had been observed between 2018 and 2019, and that cases appear to be decreasing (i.e., the number of susceptible individuals may be small).

Another consideration to make when obtaining model forecasts is that of parameter uncertainty.
It has been noted that the uncertainty in just a single parameter can lead to drastically different projections \citep{saltelli20}.
One possible approach to account for parameter uncertainty in model forecasts is by obtaining confidence intervals for each parameter, sampling parameters from the confidence intervals, simulating the model with the resulting parameter set, and then weighing the resulting model projections based on the likelihood of the given set of parameters, as was done in \citet{king15}.
\eic{THE ``OBVIOUS'' THING TO DO HERE IS A BAYESIAN APPROACH. IN SOME SENSE, KING15 IS EMPIRICAL BAYES. IT MAY TAKE CARE TO DISCUSS THIS WITHOUT OPENING UP A CAN OF WORMS - I CAN HAVE A GO AT MAKING SUGGESTIONS, NEXT TIME I'M WORKING ON THE MS.}

Note that in order to obtain projections that are consistent with the observed data, one must first be able to sample from the filtering distribution given each set of parameters.
This approach can be done for both deterministic and stochastic models, but requires a large number of computations, especially as the number of observations and model parameters increase.
Because the focus of this study is on model fitting and evaluation, we do not provide model projections accounting for parameter uncertainty.
Instead, we use the projections from point estimates to highlight a major deficiency of deterministic models, which is that the only variability in model projections is a result of parameter uncertainty, which leads to over-confidence in the projections.
This observation is consistent with those made in \cite{king15}, and suggests that stochastic models should be preferred over deterministic models.

\jwc{Replaced Ed's comment with this: }
We note---the credit of deterministic models---that of the four fitted models in \cite{lee20}, Model~2 provides the most apparently accurate forecasts.
This perhaps demonstrates that while deterministic models describe the systems in a less realistic and useful way, the relative ease in fitting these models potentially results in fewer modeling errors.
As the data analysis becomes more refined, however, the deficiencies of deterministic models become increasingly apparent.

\section{Results}\label{sec:results}

\jwc{SHOULD THESE FIRST TWO PARAGRAPHS BE MOVED TO THE DISCUSSION?}
\eic{YES. ALSO, THERE ARE RESULTS IN SEC.~3. PERHAPS, THE RAINFALL ANALYSIS IN SEC 4 COULD BE ANOTHER ASPECT OF THE SEC 3 ANALYSIS: COMPARISON OF MODEL FIT WITH OTHER LINES OF SCIENTIFIC EVIDENCE. THEN THE CURRENT SEC 4 COULD BE DISBANDED. 
SOMETIMES THE METHODS-RESULTS-DISCUSSION FORMAT DOES NOT FIT SO NATURALLY WITH APPLIED STATS.
WE CAN DISCUSS THIS. ONE IDEA IS TO HAVE A SECTION NAMED SOMETHING LIKE "LESSONS LEARNED FROM MODELING THE HAITI EPIDEMIC" OR "ADVICE FOR FUTURE STUDIES". THE LATTER CAN SEEM PRESUMPTUOUS, AND SHOULD BE DONE WITH DUE CAUTION, BUT FROM ANOTHER PERSPECTIVE IF WE ARE NOT PROVIDING CONCRETE GUIDANCE FOR FUTURE WORK THEN WHAT ARE WE DOING? OUR RESPONSIBILITY IS TO MAKE THE STRONGEST CLAIM WE THINK WE CAN PROPERLY SUPPORT; THEN THE REFEREES CAN HAVE A GO AT PULLING THE CLAIM DOWN.}
\eic{I THINK IT IS INTERESTING THAT THE MODEL-SUPPORTED SUGGESTION BY \citep{andrews11} HAS LARGELY PROVED ACCURATE (VACCINATION, NON-PHARMACEUTICAL INTERVENTIONS, AND INFRASTRUCTURE IMPROVEMENTS ALL HAVE A ROLE AND SHOULD BE FOLLOWED SIMULTANEOUSLY)DESPITE LEGITIMATE CONCERNS ABOUT WHETHER THE SCIENTIFIC SUPPORT FOR THE MODEL IS STRONG ENOUGH TO ROBUSTLY GUIDE POLICY \citep{grad12}.
THE MAIN CLAIM OF ANDREWS11 IS THAT FORECASTS AND POLICY NOT SUPPORTED BY A MODEL SEEM EVEN MORE UN-ROOTED THAN RECOMMENDATIONS BASED ON A WEAKLY SUPPORTED MODEL.
THE CONCLUSION FROM ALL THIS, WHICH I THINK IS SUPPORTED BY OUR RE-ANALYSIS OF LEE ET AL, IS THAT MECHANISTIC MODELS FITTED TO DATA ARE A STRONG WAY TO ENFORCE CONSISTENT THINKING, BUT WEAKER AS A WAY TO CHECK THAT YOU ARE ACTUALLY RIGHT DUE TO THE LIMITATIONS OF INFERRING CAUSAL MECHANISMS FROM OBSERVATIONAL DATA.
POLICY-MAKERS SHOULD FOLLOW AN APPROACH WHICH IS SELF-CONSISTENT AND CONSISTENT WITH PREVIOUS EVIDENCE THAT MIGHT WORK (WHICH MODELS CAN HELP WITH) BUT THEY MUST BE AGILE AS NEW EVIDENCE EMERGES.
MODELS, TOGETHER WITH DATA ANALYSIS, CAN ALSO HELP REVEAL WHEN THE EVIDENCE SUPPORTING A P0LICY IS BREAKING DOWN, FORCING A RE-THINK.
}

A model which aspires to provide quantitative guidance for assessing interventions should provide a quantitative statistical fit for available data.
However, strong statistical fit does not guarantee a correct causal structure: it does not even necessarily require the model to assert a causal explanation.
A causal interpretation is strengthened by corroborative evidence.
For example, reconstructed latent variables (such as numbers of susceptible and recovered individuals) should make sense in the context of alternative measurements of these variables;
parameter values which fit the data should make sense in the context of alternative lines of evidence about the phenomena being modeled.
% \jwc{Perhaps add comment on our final parameter estimates, and remove the following sentences in this paragraph.}
% A model is necessarily a simplified approximation of a complex process, and so it is unreasonable to expect models (describing and fitted to population-level processes) to perfectly match quantitative understanding of individual-level processes.
% Such discrepancies could lead to biases when interventions (modeled as effects on individuals) are included in the population-level, unless there are historical occurrences of the intervention which enable the consequence to be calibrated at the population level.

If a mechanistic model including a feature (such as a representation of a mechanism, or the inclusion of a covariate) fits better than mechanistic models without that feature, and also has competitive fit compared to associative models, this may be taken as evidence supporting the scientific relevance of the feature.
As for any analysis of observational data, we must be alert to the possibility of confounding.
For a covariate, this shows up in a similar way to regression analysis: the covariate under investigation could be a proxy for some other unmodeled or unmeasured covariate.
For a mechanism, the model feature could in principle explain the data by helping to account for some different unmodeled phenomenon.
% Assessing potential confounding is part of building an argument for a causal interpretation of a fitted model.
In the context of our analysis, the estimated trend in transmission rate could be explained by any trending variable (such as hygiene improvements, or changes in population behavior), resulting in confounding from colinear covariates.
Alternatively, the trend could be attributed to a decreasing reporting rate rather than decreasing transmission rate, resulting in confounded mechanisms.
The robust statistical conclusion is that a model which allows for change fits better than one which does not---we argue that a decreasing transmission rate is a plausible way to explain this, but the incidence data themselves do not provide enough information to pin down the mechanism.

In a similar fashion, one can take advantage of certain mechanistic features contained in a particular model in order to make inference on a system.
Examples of this are as diverse as estimating the effective reproductive number ($R_0$) of an infectious disease \citep{he10} or investigating interactions between pedestrians and autonomous vehicles \citep{domeyer22}.
In our analysis, we demonstrate this ability by examining the results of fitting the flexible cubic spline term in Model~1 (Eq.~\myeqref{model1:lambda}--\myeqref{model1:beta}), which allows for a flexible estimation of seasonality in the force of infection.
After fitting the model, we explore potential patterns in the seasonal transmission rate by plotting the average value of $\transmission$ in a typical year.
Fig.~\ref{fig:h1SeasRain} shows that the estimated seasonal transmission rate $\transmission$ mimics the rainfall dynamics in Haiti, despite Model~1 not having access to rainfall data.
This result provides evidence that rainfall is a potential driver of cholera infections in Haiti.

<<Get Model 1 Seasonality>>=
plot_model_fun <- function(mod) {
  params <- coef(mod)

  beta1 <- params['beta1']
  beta2 <- params['beta2']
  beta3 <- params['beta3']
  beta4 <- params['beta4']
  beta5 <- params['beta5']
  beta6 <- params['beta6']
  betat <- params['betat']

  get_beta <- function(time) {

    covar <- t(mod@covar@table)
    covar_df <- as.data.frame(covar)
    covar_df$times <- mod@covar@times

    betas <- matrix(c(beta1, beta2, beta3, beta4, beta5, beta6), nrow = 1)
    si <- matrix(unlist(covar_df[time, -7]), ncol = 1)
    if (time <= 430) {
      return(as.numeric(exp(betas %*% si + betat * ((time - 215) / (430-215)))))
    } else {
      return(as.numeric(exp(betas %*% si + betat)))
    }
  }

  return(get_beta)
}

get_beta1 <- plot_model_fun(h1)

h1_seas_df <- data.frame(
  'week' = 431:800,
  'date' = lubridate::ymd("2010-10-16") + lubridate::weeks(431:800),
  'trans' = purrr::map_dbl(431:800, get_beta1)
) %>%
  mutate(year = lubridate::year(date)) %>%
  filter(year == 2020) %>%
  mutate(trans_std = (trans - min(trans)) / (max(trans) - min(trans)))

std_rain <- function(x) {
  # This function simply standardizes the rain for us.
  x / max(x)
}

df_rain <- haitiRainfall %>%
  dplyr::summarize(
    date = date, dplyr::across(Artibonite:`Sud-Est`, std_rain)
  ) %>%
  pivot_longer(
    data = .,
    cols = -date,
    names_to = 'dep',
    values_to = 'rainfall'
  ) %>%
  mutate(
    year = lubridate::year(date),
    week = lubridate::week(date)
  ) %>%
  group_by(year, week) %>%
  summarize(national_weekly_rain = sum(rainfall)) %>%
  ungroup() %>% # Not needed, but good practice
  mutate(
    date_week_start = lubridate::ymd(paste0(2014, "-01-01")) + lubridate::weeks(week - 1),
    week_date = as.Date(date_week_start, format = "%m-%d"),
    std_national_rain = (national_weekly_rain - min(national_weekly_rain, na.rm = TRUE)) / (max(national_weekly_rain, na.rm = TRUE) - min(national_weekly_rain, na.rm = TRUE))
  )

mean_rain <- df_rain %>%
  group_by(week_date) %>%
  summarize(mean_rain = mean(std_national_rain)) %>%
  ungroup() %>%
  mutate(mean_rain = (mean_rain - min(mean_rain)) / ((max(mean_rain) - min(mean_rain))))

gg_rain <- df_rain %>%
  filter(week != 1, week != max(week)) %>%
  ggplot(aes(x = week_date, y = std_national_rain, group = year, col = year)) +
  geom_line() +
  theme_bw() +
  guides(color = 'none') +
  ylab('Standardized Weekly\nNational Rain') +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 7.5),
        axis.title.y = element_text(size = 9))

gg_trans <- ggplot() +
  geom_line(data = h1_seas_df, aes(x = date, y = trans_std), linetype = 'dashed') +
  geom_line(
    data = mean_rain,
    aes(x = week_date + lubridate::years(6), y = mean_rain),
    col = '#2166ac'
  ) +
  scale_x_date(date_labels = "%b", date_breaks = '1 month') +
  ylab('Standardized Seasonal\nContact Rate') +
  theme(axis.title.x = element_blank(),
        axis.text = element_text(size = 7.5),
        axis.title.y = element_text(size = 9))

@

\begin{figure}[ht]
\centering
<<Model1_Seasonality_Figure, fig.height=2.9, fig.width=5, fig.align='center'>>=
cowplot::plot_grid(gg_rain, gg_trans, align = "v", ncol = 1)
@
\caption{\label{fig:h1SeasRain}
(Top) weekly rainfall in Haiti, lighter colors representing more recent years.
(Bottom) estimated seasonality in the transmission rate (dashed line) plotted alongside mean rainfall (solid line).
% Observed spikes in rainfall correspond to peaks in the seasonal transmission $\transmission$ in Model~1; similarly, lower values of $\transmission$ correspond to the dry periods.
}
\end{figure}

While such inference is useful, one must be careful not to assume the results of a single analysis as a de facto feature of the system.
It is important instead to recognize and assess the modeling simplifications and assumptions that were used in order to arrive at the conclusions.

An additional benefit of mechanistic modeling is ability to simulate various interventions on a system; this feature is useful to inform policy and was the primary goal of \citet{lee20}.
Outcomes of their study include estimates for the probability of cholera elimination and cumulative number of cholera infections under several possible vaccination scenarios.
Mimicking their efforts, we define cholera elimination as having less than one infection of cholera over at least 52 consecutive weeks in the 10-year projection period, and provide forecasts under the following vaccination scenarios:

\begin{itemize}
  \item[$V0$:] No additional vaccines are administered.
  \item[$V1$:] Vaccination limited to the departments of Centre and Artibonite, deployed over a two-year period.
  \item[$V2$:] Vaccination limited to three departments: Artibonite, Centre, and Ouest deployed over a two-year period.
  \item[$V3$:] Countrywide vaccination implemented over a five-year period.
  \item[$V4$:] Countrywide vaccination implemented over a two-year period.
\end{itemize}

<<Load Model 1 VaccScen Sims, message=FALSE, include=FALSE>>=

# Number of particles and simulations to use for pfilter and project from filter, respectively.
h1_scen_NP   <- switch(RUN_LEVEL, 50, 500, 5000)
h1_scen_sims <- switch(RUN_LEVEL, 20, 100,  500)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
h1_S0_sims <- bake(
  file = paste0("model1/", rl_dir, "S0_sims.rds"), {

    # Create the model
    s0 <- haiti1_joint(vacscen = 'id0')

    # Load coefficients
    coef(s0) <- p1

    # Get samples from filtering distribution
    s0_pf <- pfilter(s0, save.state = TRUE, Np = h1_scen_NP)

    # project vacc scenario, with filtering dist, and model.
    project_from_filter2(mod = s0, PF = s0_pf, nsims = h1_scen_sims)
  },
  timing = FALSE
)

# Clean-up simulations for analysis/visualization
mod1_V0_sims <- agg_mod1_sims(h1_S0_sims)
mod1_V0_probs <- get_elimProbs(h1_S0_sims, model = 1)
rm(h1_S0_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
h1_S2_sims <- bake(
  file = paste0('model1/', rl_dir, 'S2_sims.rds'), {

    # Create the model
    s2 <- haiti1_joint(vacscen = "id2")

    # Set model parameters that need to be adjusted
    depts <- 2
    h1_par_names <- names(p1)
    h1_params_temp <- c(p1, rep(0.0, 5 * depts))
    names(h1_params_temp) <- c(
      h1_par_names,
      paste0("S", 1:depts, "_0"),
      paste0("E", 1:depts, "_0"),
      paste0("I", 1:depts, "_0"),
      paste0("A", 1:depts, "_0"),
      paste0("R", 1:depts, "_0")
    )
    coef(s2) <- h1_params_temp  # Set model parameters

    # Get samples from filtering distribution for Scenario 2 model:
    s2_pf <- pfilter(s2, save.state = TRUE, Np = h1_scen_NP)

    # project vacc scenario, with filtering dist, and model.
    project_from_filter2(mod = s2, PF = s2_pf, nsims = h1_scen_NP)
  },
  timing = FALSE
)

# Clean-up simulations for analysis/visualization
mod1_V1_sims <- agg_mod1_sims(h1_S2_sims)
mod1_V1_probs <- get_elimProbs(h1_S2_sims, model = 1)
rm(h1_S2_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
h1_S4_sims <- bake(
  file = paste0('model1/', rl_dir, 'S4_sims.rds'), {

    # Create the model
    s4 <- haiti1_joint(vacscen = "id4")

    # Set model parameters that need to be adjusted
    depts <- 3
    h1_par_names <- names(p1)
    h1_params_temp <- c(p1, rep(0.0, 5 * depts))
    names(h1_params_temp) <- c(
      h1_par_names,
      paste0("S", 1:depts, "_0"),
      paste0("E", 1:depts, "_0"),
      paste0("I", 1:depts, "_0"),
      paste0("A", 1:depts, "_0"),
      paste0("R", 1:depts, "_0")
    )
    coef(s4) <- h1_params_temp  # Set model parameters

    # Get samples from filtering distribution for Scenario 2 model:
    s4_pf <- pfilter(s4, save.state = TRUE, Np = h1_scen_NP)

    # project vacc scenario, with filtering dist, and model.
    project_from_filter2(mod = s4, PF = s4_pf, nsims = h1_scen_NP)
  },
  timing = FALSE
)

# Clean-up simulations for analysis/visualization
mod1_V2_sims <- agg_mod1_sims(h1_S4_sims)
mod1_V2_probs <- get_elimProbs(h1_S4_sims, model = 1)
rm(h1_S4_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
h1_S3_sims <- bake(
  file = paste0('model1/', rl_dir, 'S3_sims.rds'), {

    # Create the model
    s3 <- haiti1_joint(vacscen = "id3")

    # Set model parameters that need to be adjusted
    depts <- 10
    h1_par_names <- names(p1)
    h1_params_temp <- c(p1, rep(0.0, 5 * depts))
    names(h1_params_temp) <- c(
      h1_par_names,
      paste0("S", 1:depts, "_0"),
      paste0("E", 1:depts, "_0"),
      paste0("I", 1:depts, "_0"),
      paste0("A", 1:depts, "_0"),
      paste0("R", 1:depts, "_0")
    )
    coef(s3) <- h1_params_temp  # Set model parameters

    # Get samples from filtering distribution for Scenario 2 model:
    s3_pf <- pfilter(s3, save.state = TRUE, Np = h1_scen_NP)

    # project vacc scenario, with filtering dist, and model.
    project_from_filter2(mod = s3, PF = s3_pf, nsims = h1_scen_NP)
  },
  timing = FALSE
)

# Clean-up simulations for analysis/visualization
mod1_V3_sims <- agg_mod1_sims(h1_S3_sims)
mod1_V3_probs <- get_elimProbs(h1_S3_sims, model = 1)
rm(h1_S3_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
h1_S1_sims <- bake(
  file = paste0('model1/', rl_dir, 'S1_sims.rds'), {

    # Create the model
    s1 <- haiti1_joint(vacscen = "id1")

    # Set model parameters that need to be adjusted
    depts <- 10
    h1_par_names <- names(p1)
    h1_params_temp <- c(p1, rep(0.0, 5 * depts))
    names(h1_params_temp) <- c(
      h1_par_names,
      paste0("S", 1:depts, "_0"),
      paste0("E", 1:depts, "_0"),
      paste0("I", 1:depts, "_0"),
      paste0("A", 1:depts, "_0"),
      paste0("R", 1:depts, "_0")
    )
    coef(s1) <- h1_params_temp  # Set model parameters

    # Get samples from filtering distribution for Scenario 2 model:
    s1_pf <- pfilter(s1, save.state = TRUE, Np = h1_scen_NP)

    # project vacc scenario, with filtering dist, and model.
    project_from_filter2(mod = s1, PF = s1_pf, nsims = h1_scen_NP)
  },
  timing = FALSE
)

# Clean-up simulations for analysis/visualization
mod1_V4_sims <- agg_mod1_sims(h1_S1_sims)
mod1_V4_probs <- get_elimProbs(h1_S1_sims, model = 1)
rm(h1_S1_sims, h1_scen_NP, h1_scen_sims)

gc()
@

<<Load Model 2 VaccScenarios, message=FALSE, include=FALSE>>=
# load('model2/output/mod2_vacc_scenarios.rda')

# Run Vaccination scenarios for Model 2. Note that the code runs the same
# speed for each run-level.
mod2_VaccScenarios <- bake(
  file = paste0("model2/", rl_dir, "VaccinationScenarios.rds"), {
    haiti2_vaccScenario(
      h2_params = h2_fit$h2_params
    )
  },
  timing = FALSE
)

@

<<Load Model 3 Vacc Scenarios, message=FALSE, include=FALSE>>=
NP_BPF <- switch(RUN_LEVEL, 50, 200, 1000)
NSIM   <- switch(RUN_LEVEL, 10,  50,  500)

h3_bpf <- bake(
  file = paste0("model3/", rl_dir, "h3_bpf.rds"),
  expr = {
    bpfilter(h3Spat, Np = NP_BPF, block_size = 1, save_states = TRUE)
  },
  timing = FALSE
)

mod3_V0_res <- bake(
  file = paste0("model3/", rl_dir, "mod3_V0_res"), {

    # Create a list to store results
    h3_V0_res <- list()

    # Get "noVacc" scenario parameters
    h3_S0_par <- get_model3_vacc_scenario_params(scenario = "noVacc")
    coef(h3Spat)[names(h3_S0_par)] <- h3_S0_par

    # Using parameters and filtered distribution, project system
    h3_V0_sims <- project_from_filter2(
      h3Spat, PF = h3_bpf, covarGen = project_rain,
      nsims = NSIM, seed = 6897154
    )

    # Store nationally aggregated results
    h3_V0_res$mod3_V0_sims <- agg_mod3_sims(h3_V0_sims)
    h3_V0_res$mod3_V0_probs <- get_elimProbs(h3_V0_sims, model = 3)

    # Return results
    h3_V0_res

  }
)

mod3_V1_res <- bake(
  file = paste0("model3/", rl_dir, "mod3_V1_res"), {

    # Create a list to store results
    h3_V1_res <- list()

    # Get "noVacc" scenario parameters
    h3_V1_par <- get_model3_vacc_scenario_params(scenario = "2dep")
    coef(h3Spat)[names(h3_V1_par)] <- h3_V1_par

    # Using parameters and filtered distribution, project system
    h3_V1_sims <- project_from_filter2(
      h3Spat, PF = h3_bpf, covarGen = project_rain,
      nsims = NSIM, seed = 6897154
    )

    # Store nationally aggregated results
    h3_V1_res$mod3_V1_sims <- agg_mod3_sims(h3_V1_sims)
    h3_V1_res$mod3_V1_probs <- get_elimProbs(h3_V1_sims, model = 3)

    # Return results
    h3_V1_res

  }
)

mod3_V2_res <- bake(
  file = paste0("model3/", rl_dir, "mod3_V2_res"), {

    # Create a list to store results
    h3_V2_res <- list()

    # Get "noVacc" scenario parameters
    h3_V2_par <- get_model3_vacc_scenario_params(scenario = "3dep")
    coef(h3Spat)[names(h3_V2_par)] <- h3_V2_par

    # Using parameters and filtered distribution, project system
    h3_V2_sims <- project_from_filter2(
      h3Spat, PF = h3_bpf, covarGen = project_rain,
      nsims = NSIM, seed = 6897154
    )

    # Store nationally aggregated results
    h3_V2_res$mod3_V2_sims <- agg_mod3_sims(h3_V2_sims)
    h3_V2_res$mod3_V2_probs <- get_elimProbs(h3_V2_sims, model = 3)

    # Return results
    h3_V2_res

  }
)

mod3_V3_res <- bake(
  file = paste0("model3/", rl_dir, "mod3_V3_res"), {

    # Create a list to store results
    h3_V3_res <- list()

    # Get "noVacc" scenario parameters
    h3_V3_par <- get_model3_vacc_scenario_params(scenario = "slowNation")
    coef(h3Spat)[names(h3_V3_par)] <- h3_V3_par

    # Using parameters and filtered distribution, project system
    h3_V3_sims <- project_from_filter2(
      h3Spat, PF = h3_bpf, covarGen = project_rain,
      nsims = NSIM, seed = 6897154
    )

    # Store nationally aggregated results
    h3_V3_res$mod3_V3_sims <- agg_mod3_sims(h3_V3_sims)
    h3_V3_res$mod3_V3_probs <- get_elimProbs(h3_V3_sims, model = 3)

    # Return results
    h3_V3_res

  }
)

mod3_V4_res <- bake(
  file = paste0("model3/", rl_dir, "mod3_V4_res"), {

    # Create a list to store results
    h3_V4_res <- list()

    # Get "noVacc" scenario parameters
    h3_V4_par <- get_model3_vacc_scenario_params(scenario = "fastNation")
    coef(h3Spat)[names(h3_V4_par)] <- h3_V4_par

    # Using parameters and filtered distribution, project system
    h3_V4_sims <- project_from_filter2(
      h3Spat, PF = h3_bpf, covarGen = project_rain,
      nsims = NSIM, seed = 6897154
    )

    # Store nationally aggregated results
    h3_V4_res$mod3_V4_sims <- agg_mod3_sims(h3_V4_sims)
    h3_V4_res$mod3_V4_probs <- get_elimProbs(h3_V4_sims, model = 3)

    # Return results
    h3_V4_res

  }
)
@

Simulations from probabilistic models (Models~1 and~3) represent possible trajectories of the dynamic system under the scientific assumptions of the models.
In this case study, estimates of the probability of cholera elimination can therefore be obtained as the proportion of simulations from the fitted model that result in cholera elimination.
The results of these projections are summarized in Figs.~\ref{fig:Mod1Scenarios}--\ref{fig:elimProbs}.
These results suggest that cholera elimination was likely, even without increased vaccination efforts, which is consistent with observed reality \citep{ferguson22}. \eic{I LIKE THE IDEA OF REFERENCING POST-EPIDEMIC REFLECTIONS. UNLESS THERE ARE NO ALTERNATIVES, IT IS BEST TO AVOID CITING A NON-PEER-REVIEWED ARTICLE SUCH AS THIS. HOW ABOUT \citep{piarroux22} OR \citep{trevisin22}?}

Probability of elimination estimates of this form are not meaningful for deterministic models, as the trajectory of these models only represent the mean behavior of the system rather than individual potential outcomes.
We therefore do not provide probability of elimination estimates under Model~2.
Still, trajectories obtained by Model~2 are consistent with the simulation results of Models~1 and~3, and suggest that cholera was in the process of being eliminated from Haiti.

In additional to probability of elimination estimates, we provide estimates for the cumulative number of infections under each vaccination scenario from February 2019 -- February 2024.
Notably, the median number of cumulative cholera infections under the no-vaccination scenario using Models 1 and 3 were $\Sexpr{prettyNum(round(as.numeric(mod1_V0_probs$cumInf['q50'])), big.mark = ",")}$ and $\Sexpr{prettyNum(round(as.numeric(mod3_V0_res$mod3_V0_probs$cumInf['q50'])), big.mark = ",")}$, respectively.
While there is remaining time during this projection period in which new cholera infections can be detected, up to this point our estimates are far more consistent with the observed number of reported cholera cases than the corresponding estimates from \cite{lee20}, which were approximately $400,000$ and $1,000,000$.

<<Create Model 1 VaccScen plots>>=
gg_m1_V0 <- ggplot() +
  geom_line(data = mod1_V0_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V0_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title.x = element_blank()) +
  ggtitle("V0") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V1 <- ggplot() +
  geom_line(data = mod1_V1_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V1_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V1") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V2 <- ggplot() +
  geom_line(data = mod1_V2_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V2_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V2") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V3 <- ggplot() +
  geom_line(data = mod1_V3_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V3_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V3") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V4 <- ggplot() +
  geom_line(data = mod1_V4_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V4_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V4") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

@

\begin{figure}[ht]
<<Plot_Model1_Scenarios, fig.height=2>>=
lay <- rbind(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5))
gridExtra::grid.arrange(
  gg_m1_V0, gg_m1_V1, gg_m1_V2, gg_m1_V3, gg_m1_V4,
  layout_matrix = lay
)
@
\caption{\label{fig:Mod1Scenarios}
Simulations of Model~1 under each vaccination scenario.
Blue line indicates the median of model simulations, and ribbon represents $95\%$ of simulations.
The various vaccination campaigns made no practical difference in the median scenario, but a drastic difference in the extreme cases.
}
\end{figure}

\begin{figure}[ht]
<<Mod2Fit_and_Scenarios_Figure, fig.height=2, fig.width=5, fig.align='center'>>=
ggplot() +
  geom_line(data = h2_traj, aes(x = date, y = Ctotal + 1), col = 'blue') +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  geom_line(data = filter(mod2_VaccScenarios$all_trajs,
                          year >= max(h2_traj$year)),
            aes(x = as.Date(date), y = ReportedCases + 1, color = scenario)) +
  theme(axis.title.x = element_blank(),
        legend.title = element_blank(),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 8)) +
  ylab("Reported cholera cases") +
  scale_color_manual(values = c("V0" = '#f4a582',
                                "V1" = '#d6604d',
                                "V2" = '#b2182b',
                                "V3" = '#92c5de',
                                "V4" = '#4393c3')) +
  # geom_vline(xintercept = yearsToDate(2014.161), linetype = 'dashed') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2010-01-01"), as.Date("2030-01-01"), by = '2 years'))
@
\caption{\label{fig:mod2Traj}
Simulated trajectory of Model~2 (blue curve) and projections under the various vaccination scenarios.
Reported cholera incidence is shown in black.}
\end{figure}

<<Create Model 3 VaccScen Plots>>=
gg_m3_V0 <- ggplot() +
  geom_line(data = mod3_V0_res$mod3_V0_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V0_res$mod3_V0_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title.x = element_blank()) +
  ggtitle("V0") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V1 <- ggplot() +
  geom_line(data = mod3_V1_res$mod3_V1_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V1_res$mod3_V1_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V1") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V2 <- ggplot() +
  geom_line(data = mod3_V2_res$mod3_V2_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V2_res$mod3_V2_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V2") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V3 <- ggplot() +
  geom_line(data = mod3_V3_res$mod3_V3_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V3_res$mod3_V3_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V3") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V4 <- ggplot() +
  geom_line(data = mod3_V4_res$mod3_V4_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V4_res$mod3_V4_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V4") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))
@


\begin{figure}[ht]
<<Plot_Model3_Scenarios, fig.height=2>>=
lay <- rbind(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5))
gridExtra::grid.arrange(
  gg_m3_V0, gg_m3_V1, gg_m3_V2, gg_m3_V3, gg_m3_V4,
  layout_matrix = lay
)
@
\caption{\label{fig:Mod3Scenarios}
Simulations of Model~3 under each vaccination scenario.
Blue line indicates the median of model simulations, and ribbon represents $95\%$ of simulations.
The various vaccination campaigns made no practical difference in the median scenario, but a drastic difference in the extreme cases.
}
\end{figure}

% \begin{figure}[ht]
% \centering
%
% mod1_cumInf <- data.frame(
%   'scenario' = paste0("V", 0:4),
%   'low' = c(
%     as.numeric(mod1_V0_probs$cumInf['q025']),
%     as.numeric(mod1_V1_probs$cumInf['q025']),
%     as.numeric(mod1_V2_probs$cumInf['q025']),
%     as.numeric(mod1_V3_probs$cumInf['q025']),
%     as.numeric(mod1_V4_probs$cumInf['q025'])
%   ),
%   'high' = c(
%     as.numeric(mod1_V0_probs$cumInf['q975']),
%     as.numeric(mod1_V1_probs$cumInf['q975']),
%     as.numeric(mod1_V2_probs$cumInf['q975']),
%     as.numeric(mod1_V3_probs$cumInf['q975']),
%     as.numeric(mod1_V4_probs$cumInf['q975'])
%   ),
%   'med' = c(
%     as.numeric(mod1_V0_probs$cumInf['q50']),
%     as.numeric(mod1_V1_probs$cumInf['q50']),
%     as.numeric(mod1_V2_probs$cumInf['q50']),
%     as.numeric(mod1_V3_probs$cumInf['q50']),
%     as.numeric(mod1_V4_probs$cumInf['q50'])
%   )
% )
%
% mod3_cumInf <- data.frame(
%   'scenario' = paste0("V", 0:4),
%   'low' = c(
%     as.numeric(mod3_V0_probs$cumInf['q025']),
%     as.numeric(mod3_V1_probs$cumInf['q025']),
%     as.numeric(mod3_V2_probs$cumInf['q025']),
%     as.numeric(mod3_V3_probs$cumInf['q025']),
%     as.numeric(mod3_V4_probs$cumInf['q025'])
%   ),
%   'high' = c(
%     as.numeric(mod3_V0_probs$cumInf['q975']),
%     as.numeric(mod3_V1_probs$cumInf['q975']),
%     as.numeric(mod3_V2_probs$cumInf['q975']),
%     as.numeric(mod3_V3_probs$cumInf['q975']),
%     as.numeric(mod3_V4_probs$cumInf['q975'])
%   ),
%   'med' = c(
%     as.numeric(mod3_V0_probs$cumInf['q50']),
%     as.numeric(mod3_V1_probs$cumInf['q50']),
%     as.numeric(mod3_V2_probs$cumInf['q50']),
%     as.numeric(mod3_V3_probs$cumInf['q50']),
%     as.numeric(mod3_V4_probs$cumInf['q50'])
%   )
% )
%
% mod1_cumInf$model <- "1"
% mod3_cumInf$model <- "3"
%
% all_cumInf <- rbind(mod1_cumInf, mod3_cumInf)
% all_cumInf$scenario <- factor(all_cumInf$scenario, levels = rev(c("V0", "V1", "V2", "V3", "V4")))
%
%
% ggplot(all_cumInf, aes(y = scenario)) +
%   geom_point(aes(x = med)) +
%   geom_linerange(aes(xmin = low, xmax = high)) +
%   scale_x_continuous(
%     labels = scales::unit_format(scale = 1e-6, accuracy = 1),
%     limits = c(0, 5e6)
%   ) +
%   xlab("Cumulative infections (million)") +
%   theme(axis.title.y = element_blank(),
%         axis.title.x = element_text(size = 10),
%         axis.text = element_text(size = 8)) +
%   facet_wrap(~model, ncol = 1,
%              labeller = as_labeller(
%                c("1" = "Model 1", "3" = "Model 3", "2" = "Model 2")
%              ))
%
% \caption{\label{fig:cumInfPlot}
% Simulated cumulative infections under various vaccination scenarios from February, 2019, to February, 2024.
% Point estimates (median cumulative incidence) and $95\%$ error bars are included.
% x-axis scale is intended to ease comparison with Figure~4 of \cite{lee20}.
% \jwc{MAYBE JUST SUMMARIZE THIS FIGURE IN A PARAGRAPH OR MOVE TO SUPPLEMENT TO MAKE SPACE?}
% }
% \end{figure}

\jwc{PARAGRAPH REMOVED}
% Despite these results suggesting that cholera was likely to be eliminated from Haiti without an increase in vaccination effort, we in no way suggest that vaccinations shouldn't be used to combat cholera---or other infectious diseases---in other parts of the world.
% In fact, our analysis shows that an increase in vaccination does drastically reduce the negative effects of a worst-case scenario.
% Avoid comments on vaccination policy recommendations - Jesse

<<Compute Elimination Probs, echo=FALSE>>=
mod1_V0_probs$ElimTime$scenario = "V0"
mod1_V1_probs$ElimTime$scenario = "V1"
mod1_V2_probs$ElimTime$scenario = "V2"
mod1_V3_probs$ElimTime$scenario = "V3"
mod1_V4_probs$ElimTime$scenario = "V4"

mod1_probElims <- rbind(
  mod1_V0_probs$ElimTime,
  mod1_V1_probs$ElimTime,
  mod1_V2_probs$ElimTime,
  mod1_V3_probs$ElimTime,
  mod1_V4_probs$ElimTime
)

mod3_V0_res$mod3_V0_probs$ElimTime$scenario = "V0"
mod3_V1_res$mod3_V1_probs$ElimTime$scenario = "V1"
mod3_V2_res$mod3_V2_probs$ElimTime$scenario = "V2"
mod3_V3_res$mod3_V3_probs$ElimTime$scenario = "V3"
mod3_V4_res$mod3_V4_probs$ElimTime$scenario = "V4"

mod3_probElims <- rbind(
  mod3_V0_res$mod3_V0_probs$ElimTime,
  mod3_V1_res$mod3_V1_probs$ElimTime,
  mod3_V2_res$mod3_V2_probs$ElimTime,
  mod3_V3_res$mod3_V3_probs$ElimTime,
  mod3_V4_res$mod3_V4_probs$ElimTime
)

mod1_probElims$mod <- "Model 1"
mod3_probElims$mod <- "Model 3"
mod3_probElims$time <- yearsToDate(mod3_probElims$time)

all_prob_elims <- dplyr::bind_rows(mod1_probElims, mod3_probElims)
@

\begin{figure}[ht]
<<Elimination_Probs_Figure, fig.height=2.5>>=
ggplot(tidyr::drop_na(all_prob_elims), aes(x = time, y = elim_prob, col = mod)) +
  geom_line() +
  facet_wrap(~scenario, nrow = 1) +
  scale_color_manual(values = c("Model 1" = "#377eb8", "Model 3" = "#e41a1c")) +
  ylab("Probability of Elimination (%)") +
  theme(axis.title.x = element_blank(),
        legend.position = 'bottom',
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 35, hjust = 1),
        legend.margin=margin(c(-5, 0, -3, 0)),
        legend.box.margin = margin(c(-5, 0, -3, 0))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2010-01-01"), as.Date("2030-01-01"), by = '2 years'))
@
\caption{\label{fig:elimProbs}
Probability of elimination across simulations for a 10 year period.
Compare to Figure 3A of \cite{lee20}.}
\end{figure}

%%% table tttttttttt

\begin{table}
\small

\hspace{-10mm}\begin{tabular}{|l@{}|l@{}c@{}|l@{}c@{}|l@{}c@{}|}
\hline
Mechanism & Model 1 && Model 2 && Model 3 &
\\
\hline
\hline
Infection (day)
  & {\fixed $\muIR^{-1}=\Sexpr{myround(7/p1["gamma"],1)}$}
  & \eqref{model1:toR}
  & {\fixed $\muIR^{-1}=\Sexpr{myround(365/h2_fit$h2_params["gamma"],1)}$}
  & \eqref{model2:mu_IR}
  & {\fixed $\muIR^{-1}=\Sexpr{myround(365/p3["gamma"],1)}$ }
  & \eqref{eq:model3:IR}
\\
Latency (day)
  & {\fixed $\muEI^{-1}=\Sexpr{myround(7/p1["sigma"],1)}$}
  & \eqref{model1:EA}
  & {\fixed $\muEI^{-1}=\Sexpr{myround(365/h2_fit$h2_params["gammaE"],1)}$}
  & \eqref{model2:mu_EI}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
\\
\hline
Seasonality
  & $\begin{array}{l}
    \hspace{-1mm} \transmission_{1:6}=(
      \Sexpr{myround(p1["beta1"],1)},
      \Sexpr{myround(p1["beta2"],1)},
    \\
      \Sexpr{myround(p1["beta3"],1)},
      \Sexpr{myround(p1["beta4"],1)},
      \Sexpr{myround(p1["beta5"],1)},
      \Sexpr{myround(p1["beta6"],1)})
    \end{array}$
  & \eqref{model1:beta}
  & {\fixed $\seasAmplitude=0.4$ }
  & \eqref{model2:lambda}
  & $\begin{array}{l} \seasAmplitude=\Sexpr{myround(p3s["lambdaR"],2)} \\ r=\Sexpr{signif(p3s["r"],3)} \end{array}$
  & \eqref{eq:model3:water}
\\
\hline
$\begin{array}{l}
\text{Immunity} \\
\text{(year)}
\end{array}$
  & {\fixed $\muRS^{-1}=\Sexpr{myround(1/p1["alpha"]/52,1)}$}
  & \eqref{model1:RS}
  & {\fixed
    $\begin{array}{l}
    \muRS^{-1}= \Sexpr{signif(1/h2_fit$h2_params["sigma"],2)} \\
     \omega_1^{-1} = \Sexpr{myround(1/h2_fit$h2_params["Omega1"],1)} \\
    \omega_2^{-1} = \Sexpr{myround(1/h2_fit$h2_params["Omega2"],1)}
    \end{array}$
    }
  & $\begin{array}{l}
    \eqref{model2:RS} \\
    \eqref{model2:omega1} \\
    \eqref{model2:omega1}
    \end{array} $
  & {\fixed $\muRS^{-1}=\Sexpr{myround(1/p3s["rho"],1)}$}
  & \eqref{eq:model3:RRnext}
\\
\hline
Vaccine efficacy
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\begin{array}{l}
      \hspace{-1mm} \fixed{\vaccineEfficacy_{1:4} = (
        \Sexpr{myround(1 - h2_fit$h2_params["VE1"] * 0.4688, 2)},
        \Sexpr{myround(1 - h2_fit$h2_params["VE2"] * 0.4688, 2)}} \\
        \fixed{\Sexpr{myround(1 - h2_fit$h2_params["VE1"], 2)},
        \Sexpr{myround(1 - h2_fit$h2_params["VE2"], 2)}})
        \end{array}$
  & \eqref{model2:mu_SE}
  & \fixed{$\eta_{ud}(t)$}
  &
  \\
Birth/death (yr)
  & \fixed{$\begin{array}{l}
    \muBirth^{-1} = \Sexpr{myround(1/p1["mu"]/52,1)} \\
    \muDeath^{-1} = \Sexpr{myround(1/p1["delta"]/52,1)}
    \end{array}$}
  & \eqref{model1:death}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & {\fixed $\muDeath^{-1} = \Sexpr{myround(1/p3s["mu"],1)}$}
  & \eqref{eq:model3:IS}
\\
Symptomatic frac.
  & {\fixed $\symptomFrac_z(t)=c\theta^*(t-\tau_d)$}
  & (\ref{model1:EI}-\ref{model1:EA})
  & {\fixed $\symptomFrac=\Sexpr{myround(h2_fit$h2_params["k"],1)}$}
  & \eqref{model2:mu_EI}
  & {\fixed $\symptomFrac=\Sexpr{myround(p3s["sigma"],2)}$}
  & \eqref{eq:model3:SA}
\\
$\begin{array}{l}
\text{Asymptomatic} \\
\text{infectivity}
\end{array}$
  & {\fixed $\asymptomRelativeInfect=0.05$ }
  & \eqref{model1:lambda}
  & $\begin{array}{l}
      {\fixed \asymptomRelativeInfect =0.001 } \\
      {\fixed \asymptomRelativeShed = \Sexpr{1e-7} }
    \end{array}$
  & \begin{tabular}{l}
      \eqref{model2:lambda} \\
      \eqref{model2:to_W}
    \end{tabular}
  &  $\begin{array}{l}
      {\fixed \asymptomRelativeInfect =1 } \\
      {\fixed \asymptomRelativeShed = \Sexpr{myround(p3s["XthetaA"],3)} }
    \end{array}$
  & \begin{tabular}{l}
      \eqref{eq:model3:foi} \\
      \eqref{eq:model3:water}
    \end{tabular}
\\
\hline
$\begin{array}{l}
\text{Human to}\\
\text{human}\end{array}$
  & $\transmission_{1:6}$ as above
  & \eqref{model1:lambda}
  & $\transmission=$\Sexpr{signif(h2_fit$h2_params["Beta"],3)}
  & \eqref{model2:lambda}
  & $\begin{array}{l}
      \hspace{-1mm} \transmission_{1:10}=(
        \Sexpr{myround(p3u["foi_add",1]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",2]*1e6,2)},
      \\
	\Sexpr{myround(p3u["foi_add",3]*1e6,2)},
        \Sexpr{myround(p3u["foi_add",4]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",5]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",6]*1e6,2)},
      \\
	\Sexpr{myround(p3u["foi_add",7]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",8]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",9]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",10]*1e6,2)}
	)
      \\
      \times 10^{-6}
    \end{array}$
  & \eqref{eq:model3:foi}
\\
\hline
$\begin{array}{l}
\text{Water to}\\ \text{human}
\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\begin{array}{l}
    {\fixed \Wsat = \Sexpr{signif(h2_fit$h2_params["Sat"],2)} }
    \\
    \beta_W= \Sexpr{signif(h2_fit$h2_params["BetaW"],3)}
    \end{array}$
  & \eqref{model2:lambda}
  & $\begin{array}{l}
      \hspace{-1mm} \Wbeta{_{1:10}}= (
        \Sexpr{myround(p3u["betaB",1],2)}, \Sexpr{myround(p3u["betaB",2],2)},
       \\
        \Sexpr{myround(p3u["betaB",3],2)}, \Sexpr{myround(p3u["betaB",4],2)},
        \Sexpr{myround(p3u["betaB",5],2)}, \Sexpr{myround(p3u["betaB",6],2)},
       \\
        \Sexpr{myround(p3u["betaB",7],2)}, \Sexpr{myround(p3u["betaB",8],2)},
        \Sexpr{myround(p3u["betaB",9],2)}, \Sexpr{myround(p3u["betaB",10],2)}
      ) \end{array}$
  & \eqref{eq:model3:foi}
\\
\hline
$\begin{array}{l}
\text{Human to}\\
\text{water}\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\Wshed = $ \Sexpr{signif(h2_fit$h2_params["Mu"],3)}
  & \eqref{model2:to_W}
  & $\Wshed= \Sexpr{signif(p3s["thetaI"],3)}$
  & \eqref{eq:model3:water}
\\
$\begin{array}{l}
\text{Water} \\
\text{survival (wk)}
\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\Wremoval^{-1} = $ \Sexpr{signif(52/h2_fit$h2_params["Delta"],3)}
  & \eqref{model2:from_W}
  & $\Wremoval^{-1}=\Sexpr{myround(52/p3s["mu_B"],2)}$
  & \eqref{eq:model3:Decay}
\\
Mixing exponent
  & $\mixExponent=\Sexpr{myround(p1["nu"],2)}$
  & \eqref{model1:lambda}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
\\
$\begin{array}{l}
\text{Process} \\
\text{noise} (\text{wk}^{1/2})
\end{array}$
  & $\sigmaProc=(\Sexpr{myround(sqrt(p1["sig_sq_epi"]),2)}, \Sexpr{myround(sqrt(p1["sig_sq_end"]),2)})$
  & \eqref{model1:lambda}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\sigmaProc= \Sexpr{myround(p3s["std_W"],3)}$
  & \eqref{eq:model3:SA}
\\
Reporting rate
  & $\reportRate=(\Sexpr{myround(p1["rho_epi"],3)}, \Sexpr{myround(p1["rho_end"],3)})$
  & (S16) % TODO: Make sure manual equations match!
  & {\fixed $\reportRate=\Sexpr{myround(h2_fit$h2_params["Rho"],2)}$}
  & (S17)
  & $\reportRate=\Sexpr{myround(p3s["epsilon"],2)}$
  & (S18)
\\
$\begin{array}{l}
\text{Observation} \\
\text{overdispersion}
\end{array}$
  & $\obsOverdispersion=(\Sexpr{round(p1["tau_epi"],2)}, \Sexpr{round(p1["tau_end"],2)})$
  & (S16)
  &
  &
  & $\obsOverdispersion=\Sexpr{myround(p3s["k"],2)}$
  & (S18)
\\
\hline
\end{tabular}
\caption{References to the relevant equation are given in parentheses.
Parameters in blue were fixed based on scientific reasoning and not fitted to the data.
[N] denotes parameters added during our re-analysis, not considered by Lee et al.
Translations back into the notation of \citet{lee20} are given in Table~S1.
% \eic{TODO: CHECK PARAMETERIZATION AND UNITS FOR $\obsOverdispersion$ AND $\sigmaProc$ TO MAKE THEM COMPARABLE}
}
\end{table}

\section{Discussion}\label{sec:discussion}

The ongoing global COVID-19 pandemic has provided a clear example on how government policy may be affected by the conclusions of scientific models.
This article demonstrates that fitting appropriate scientific models remains a challenging statistical task, and therefore great care is needed when fitting scientific models for policy recommendations.
We provided a few suggestions that may aid the fitting of mechanistic models such as comparing model likelihoods to a benchmark.
Improved model fits allows for meaningful statistical inference that may provide valuable insight on a dynamic system in question and may improve the accuracy of model-based projections.
% TODO: "Improved model fits", or "Improved model fitting", based on Anna's suggestion.
Caution is nonetheless needed when making policy based on modeling conclusions, as model misspecification may invalidate conclusions.
% TODO: Citation above?

Various suggestions been made about why \citet{lee20} failed to accurately predict the eventual eradication of cholera from Haiti, including model misspecification, overly difficult elimination criteria, and a potential conflict of interest \citep{rebaudetComment20,henrys20}.
Here, we instead argue that careful attention to important statistical details could have correctly resulted in the conclusion of imminent cholera elimination.
We acknowledge the benefit of hindsight: our demonstration of a statistically principled route to obtain better-fitting models with more accurate predictions does not rule out the possibility of discovering other models that fit well yet predict poorly.

We used the same data and models, and even much of the same code, as \citet{lee20}, and yet ended up with drastically different conclusions.
At a minimum, we have shown that the conclusions are sensitive to details in how the data analysis is carried out, and that attention to statistical fit (including numerical issues such as likelihood maximization) can lead to improved policy guidance.

We acknowledge there are limitations to this study;
one example was the inability to fit model parameters to the fully-coupled SpatPOMP version of Model~3.
Promising theoretical and methodological developments \citep{ning21ibpf,ionides22} based on the Block Particle Filter \citep{rebeschini15} may potentially be used to fit the SpatPOMP version of Model~3 in future work.

% \arc{Right now, the manuscript's main point/claim is a bit muddled (at least to me). I am not sure how to directly address this, but having a additional paragraph or two in the discussion may help the impact of this article/"drive our points home". I will think more on this in the next week, but maybe we could further emphasize the balance/dependence between policy, prediction, inference, model quality, and model fitting? Well informed policy depends upon reliable prediction which depends upon model quality/performance which depends upon rigorous evaluation and estimation, regardless of correct model specification (though that is also important/of great interest).}
% \jwc{Thanks, I agree that the main point isn't clear, and Ed has suggested as much also. I think that adding additional paragraphs would help this, but we are already 2 pages over the recommended submission length, so re-wording the text may be a better option than adding more to it... I'll also have to think about this more this week.}
% \eic{HERE'S A SUGGESTION FOR A CONCLUDING PARAGRAPH. I THINK IT IS SUPPORTED BY THE RESULTS...}
Inference for mechanistic time series models offers opportunities for understanding and controlling complex dynamic systems. This case study has investigated issues requiring attention when applying powerful new statistical techniques that can enable statistically efficient inference for a general class of partially observed Markov process models. Care must be taken to ensure that the computationally intensive numerical calculations are carried out adequately. Once that is accomplished, care is required to assess what causal conclusions can properly be inferred given the possibility of alternative explanations consistent with the data. Studies that combine model development with thoughtful data analysis, supported by a high standard of reproducibility, build knowledge about the system under investigation. Cautionary warnings about the difficulties inherent in understanding complex systems \citep{saltelli20,ioannidis20} should motivate us to follow best practices in data analysis, rather than avoiding the challenge.

\subsection{Reproducibility and Extendability}

\citet{lee20} published their code and data online, and this reproducibility facilitated our work.
By design, the models were coded and analyzed independently, leading to differing implementation decisions.
Robust data analysis requires not only reproducibility but also extendability: if one wishes to try new model variations, or new approaches to fitting the existing models, or plotting the results in a different way, this should be not excessively burdensome.
Scientific results are only trustworthy so far as they can be critically questioned, and an extendable analysis should facilitate such examination \citep{gentleman07}.

We provide a strong form of reproducibility, as well as extendability, by developing our analysis in the context of an \code{R} package, \code{haitipkg}.
Using a software package mechanism supports documentation, standardization and portability that promote extendability.
In the terminology of \citet{gentleman07}, the source code for this article is a {\it dynamic document} combining code chunks with text.
In addition to reproducing the article, the code can be extended to examine alternative analysis to that presented.
The dynamic document, together with the R packages, form a {\it compendium}, defined by \citet{gentleman07} as a distributable and executable unit which combines data, text and auxiliary software (the latter meaning code written to run in a general-purpose, portable programming environment, which in this case is R).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Single Appendix:                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{appendix}
% \section*{Block Panel Iterated Filter}
%
% This algorithm is a modification of the panel iterated filter \citep{breto20}.
% The intuition behind this modification is that if the unit-specific
% parameters are independent of one another, we allow these parameters to be
% fit as such by not allowing the particle ancestry in other units to influence
% the fit of unit specific parameter in question.
% Pseudo-code for this algorithm is given in Algorithm~1.
%
% While theoretical properties of this algorithm have yet to be derived, empirical
% results using Model~3 and on simulated data from a simple model suggest that this
% modification may lead to quicker convergence to the model likelihood.
%
% \begin{algorithm}[ht]
%    \caption{\textbf{BPIF}. \\
%     {\bf Inputs}:
%     Simulator of initial density, $f_{X_{u, 0}}(x_{u, 0}; \theta)$ for $u$ in $1:U$.
%     Simulator of transition density, $f_{X_{u, n}|X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \theta)$ for $u$ in $1:U$, $n$ in $1:N_u$.
%     Evaluator of measurement density, $f_{Y_{u, n}|X_{u, n}}(y_{u, n}|x_{u, n} ; \theta)$ for $u$ in $1:U$, $n$ in $1:N_u$.
%     Data $y_{u, n}^*$,for $u$ in $1:U$, $n$ in $1:N_u$.
%     Number of iterations, $M$.
%     Number of particles, $J$.
%     Starting parameter swarm, $\Theta_j^0$ for $j$ in $1:J$.
%     Simulator of perturbation density, $h_{u, n}(\theta | \phi; \sigma)$ for $u$ in $1:U$, $n$ in $0:N_u$
%     Perturbation sequence, $\sigma_m$ for $m$ in $1:M$.
%     {\bf Output:} Final parameter swarm, $\Theta_{j}^{m}$ for $j$ in $1:J$
%     \label{alg:bpif}}
% %\BlankLine
% \For{$m \in 1:M$}{
%   Set $\Theta_{0, j}^m = \Theta_j^{m-1}$ for $j \in 1:J$\;
%     \For{$u \in 1:U$}{
%       Perturb parameters: $\Theta_{u, 0, j}^{F, m} \sim h_{u, 0}(\theta| \Theta_{u - 1, j}^m; \sigma_m)$ for $j \in 1:J$\;
%       Initialize: $X_{u, 0, j}^{F, m} \sim f_{X_{u, 0}}(x_{u, 0}; \Theta_{u, 0, j}^{F, m})$ for $j \in 1:J$\;
%         \For{$n \in 1:N_u$} {
%           Perturb parameters: $\Theta_{u, n, j}^{P, m} \sim h_{u, n}(\theta|\Theta_{u, n-1, j}^{F, m}, \sigma_m)$ for $j \in 1:J$\;
%           Prediction Simulation: $X_{u, n, j}^{P, m} \sim f_{X_{u, n}|X_{u, n-1}}(x_{u, n}|X_{u, n-1, j}^{F, m}; \Theta_{u, n, j}^{P, m})$ for $j \in 1:J$\;
%           Prediction weights: $w_{u, n, j}^m = f_{Y_{u, n}|X_{u, n}}(y_{u, n}^*|X_{u, n, j}^{P, m})$ for $j \in 1:J$\;
%           Draw $k_{1:j}$ with $P(k_j = i) = w_{u, n, i}^m / \sum_{v = 1}^J w_{u, n, v}^m$ for $i, j \in 1:J$\;
%           Set: $\Theta_{u, n, j}^{F, m} = \Theta_{u, n, k_j}^{P, m}$ and $X_{u, n, j}^{F, m} = X_{u, n, k_j}^{P, m}$ for $j \in 1:J$\;
%         }
%       Set $\Theta_{u, j}^m = \Theta_{u, N_u, j}^{F, m}$ for $j \in 1:J$\;
%     }
%   Set $\Theta_j^m = \Theta_{U, j}^m$ for $j \in 1:J$\;
% }
% \end{algorithm}

% \section*{}%% if no title is needed, leave empty \section*{}.
% \begin{table}
%   \begin{tabular}{|c|c|c|c|}\hline
%     \thead{Parameter} & \thead{Our \\ Notation} & \thead{\cite{lee20} \\ Notation} & \thead{Relevant \\ Model(s)} \\
%     \hline
%     \hline
%     % Table begins here
%     \multirow{2}{*}{Reporting Rate} & \multirow{2}{*}{$\reportRate$} & $\rho$ & 1, 2 \\\cline{3-4}
%       & & $\epsilon_1, \epsilon_2$ & 3\\\hline
%     Mixing Coefficient & $\mixExponent$ & $\nu$ & 1 \\\hline
%     \multirow{2}{*}{Measurement Over-Dispersion} & \multirow{2}{*}{$\obsOverdispersion$} & $\tau$ & 1\\\cline{3-4}
%     & & $p$ & 3 \\\hline
%     Birth Rate & $\muBirth$ & $\mu$ & 1 \\\hline
%     \multirow{2}{*}{Natural Mortality Rate} & \multirow{2}{*}{$\muDeath$} & $\delta$ & 1 \\\cline{3-4}
%      & & $\mu$ & 3 \\\hline
%      Cholera Mortality Rate & $\choleraDeath$ & $\alpha$ & 3 \\\hline
%     \multirow{2}{*}{Latent Period} & \multirow{2}{*}{$1/\muEI$} & $1/\sigma$ & 1 \\\cline{3-4}
%                                    & & $1/\gamma_E$ & 2 \\\hline
%     Recovery Rate & $\muIR$ & $\gamma$ & 1, 2, 3 \\\hline
%     \multirow{3}{*}{Loss of Immunity} & \multirow{3}{*}{$\muRS$} & $\alpha$ & 1 \\\cline{3-4}
%                                       & & $\sigma$ & 2 \\\cline{3-4}
%                                       & & $\rho$ & 3 \\\hline
%     \multirow{3}{*}{Symptomatic Ratio} & \multirow{3}{*}{$\symptomFrac$} & $1 - \theta_0$ & 1 \\\cline{3-4}
%     & & $k$ & 2 \\\cline{3-4}
%     & & $\sigma$ & 3\\\hline
%     Asymptomatic & \multirow{2}{*}{$\asymptomRelativeInfect$} & $\kappa$ & 1 \\\cline{3-4}
%     Relative Infectiousness & & $red_\beta$ & 2 \\\hline
%     \multirow{2}{*}{Human-to-Water Shedding} & \multirow{2}{*}{$\Wshed$} & $\mu$ & 2 \\\cline{3-4}
%      & & $\theta_I$ & 3 \\\hline
%     Asymptomatic & \multirow{2}{*}{$\asymptomRelativeShed$} & $red_\mu$ & 2 \\\cline{3-4}
%     Relative Shedding & & $\theta_A/\theta_I$ & 3 \\\hline
%     \multirow{2}{*}{Seasonal Amplitude} & \multirow{2}{*}{$\seasAmplitude$} & $\alpha_s$ & 2 \\\cline{3-4}
%     & & $\lambda$ & 3 \\\hline
%     \multirow{2}{*}{Transmission} & \multirow{2}{*}{$\transmission$} & $\beta$ & 1, 2 \\\cline{3-4}
%     & & $c$ & 3 \\\hline
%     Water-to-Human & \multirow{2}{*}{$\beta_W$} & $\beta_W$ & 2 \\\cline{3-4}
%     Infection Rate & & $\beta$ & 3 \\\hline
%     Bacteria Mortality & \multirow{2}{*}{$\Wremoval$} & $\delta$ & 2 \\\cline{3-4}
%     Rate & & $\mu_\beta$ & 3 \\\hline
%     \multirow{3}{*}{Vaccination Efficacy} & \multirow{3}{*}{$\vaccineEfficacy$} & $\theta_{vk}$ & 1 \\\cline{3-4}
%     & & $\text{VE}_1, \text{VE}_2$ & 2 \\\cline{3-4}
%     & & $\eta_{1d}, \eta_{2d}$ & 3 \\\hline
%     Process Over-dispersion & $\sigmaProc$ & $\sigma_w$ & 3 \\\hline
%     % Table ends here
%   \end{tabular}
%   \caption{
%   \label{tab:translate}Translations between our common notation and notation used in \cite{lee20}\eic{TODO: half saturation constant, 2 and 3. Fraction of susceptible, 2.
%   Initial Value parameters.}
%   }
% \end{table}

% \section*{Initial Values}
%
% To perform inference on POMP models, it is necessary to propose an initial density for the latent process $f_{X_0}(x_0;\theta)$.
% This density is used to obtain initial values of the latent state when fitting and evaluating the model.
% For each of the models considered in this analysis, the initial conditions are derived by enforcing the model dynamics on reported cholera cases.
% For example, in a deterministic model, the total number of Infected and Asymptomatic individuals may be calculated using the observed number of cholera cases at time time of model initialization based on model parameters such as reporting rate $\reportRate$ and fraction of symptomatic cases $\symptomFrac$.
% It is also sometimes necessary to fit some initial value parameters in order to help determine starting values for weakly identifiable compartments.
% In the following subsections, we mention initial value parameters that were fit for each model.
%
% \subsection*{Model~1}
%
% For this model, the number of individuals in the Recovered and Asymptomatic compartments are set to zero, but the initial proportion of Infected and Exposed individuals is estimated as initial value parameters ($I_0$ and $E_0$, respectively) using the MIF2 algorithm.
% Finally, the initial proportion of Susceptible individuals $S_0$ is calculated as $S_0 = 1 - I_0 - E_0$.
%
% \subsection*{Model~3}
%
% We use the reported cases at the start of the pandemic to approximate the number of Asymptomatic, Infectious, and Recovered individuals in each department $u \in 1:U$ using the same approximation as provided in Eq.~\myeqref{eq:model3:approx}.
% The susceptible compartment is initialized so that the sum $S_{u}(0) + I_u(0) + A_u(0) + \sum_k R_{u, k}(0) = \text{population}_u$.
% The bacteria compartment is then initialized using Eq.~\myeqref{eq:model3:b0}:
%
% \begin{equation}
%   B_u(0) = \big[1 + \seasAmplitude \big(\xi_u)^r \big] D_i \, \Wshed \big[ I_{u}(0)+ \asymptomRelativeShed A_{u}(0) \big]\label{eq:model3:b0}
% \end{equation}
%
% Where $\xi_u \in (0, 1)$ are initial value parameters that we introduce in order to allow some flexibility in determining the initial state of the bacteria compartment.

% \section*{Measurement Models}
%
% Each POMP requires specification of a measurement model, which is a statistical description of how observations on the system are obtained.
% In general, we used the same measurement models that were reported in \cite{lee20}.
%
% \subsection*{Model~1}
%
% The only reported cases under this measurement model come from individuals who develop symptoms, as noted in Eq.~\myeqref{model1:meas}.
% \begin{equation}
%   \label{model1:meas}
%   y_{t} \mid \Delta N_{E_{k}I_{k}} = z_{t} \sim \text{NB}\left(\reportRate z_{t}, \obsOverdispersion \right)
% \end{equation}
%
% \subsection*{Model~2}
%
% Model~2 was fit via trajectory matching and therefore does not have an explicit measurement model.
% Implicitly, however, this fitting process is equivalent to having a Gaussian measurement process.
% Therefore the measurement model can be expressed as Eq.~\myeqref{model2:meas}.
% \begin{equation}
%   \label{model2:meas}
%   y_{u, t} \mid \Delta N_{I_{ud}R_{ud}} = z_{u, t} \sim \text{N}\left(\reportRate z_{u, t}, \obsOverdispersion^2 \right)
% \end{equation}
%
% Where $\Delta N_{I_{ud}R_{ud}}$ is the number of individuals who move from compartment $I_{ud}$ to $R_{ud}$, for unit $u \in 1:U$ and vaccination cohort $d \in 0:5$.
% We also note here that the variance parameter $\obsOverdispersion^2$ is separately fit for the epidemic and endemic phases.
%
% \subsection*{Model~3}
%
% In this model, reported cholera cases are assumed to stem from individuals who develop symptoms and seek healthcare.
% Therefore reported cases are assumed to come from an over-dispersed negative binomial model, given the increase in infected individuals:
% \begin{equation}
%   \label{model3:meas}
%   y_{u, t} \mid \Delta N_{S_{ud}I_{ud}} = z_{u, t} \sim \text{NB}\left(\reportRate z_{u, t}, \obsOverdispersion \right)
% \end{equation}
%
% This measurement model is a minor change from that used in \cite{lee20}, which allowed for a change in the reporting rate on January 1st, 2018.
% The fitted values of the reporting rate---before and after January 2018---were $0.97$ and $0.097$, respectively.
% This major change in reporting rate alone could have been the cause that Model~3 originally failed to predict the eradication of cholera, as an overnight change from near perfect to almost non-existent reporting forces the model to explain the observed decrease in cases as a decrease in the reporting of cases rather than of prevalence of cases.
% This shift was justified by a "change of the case definition that occurred on January 1st, 2018";
% this claim was not cited, and we could find no evidence that such a drastic change in the reporting rate would be warranted.


% \end{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Multiple Appendixes:                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support information, if any,             %%
%% should be provided in the                %%
%% Acknowledgements section.                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{acks}[Acknowledgments]
% The authors would like to thank ...
%\end{acks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{funding}
This work was supported by National Science Foundation grants DMS-1761603 and DMS-1646108.
%
% The second author was supported in part by ...
\end{funding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, including data   %%
%% sets and code, should be provided in     %%
%% {supplement} environment with title      %%
%% and short description. It cannot be      %%
%% available exclusively as external link.  %%
%% All Supplementary Material must be       %%
%% available to the reader on Project       %%
%% Euclid with the published article.       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Eliminating cholera in Haiti: Supplement}
\sdescription{
This document contains additional details for Models~1--3, as well as a translation table that facilitates comparisons between these models and those described in \cite{lee20}.
The supplement also demonstrates our capability to faithfully replicate the results
of \cite{lee20}.
}
\end{supplement}

\begin{supplement}
\stitle{\code{haitipkg}}
\sdescription{
This \code{R} package is contained in a public GitHub repository: zjiang2/haitipkg. The package contains all of the data and code used to create and fit the models, as well as other useful functions that were used in this article.
}
\end{supplement}

\begin{supplement}
\stitle{\code{jesseuwheeler/haiti}}
\sdescription{
This GitHub repository contains the \code{.Rnw} files that were used to create this document and the supplement material.
}
\end{supplement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  imsart-nameyear.bst  will be used to                   %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%  MR numbers will be added by VTeX.                      %%
%%                                                         %%
%%  Use \cite{...} to cite references in text.             %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% if your bibliography is in bibtex format, uncomment commands:
\bibliographystyle{imsart-nameyear} % Style BST file
\bibliography{bib-haiti}       % Bibliography file (usually '*.bib')

\end{document}

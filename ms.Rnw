%% Template for the submission to:
%%   The Annals of Applied Statistics [AOAS]
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aoas]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb,graphicx,enumerate,url,xr,lmodern}
\RequirePackage[authoryear]{natbib}
\usepackage{url}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage[normalem]{ulem}% to use \sout in feedback commands
\usepackage{bm}
\usepackage[mathscr]{euscript}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{remark}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% parameters %%%%%%%%%%%
\newcommand\Wsat{W_{\mathrm{sat}}}
\newcommand\muIR{\mu_{IR}}
\newcommand\muEI{\mu_{EI}}
\newcommand\transmission{\beta}
\newcommand\seasAmplitude{a}
\newcommand\rainfallExponent{r}
\newcommand\muRS{\mu_{RS}}
\newcommand\vaccineEfficacy{\theta}
\newcommand\muBirth{\mu_S}
\newcommand\muDeath{\delta}
\newcommand\choleraDeath{\delta_{C}}
\newcommand\symptomFrac{f}
\newcommand\asymptomRelativeInfect{\epsilon}
\newcommand\asymptomRelativeShed{\epsilon_{W}}
\newcommand\Wbeta[1]{\beta_{W#1}}
\newcommand\Wremoval{\delta_W}
\newcommand\Wshed{\mu_W}
\newcommand\mixExponent{\nu}
\newcommand\sigmaProc{\sigma_{\mathrm{proc}}}
\newcommand\reportRate{\rho}
\newcommand\obsOverdispersion{\psi}
\newcommand\phaseParm{\phi}
\newcommand\transmissionTrend{\zeta}
\newcommand\vaccClass{Z}
\newcommand\vaccCounter{z}
\newcommand\modelCounter{m}
\newcommand\missing{---}
\newcommand\fixed{\color{blue}}
\newcommand\demography{D}
\newcommand\code[1]{\texttt{#1}}
\newcommand\paramVec{\theta}

\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\varv}{\mathord}{matha}{118}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand\myeqref[1]{(\ref{#1})}
\newcommand{\blind}{1}

%% customized math macros
\newcommand\seq[2]{{#1}\!:\!{#2}}
\newcommand\R{\mathbb{R}}
\newcommand\Var{\mathrm{Var}}
\newcommand\var{\Var}
\newcommand\Cov{\mathrm{Cov}}
\newcommand\cov{\Cov}
\newcommand\iid{\mathrm{iid}}
\newcommand\dist{d}
\def\lik{L}
\def\loglik{\ell}

%%%%%% EDITING MACROS %%%%%%%%%
% orange for EI
\definecolor{orange}{rgb}{1,0.5,0}
\newcommand\ei[2]{\sout{#1} \textcolor{orange}{#2}}
\newcommand\eic[1]{\textcolor{orange}{[#1]}}
% green for JW
\definecolor{green}{rgb}{0,0.5,0}
\newcommand\jw[2]{\sout{#1} \textcolor{green}{#2}}
\newcommand\jwc[1]{\textcolor{green}{[#1]}}
% purple for JJ
\definecolor{purple}{rgb}{0.5,0,1}
\newcommand\jj[2]{\sout{#1} \textcolor{purple}{#2}}
\newcommand\jjc[1]{\textcolor{purple}{[#1]}}
% cyan for AR
\definecolor{cyan}{rgb}{0,.5,.5}
\newcommand\ar[2]{\sout{#1} \textcolor{cyan}{#2}}
\newcommand\arc[1]{\textcolor{cyan}{#1}}
% light brown for KT
\definecolor{lightbrown}{rgb}{0.5,0.5,0}
\newcommand\kt[2]{\sout{#1} \textcolor{lightbrown}{#2}}
\newcommand\ktc[1]{\textcolor{lightbrown}{#1}}

\newcolumntype{t}{>{\tiny}c}

\endlocaldefs

\begin{document}

<<Setup, include=FALSE,echo=FALSE,results='hide'>>=
library(knitr)
library(pomp)
library(panelPomp)
library(spatPomp)
library(foreach)
library(doParallel)
library(doRNG)
library(haitipkg)
library(tidyverse)

RUN_LEVEL <- 1
rl_dir <- paste0("run_level_", RUN_LEVEL, "/")

for (i in 1:3) {
  if (!dir.exists(paste0("model", i, '/', rl_dir))) {
    dir.create(paste0("model", i, '/', rl_dir), recursive = TRUE)
  }
}

opts_knit$set(concordance=TRUE)
opts_chunk$set(
    progress = TRUE,
    concordance = TRUE,
    prompt = TRUE,
    highlight = FALSE,
    tidy = TRUE,
    tidy.opts = list(
        keep.blank.line = FALSE
    ),
    comment = "",
    warning = FALSE,
    message = FALSE,
    error = TRUE,
    echo = FALSE,
    cache = FALSE,
    strip.white = TRUE,
    # results="markup",
    background = "#FFFFFF00",
    size = "normalsize",
    fig.path = "figure/",
    fig.lp = "fig:",
    fig.align = "left",
    fig.show = "asis",
    dev = "pdf",
    dev.args = list(
        bg = "transparent",
        pointsize = 9
    )
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5)))

# 40 cores for doob, 8 for ito
# doob_cores <- 40
# gl_cores <- 36

cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)

dep_labeller <- as_labeller(
  c(
    'Artibonite' = 'Artibonite',
    'Sud_Est' = 'Sud-Est',
    'Sud.Est' = 'Sud-Est',
    'Nippes' = 'Nippes',
    'Nord_Est' = 'Nord-Est',
    'Nord.Est' = 'Nord-Est',
    'Ouest' = 'Ouest',
    'Centre' = 'Centre',
    'Nord' = 'Nord',
    'Sud' = 'Sud',
    'Nord_Ouest' = 'Nord-Ouest',
    'Nord.Ouest' = 'Nord-Ouest',
    'Grande_Anse' = 'Grand\'Anse',
    'Grand.Anse' = 'Grand\'Anse'
  )
)

options(
  scipen = 0,
  help_type = "html",
  stringsAsFactors = FALSE,
  # prompt="R> ",
  continue = "+  ",
  width = 70,
  useFancyQuotes = FALSE,
  reindent.spaces = 2,
  xtable.comment = FALSE
)

dep_plot_df <- haitiCholera %>%
  select(-report) %>%
  pivot_longer(
    data = .,
    cols = -date_saturday,
    values_to = 'cases',
    names_to = 'dep'
  ) %>%
  mutate(
    date = as.Date(date_saturday),
    dep = gsub("\\.", "_", dep)
  ) %>%
  mutate(
    dep = case_when(dep == "Grand_Anse" ~ "Grande_Anse", TRUE ~ dep)
  )

true_agg_cases <- dep_plot_df %>%
  tidyr::pivot_wider(
    data = .,
    id_cols = c(date),
    names_from = dep,
    values_from = cases,
    names_prefix = 'cases_'
  ) %>%
  mutate(
    ReportedAll = cases_Artibonite + cases_Centre +
      cases_Grande_Anse + cases_Nippes + cases_Nord +
      cases_Nord_Est + cases_Ouest + cases_Sud +
      cases_Sud_Est + cases_Nord_Ouest
  )
@

<<ReplicateModel1, echo=FALSE, message=FALSE, include=FALSE>>=
# In this chunk, we replicate the results in Lee et al for model 1.
# We do this replication here in the ms.Rnw file because we need to get
# estimates for the likelihood of the models used by lee et al. The chunk has
# the following steps:
#   (1) Initialize some starting values close to those used in lee et al
#   (2) do small MIF, similar to what was done in lee et al, to get a fit for
#       epidemic period.
#   (3) Simulate from model
#   (4) Use simulations to initialize endemic phase of the model, and
#       perform MIF, similar to what lee et al did for this phase
#   (5) Simulate from model
#   (6) Calculate the likelihood of each possible set of parameters.

N_PARAMS   <- switch(RUN_LEVEL,  10,  20,  300)
N_LEE_SIMS <- switch(RUN_LEVEL,  10,  20,   20)
NMIF       <- switch(RUN_LEVEL,   5,  20,   50)
NP_MIF     <- switch(RUN_LEVEL,  20,  50,  100)  # Too few, but matches Lee et al
NP_EVAL    <- switch(RUN_LEVEL,  50, 200, 2000)
NREPS_EVAL <- switch(RUN_LEVEL,   3,   5,   36)

#-------------------------- Step 1 ----------------------

set.seed(141837)
sample_beta_nu <- function(mu_beta = 5.8, mu_nu = 0.96, n_params = N_PARAMS) {
  # This function is used to sample starting values for beta and nu.
  #
  # params:
  #   mu_beta: Average of the marginal distribution for beta1. Value was fit visually.
  #   mu_nu: Average of the marginal distribution for nu. Value was fit visually.
  # returns:
  #   data.frame of size 300x2, values of beta1 and nu with truncated bivariate
  #   normal relation.

  # Determine a good value for the marginal standard deviations
  sd_beta <- (12.1-mu_beta) / qnorm(0.995)
  sd_nu <- (0.9-mu_nu) / qnorm(0.005)

  # Create covariance matrix for the bivariate-normal distribution
  cov_mat <- rbind(
    c(sd_beta^2, -0.7 * sd_beta * sd_nu),
    c(-0.7 * sd_beta * sd_nu, sd_nu^2)
  )

  # Sample from the bivariate normal distribution
  X <- MASS::mvrnorm(n = n_params, mu = c(mu_beta, mu_nu), Sigma = cov_mat)
  colnames(X) <- c("beta1", "nu")

  # Identify values that fell outside of range, so they can be resampled
  bad_beta <- which(X[, 'beta1'] > 12.1 | X[, 'beta1'] < 0.04)
  bad_nu <- which(X[, 'nu'] > 1 | X[, 'nu'] < 0.9)
  n_resamp <- length(unique(c(bad_beta, bad_nu)))

  # Keep resampling values until all values fall within the range.
  while (n_resamp > 0) {
    to_resamp <- unique(c(bad_beta, bad_nu))
    X[to_resamp, ] <- MASS::mvrnorm(n = n_resamp, mu = c(mu_beta, mu_nu), Sigma = cov_mat)

    bad_beta <- which(X[, 'beta1'] > 12.1 | X[, 'beta1'] < 0.04)
    bad_nu <- which(X[, 'nu'] > 1 | X[, 'nu'] < 0.9)
    n_resamp <- length(unique(c(bad_beta, bad_nu)))
  }

  # Return results as a data.frame
  as.data.frame(X)
}
# Run the function to get values of beta1 and nu
starting_params <- sample_beta_nu()

sample_rho <- function(beta, n_params = N_PARAMS) {
  # This function is used to get starting values for the parameter rho, based
  # on it's strong correlation with beta1 and nu. Here, we first sample beta1
  # and nu, and then sample rho based on a regression on beta1. Additionally,
  # it is noted that rho has a bi-modal distribution, with lower values of rho
  # corresponding with high probability to higher values of beta1.
  #
  # params:
  #   beta: previously sampled values of beta1 that will be used to get values
  #         of rho.
  # returns:
  #   vector of length 300 that have an appropriate relationship with the parameter
  #   beta1.

  rhos <- rep(0, N_PARAMS)

  # Get slope and intercept for the regression
  slope <- (0.4/(5.1 - 6.75))
  intercpt <- -6.75 * slope

  # Determine whice mode should the value of rho belong to, based on regression
  case_when(
    beta > 6.75 ~ 0,  # Large beta --> group "0"
    # Small beta --> more likely to be group "1", with increasing probability as beta gets smaller
    beta <= 6.75 & beta > (0.95 - intercpt) / slope ~ slope * beta + intercpt,
    TRUE ~ 0.95
  ) -> probs

  # Sample the groups that rho belongs to
  rho_group1 <- rbernoulli(N_PARAMS, probs)
  rho_group2 <- !rho_group1

  # Keep track of how many values are in each group, used later for resampling
  n_group1 <- sum(rho_group1)
  n_group2 <- sum(rho_group2)

  # Resample values of rho that are larger than 1
  group1 <- rnorm(n_group1, mean = 1, sd = 0.05)
  n_bad1 <- sum(group1 > 1)
  while(n_bad1 > 0) {
    group1[group1 > 1] <- rnorm(n_bad1, mean = 1, sd = 0.175)
    n_bad1 <- sum(group1 > 1)
  }

  # get standard deviation for regression
  std_beta1 <- (beta[rho_group2] - mean(beta[rho_group2])) / sd(beta[rho_group2])

  group2 <- 0.25 - 0.025 * std_beta1 - (rlnorm(n_group2, sdlog = 0.2) - 1)

  # Resample values of rho in group2 as needed
  which_bad2 <- group2 < .13
  while(sum(which_bad2) > 0) {
    group2[group2 < 0.13] <- 0.25 - 0.1 * std_beta1[which_bad2] - (rlnorm(sum(which_bad2), sdlog = 0.2) - 1)

    which_bad2 <- group2 < .13
  }

  rhos[rho_group1] <- group1
  rhos[rho_group2] <- group2

  # Return the values of rho
  rhos
}
# Sample rho, using already sampled beta1.
starting_params$rho <- sample_rho(starting_params$beta1)

sample_tau <- function(mu = 4.1, n_params = N_PARAMS) {
  # Function used to sample tau. Tau is approximately independent of the other
  # parameters.
  #
  # params:
  #   mu: mean of the truncated normal distribution
  # returns:
  #   vector of length 300, sampled values of tau from truncated normal distribution

  # Sample from normal distribution
  X <- rnorm(n_params, mean = mu, sd = (1.7-mu) / qnorm(0.001))

  # Get how many resamples are needed
  n_resamp <- sum(X > 5.9 | X < 1.7)

  # Keep resampling until all values fall within desired range
  while (n_resamp > 0) {
    X[X > 5.9 | X < 1.7] <- rnorm(n_resamp, mean = mu, sd = (1.7-mu) / qnorm(0.001))

    n_resamp <- sum(X > 5.9 | X < 1.7)
  }

  # Return values of tau
  X
}
# Use function to sample tau
starting_params$tau <- sample_tau()

# Here we note that there are additional parameters that were fit, but their
# relationship between the remaining parameters is not given in Lee et al. (2020).
# Because of this, we simply sample these parameters using a truncated normal distribution:

POP <- 10911819
E_0_probs <- dnorm(seq(1, 1752, 1), mean = 400, sd = (1752-400) / qnorm(0.99))
I_0_probs <- dnorm(seq(1, 2271, 1), mean = 500, sd = (2271-500) / qnorm(0.99))
starting_params$E_0 <- sample(1:1752, size = N_PARAMS, replace = TRUE, prob = E_0_probs) / POP
starting_params$I_0 <- sample(1:2271, size = N_PARAMS, replace = TRUE, prob = I_0_probs) / POP
starting_params$S_0 <- 1 - starting_params$E_0 - starting_params$I_0

rm(sample_beta_nu, sample_rho, sample_tau, POP, I_0_probs, E_0_probs)
gc()

#-------------------------- Step 2 ----------------------

# Create epi model for haiti1
h1_epi <- haiti1()

bounds <- tribble(
  ~param, ~lower, ~upper,
  "beta2", 1e-08, 10,
  "beta3", 1e-08, 10,
  "beta4", 1e-08, 10,
  "beta5", 1e-08, 10,
  "beta6", 1e-08, 10
)

lower <- bounds$lower
names(lower) <- bounds$param

upper <- bounds$upper
names(upper) <- bounds$param

betas <- sobol_design(
  lower = lower,
  upper = upper,
  nseq  = N_PARAMS
)

epi_params <- cbind(starting_params, betas)

not_in_start_params <- names(coef(h1_epi))[!names(coef(h1_epi)) %in% colnames(epi_params)]

for (i in 1:length(not_in_start_params)) {
  epi_params[, not_in_start_params[i]] <- coef(h1_epi)[not_in_start_params[i]]
}

rm(i)

epi_params$sig_sq <- 0

# simply re-ordering to match model order
epi_params <- epi_params[, names(coef(h1_epi))]

epi_rw <- rw.sd(
  # Parameters close to desired values
  beta1 = 0.001,
  tau   = 0.001,
  nu    = 0.001,
  rho   = 0.001,
  # Parameters not close to desired values
  beta2 = 0.02,
  beta3 = 0.02,
  beta4 = 0.02,
  beta5 = 0.02,
  beta6 = 0.02,
  E_0   = ivp(0.1),
  I_0   = ivp(0.1)
)

registerDoRNG(48963587)

# Check if RUN_LEVEL == 3 results exists, if so, use them.
# Include a flag to check if RUN_LEVEL == 3. If so, we might want to recompute.
# TODO: Only save parameters and likelihoods, not entire MIF objects.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/lee1_epi_fit.rds")) {
  lee1_epi_mif <- readRDS("model1/run_level_3/lee1_epi_fit.rds")
} else {
  lee1_epi_mif <- bake(
    file = paste0("model1/", rl_dir, "lee1_epi_fit.rds"),
    {
      foreach(
        i = 1:nrow(epi_params),
        .packages = c('pomp'),
        .combine = c
      ) %dopar% {
        r_params <- unlist(epi_params[i, ])
        coef(h1_epi) <- r_params
        mif2(
          h1_epi,
          Np = NP_MIF,
          Nmif = NMIF,
          cooling.fraction.50 = 0.5,  # Cooling set so that we will reach smaller rw.sd at the end than the end of the unit3 search.
          rw.sd = epi_rw
        )
      }
    }
  )
}

rm(epi_rw, epi_params, not_in_start_params,
   lower, upper, betas, bounds, starting_params)
gc()

# epi_complete <- as.data.frame(t(coef(lee1_epi_mif)))
# epi_complete$logLik <- logLik(lee1_epi_mif)

# GGally::ggpairs(
#   filter(epi_results, logLik > -2200),
#   columns = c("rho", "tau", "beta1", "nu", "logLik")
# )

# epi_complete <- filter(epi_complete, logLik > -2200)

if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/lee1_epi_evals.rds")) {
  lee_epi_ll <- readRDS("model1/run_level_3/lee1_epi_evals.rds")
} else {
  lee_epi_ll <- bake(
    file = paste0("model1/", rl_dir, "lee1_epi_evals.rds"), {

      # Create data.frame to save results
      lee_epi_ll <- data.frame(
        "pfLL" = rep(0, length(lee1_epi_mif)),
        "pfse" = rep(0, length(lee1_epi_mif))
      )

      for (j in 1:nrow(lee_epi_ll)) {
        # Get parameters of interest
        pf_params <- coef(lee1_epi_mif[[j]])

        # make results reproducible
        registerDoRNG((j * 687383921) %% 7919)

        # Calculate log likelihoods
        ll_evals <- foreach(i=1:NREPS_EVAL, .combine = c) %dopar% {
          logLik(pfilter(h1_epi, params = pf_params, Np = NP_EVAL))
        }
        lee_epi_ll[j, 'pfLL'] <- logmeanexp(ll_evals, se = TRUE)[1]
        lee_epi_ll[j, 'pfse'] <- logmeanexp(ll_evals, se = TRUE)[2]
      }

      lee_epi_ll
    }
  )
}

# First, we need to simulate the epidemic model using the fitted parameters.
registerDoRNG(18599687)
foreach(
  i = 1:length(lee1_epi_mif),
  .combine = rbind
) %dopar% {
  guess <- coef(lee1_epi_mif)
  sims <- simulate(h1_epi, nsim = N_LEE_SIMS, format = 'data.frame',
                   params = guess)  # 20 sims for each set of parameters
  sims$param_set <- i
  sims
} -> all_sims

#-------------------------- Step 3 ----------------------

# quants <- all_sims %>%
#   select(.id, week, cases) %>%
#   group_by(week) %>%
#   summarize(
#     q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
#     q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
#     q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
#   ) %>%
#   ungroup() %>%
#   mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

#-------------------------- Step 4 ----------------------

h1_end <- haiti1(period = "endemic")
start_states <- all_sims %>% filter(week == max(week))

end_rw <- rw.sd(
  beta1 = 0.05,
  tau   = 0.005,
  nu    = 0.04,
  rho   = 0.01,
  beta2 = 0.05,
  beta3 = 0.05,
  beta4 = 0.05,
  beta5 = 0.05,
  beta6 = 0.05
)

registerDoParallel(20)
registerDoRNG(21489587)

end_start <- t(coef(lee1_epi_mif))

# end_start <- epi_complete %>%
#   select(-logLik)

# TODO: Only save parameter values and "likelihoods".
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/lee1_end_fit.rds")) {
  lee1_end_mif <- readRDS("model1/run_level_3/lee1_end_fit.rds")
} else {
  lee1_end_mif <- bake(
    file = paste0("model1/", rl_dir, "lee1_end_fit.rds"),
    {
      foreach(
        i = 1:nrow(end_start),
        .packages = c('pomp'),
        .combine = c
      ) %dopar% {

        output <- list()
        r_params <- end_start[i, ]

        start_states %>%
          filter(param_set == i) %>%
          select(-.id, -week, -param_set) %>%
          sapply(., median, na.rm = TRUE) -> rinit_parms

        # Set the inital state
        new_rinit <- Csnippet(
          sprintf(
            "
          S = nearbyint(%f);
          E = nearbyint(%f);
          I = nearbyint(%f);
          A = nearbyint(%f);
          R = nearbyint(%f);
          incid = nearbyint(%f);
          foival = %f;
          Str0 = nearbyint(%f);
          Sout = nearbyint(%f);
          Sin = nearbyint(%f);
          ",
          rinit_parms['S'],
          rinit_parms['E'],
          rinit_parms['I'],
          rinit_parms['A'],
          rinit_parms['R'],
          rinit_parms['incid'],
          rinit_parms['foival'],
          rinit_parms['Str0'],
          rinit_parms['Sout'],
          rinit_parms['Sin']
          )
        )

        h1_end <- h1_end %>% pomp(
          rinit = new_rinit,
          statenames = c(
            "S", "E", "I", "A", "R", "incid",
            "foival", "Str0", "Sout", "Sin"
          )
        )

        coef(h1_end) <- r_params
        mif2(
          h1_end,
          Np = NP_MIF,
          Nmif = NMIF,
          cooling.type = "hyperbolic",
          cooling.fraction.50 = 0.05,  # Cooling set so that we will reach smaller rw.sd at the end than the end of the unit3 search.
          rw.sd = end_rw
        )
        # output[['m2']] <- m2
        #
        # coef(h1_end) <- coef(m2)
        # sims <- simulate(  # Step 5
        #   h1_end, nsim = N_LEE_SIMS, format = 'data.frame',
        #   params = coef(m2)
        # )
        # sims$param_set <- i
        #
        # output[['sims']] <- sims
        # output
      }
    }
  )
}

rm(end_rw, start_states)
gc()

if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/lee1_end_evals.rds")) {
  lee_end_ll <- readRDS("model1/run_level_3/lee1_end_evals.rds")
} else {
  lee_end_ll <- bake(
    file = paste0("model1/", rl_dir, "lee1_end_evals.rds"), {

      # Create data.frame to save results
      lee_end_ll <- data.frame(
        "pfLL" = rep(0, length(lee1_end_mif)),
        "pfse" = rep(0, length(lee1_end_mif))
      )

      # Perform particle filter to get likelihood estimates
      for (j in 1:nrow(lee_end_ll)) {
        # Get parameters of interest
        pf_params <- coef(lee1_end_mif[[j]])

        # make results reproducible
        registerDoRNG((j * 687383921) %% 7919)

        # Calculate log likelihoods
        ll_evals <- foreach(i=1:NREPS_EVAL, .combine = c) %dopar% {
          logLik(pfilter(h1_end, params = pf_params, Np = NP_EVAL))
        }

        lee_end_ll[j, 'pfLL'] <- logmeanexp(ll_evals, se = TRUE)[1]
        lee_end_ll[j, 'pfse'] <- logmeanexp(ll_evals, se = TRUE)[2]
      }

      lee_end_ll
    }
  )
}

rm(lee1_end_mif, lee1_epi_mif)
gc()

# MIF_params <- matrix(nrow = length(lee1_end_mif$m2) / 2, ncol = length(coef(lee1_end_mif[[1]])))
# all_sims_end <- data.frame()
# liks <- rep(0, nrow(end_start))
# for (i in 1:nrow(end_start)) {
#   MIF_params[i, ] <- coef(lee1_end_mif[[(2 * i) - 1]])
#   all_sims_end <- rbind(all_sims_end, lee1_end_mif[[(2 * i)]])
#   liks[i] <- logLik(lee1_end_mif[[(2 * i) - 1]])
# }
#
# colnames(MIF_params) <- names(coef(lee1_end_mif[[1]]))
# end_results <- as.data.frame(MIF_params)
# end_results$logLik <- liks

# GGally::ggpairs(
#   filter(end_results, logLik > -2200),
#   columns = c("rho", "tau", "beta1", "nu", "logLik")
# )

#-------------------------- Step 6 ----------------------

mod1_lee_ll <- max(lee_end_ll$pfLL, na.rm = TRUE) + max(lee_epi_ll$pfLL, na.rm = TRUE)

rm(
  lee_end_ll, lee_epi_ll, h1_end, h1_epi, all_sims, end_start,
  N_LEE_SIMS, N_PARAMS, NMIF, NP_EVAL, NP_MIF, NREPS_EVAL
)
gc()
@



\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Informing policy via dynamic models: Eliminating cholera in Haiti}
\runtitle{Eliminating Cholera in Haiti}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Jesse}~\snm{Wheeler}\ead[label=e1]{jeswheel@umich.edu}\orcid{0000-0003-3941-3884}},
\author[A]{\fnms{AnnaElaine}~\snm{Rosengart}\ead[label=e2]{aelr@umich.edu}}
\author[A]{\fnms{Zhuoxun}~\snm{Jiang}\ead[label=e3]{zhuoxunj@umich.edu}},
\author[A]{\fnms{Kevin}~\snm{Hao En Tan}\ead[label=e4]{kevtan@umich.edu}},
\author[A]{\fnms{Noah}~\snm{Treutle}\ead[label=e5]{ntreutle@umich.edu}}
\and
\author[A]{\fnms{Edward}~\snm{Ionides}\ead[label=e6]{ionides@umich.edu}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Statistics Department, University of Michigan\printead[presep={,\ }]{e1,e2,e3,e4,e5,e6}}
\end{aug}

\begin{abstract}
Public health decisions must be made about when and how to implement interventions to control an infectious disease epidemic.
These decisions should be informed by data on the epidemic as well as current understanding about the transmission dynamics.
Such decisions can be posed as statistical questions about scientifically motivated dynamic models.
Thus, we encounter the methodological task of building credible, data-informed decisions based on stochastic, partially observed, nonlinear dynamic models.
This necessitates addressing the tradeoff between biological fidelity and model simplicity, and the reality of misspecification for models at all levels of complexity.
As a case study, we consider a cholera epidemic in Haiti.
The 2010 introduction of cholera to Haiti led to an extensive outbreak and sustained transmission until it was eliminated in 2019.
We study three models developed by expert teams to advise on vaccination policies.
We assess methods used for fitting and evaluating these models, leading to recommendations for future studies.
Diagnosis of model misspecification and development of alternative models can lead to improved statistical fit, but caution is nevertheless required in drawing policy conclusions based on causal interpretations of the models.
\end{abstract}

\begin{keyword}
  \kwd{Partially observed Markov process}
  \kwd{Hidden Markov model}
  \kwd{infectious disease}
  \kwd{cholera}
  \kwd{sequential Monte Carlo}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:

\section{Introduction}

Quantitative models for dynamic systems offer potential for designing effective control measures.
Regulation of biological populations is a fundamental topic in epidemiology, ecology, fisheries and agriculture.
Quantitative models for these population dynamics may be nonlinear and stochastic, with the resulting complexities compounded by incomplete understanding of the underlying biological mechanisms and by partial observability of the system variables.
Developing and testing such models, and assessing their fitness for guiding policy, is a challenging statistical task.
Questions of interest include: What indications should we look for in the data to assess whether the model-based inferences are trustworthy?
What diagnostic tests and model variations can and should be considered in the course of the data analysis?
What are the possible trade-offs of increasing model complexity, such as the inclusion of interactions across spatial units?

This case study investigates the use of dynamic models and spatiotemporal data to inform a policy decision in the context of the cholera outbreak in Haiti, which started in 2010.
We build on a multi-group modeling exercise by \citet{lee20} in which four expert modeling teams developed models to the same dataset with the goal of comparing conclusions on the feasibility of eliminating cholera by a vaccination campaign.
Model~1 is stochastic and describes cholera at the national level;
Model~2 is deterministic with spatial structure, and includes transmission via contaminated water;
Model~3 is stochastic with spatial structure, and accounts for measured rainfall.
Model~4 has an agent-based construction, featuring considerable mechanistic detail but limited ability to calibrate these details to data.
The strengths and weaknesses of the agent-based modeling approach  \citep{tracy18} are outside the scope of this article, and we focus on Models~1--3.

The four independent teams were given the task of estimating the
potential effect of prospective oral cholera vaccine (OCV) programs.
While OCV is accepted as a safe and effective tool for controlling the spread of cholera, the global stockpile of OCV doses remains limited \citep{pezzoli20}.
Advances in OCV technology and vaccine availability, however, raised the possibility of planning a national vaccination program \citep{lee20}.
In the study, certain data were shared between the groups, including demography and vaccination history; vaccine efficacy was also fixed at a shared value between groups.
Beyond this, the groups made autonomous decisions on what to include and exclude from their models; this autonomy reduced the possible effect that assumptions about the dynamic system may have on the final conclusion of the study.
Despite this autonomy, and largely adhering to existing guidelines on creating models to inform policy \citep{behrend20,saltelli20}, the consensus across the four models was that an extensive nationwide vaccination campaign would be necessary to eliminate cholera from Haiti.
This conclusion is inconsistent with the fact that there have been no confirmed cases since February, 2019 \citep{trevisin22}, despite the lack of a concentrated vaccination effort.

The failure of \cite{lee20} to correctly predict the elimination of cholera has been debated \citep{francois20,rebaudetComment20,henrys20,leeReply20}.
\citet{rebaudetComment20} suggested that the models proposed by \citet{lee20} were too unrealistic.
We find a more nuanced conclusion: attention to methodological details in model fitting, diagnosis and forecasting can improve each of the proposed model's ability to quantitatively describe observed data.
These improvements results in forecasts that are more consistent with the observed outcome, without requiring major changes to the model structures.
Based on this retrospective analysis, we offer suggestions on fitting mechanistic models to dynamic systems for future studies.

We proceed by introducing Models~1--3 in Sec.~\ref{sec:models};
in Sec.~\ref{sec:methods}, we present a methodological approach to examining and refining these models, and then use improved model fits to project cholera incidence in Haiti under various vaccination scenarios.
We then conclude with recommendations on the use of mechanistic models to inform policy decisions in Sec.~\ref{sec:advice}.

\begin{figure}[ht]
<<Plot_Reported_Cases, echo=FALSE, fig.height=3.5>>=
plot_df <- haitiCholera %>%
  select(-report) %>%
  mutate(date = as.Date(date_saturday)) %>%
  select(-date_saturday) %>%
  pivot_longer(
    data = .,
    cols = -c(date),
    names_to = 'Departement',
    values_to = "Cases",
  )

ggplot(plot_df, aes(x = date, y = Cases + 1)) +
  facet_wrap(~Departement, nrow = 2, labeller = dep_labeller) +
  geom_line() +
  theme(
    axis.title.x = element_blank()
  ) +
  ylab('Reported Cases') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years'))

@
\caption{\label{CholeraData}
\small
Reported Cholera cases in the outbreak in Haiti from 2010-2019.
}
\end{figure}

\section{Mechanistic models for cholera in Haiti}\label{sec:models}
Models that focus on learning relationships between variables in a dataset are called {\it associative}, whereas models that incorporate a known scientific property of the system are called {\it causal} or {\it mechanistic}.
The danger in using forecasting techniques which rely on associative models to predict the consequence of interventions is called the Lucas critique in an econometric context.
\citet{lucas76} pointed out that it is naive to predict the effects of an intervention on a given system based entirely on historical associations.
To successfully predict the effect of an intervention, a model should therefore both provide a quantitative explanation of existing data and should have a causal interpretation: a manipulation of the system should correspond quantitatively with the corresponding change to the model.
This motivates the development of mechanistic statistical models, which provides a statistical fit to the available data while also supporting a causal intepretation.

The deliberate limitation of coordination between the groups of \citet{lee20} allows us to treat the models as fairly independently developed expert approaches to understanding cholera transmission.
However, it led to differences in notation, and in subsets of the data chosen for analysis, that hinder direct comparison.
Here, we have put all three models into a common notational framework.
Translations back to the original notation of \citet{lee20} are given in Table~S-1.

Each model describes the cholera dynamics using a latent state vector $\bm{X}^{(\modelCounter)}(t)$ for each continuous time-point $t \in \mathcal{T}$, where $\mathcal{T}$ is the set of all time-points and $\modelCounter \in \{1, 2, 3\}$ indexes the model.
The observation at time $t_n$ is modeled by a response vector $\bm{Y}^{(\modelCounter)}_n$.
The latent state vector $\bm{X}^{(\modelCounter)}(t)$ consists of individuals labeled as susceptible (S), infected (I), asymptomatically infected (A), vaccinated (V), and recovered (R), with various sub-divisions sometimes considered in each model for $\modelCounter \in \{1, 2, 3\}$.
Models~2 and~3 have metapopulation structure, meaning that each individual is a member of a spatial unit, denoted by a subscript $u\in \seq{1}{U}$.
Here, the spatial units are the $U=10$ Haitian administrative d\'{e}partements (henceforth anglicized as departments).

In the following subsections, complete descriptions of Models~1--3 are provided.
While the model description is scientifically critical, as well as being necessary for transparency and reproducibility, the model details are not essential to our methodological discussions of how to diagnose and address model misspecification with the purpose of informing policy.
A first-time reader may choose to skim through the rest of this section, and return later.

%%%%%%%%% 11111111111 %%%%%%%%%%

\subsection{Model~1}
\label{sec:model1}
$\bm{X}^{(1)}(t) = \big(S_{\vaccCounter}(t),  E_{\vaccCounter}(t), I_{\vaccCounter}(t), A_{\vaccCounter}(t), R_{\vaccCounter}(t), \vaccCounter \in 0:\vaccClass\big)$ describes susceptible, latent (exposed), infected (and symptomatic), asymptomatic, and recovered individuals in vaccine cohort $\vaccCounter$.
Here, $\vaccCounter=0$ corresponds to unvaccinated individuals, and $\vaccCounter \in \seq{1}{\vaccClass}$ describes hypothetical vaccination programs.
The force of infection is
\begin{equation}
\label{model1:lambda}
\lambda(t) = \Big(\sum_{\vaccCounter=0}^{\vaccClass}  I_{\vaccCounter}(t) + \asymptomRelativeInfect \sum_{\vaccCounter=0}^{\vaccClass} A_{\vaccCounter}(t) \Big)^\nu \frac{d\Gamma(t)}{dt} \transmission(t)/N
\end{equation}
where $\transmission(t)$ is a periodic cubic spline representation of seasonality, given in terms of a B-spline basis $\{ s_j(t), j\in \seq{1}{6}\}$ and parameters $\transmission_{1:6}$ as
\begin{equation}
\label{model1:beta}
\log\transmission(t) = \sum_{j=1}^6 \transmission_j s_j(t).
\end{equation}
The process noise $d\Gamma(t)/dt$ is multiplicative Gamma-distributed white noise, with infinitesimal variance parameter $\sigmaProc^2$.
\cite{lee20} included process noise in Model~3 but not in Model~1, i.e., they fixed $\sigmaProc^2 = 0$.
Gamma white noise in the transmission rate gives rise to an over-dispersed latent Markov process \citep{breto11} which has been found to improve the statistical fit of disease transmission models \citep{stocks20,he10}.

Per-capita transition rates are given in Equations~\ref{model1:SE}-\ref{model1:birth}:
\begin{eqnarray}
\label{model1:SE}
\mu_{S_{\vaccCounter}E_{\vaccCounter}} &=& \lambda(t)
\\[-3pt]
\label{model1:EI}
\mu_{E_{\vaccCounter}I_{\vaccCounter}} &=& \muEI\big(1-\symptomFrac_{\vaccCounter}(t)\big)
\\[-3pt]
\label{model1:EA}
\mu_{E_{\vaccCounter}A_{\vaccCounter}} &=& \muEI\, \symptomFrac_{\vaccCounter}(t)
\\[-3pt]
\label{model1:toR}
\mu_{I_{\vaccCounter}R_{\vaccCounter}} &=& \mu_{A_{\vaccCounter}R_{\vaccCounter}} = \muIR
\\[-3pt]
\label{model1:RS}
\mu_{R_{\vaccCounter}S_{\vaccCounter}} &=& \muRS
\\[-3pt]
\label{model1:vacc}
\mu_{S_0S_{\vaccCounter}} &=& \mu_{E_0E_{\vaccCounter}} = \mu_{I_0I_{\vaccCounter}} = \mu_{A_0A_{\vaccCounter}} = \mu_{R_0R_{\vaccCounter}} = \eta_{\vaccCounter}(t)
\\[-3pt]
\label{model1:death}
\mu_{S_{\vaccCounter}\demography} &=& \mu_{E_{\vaccCounter}\demography} = \mu_{I_{\vaccCounter}\demography} = \mu_{A_{\vaccCounter}\demography}=\mu_{R_{\vaccCounter}\demography} = \delta
\\
\label{model1:birth}
\mu_{\demography S_0} &=& \muBirth
\end{eqnarray}
where $\vaccCounter\in \seq{0}{\vaccClass}$.
Here, $\mu_{AB}$ is a transition rate from compartment $A$ to $B$.
We have an additional demographic source and sink compartment $\demography$ modeling entry into the study population due to birth or immigration, and exit from the study population due to death or immigration.
Thus, $\mu_{A\demography}$ is a rate of exiting the study population from compartment $A$ and $\mu_{\demography B}$ is a rate of entering the study population into compartment $B$.

In Model~1, the advantage afforded to vaccinated individuals is an increased probability that an infection is asymptomatic.
Conditional on infection status, vaccinated individuals are also less infectious than their non-vaccinated counterparts by a rate of $\asymptomRelativeInfect = 0.05$ in Eq.~\eqref{model1:lambda}.
In \eqref{model1:EA} and~\eqref{model1:EI} the asymptomatic ratio for non-vaccinated individuals is set $\symptomFrac_0(t)=0$, so that the asymptomatic route is reserved for vaccinated individuals.
For $\vaccCounter\in\seq{1}{\vaccClass}$, the vaccination cohort $\vaccCounter$ is assigned a time $\tau_{\vaccCounter}$, and we take $\symptomFrac_{\vaccCounter}(t) = c \, \theta^*(t-\tau_{\vaccCounter})$
% \begin{equation}
% \label{model1:theta_{\vaccCounter}}
% \symptomFrac_{\vaccCounter}(t) = c \, \theta^*(t-\tau_{\vaccCounter})
% \end{equation}
% \arc{I think we could put this in Table 2 and replace this chunk with a sentence or two defining $\symptomFrac_0(t)$ in words as the proportion of exposed individuals who develop into symptomatic cases and adding a reference to Table 2. This would save a good quarter of a page or so.}
where $\theta^*(t)$ is efficacy at time $t$ since vaccination for adults, taken from \citet{lee20}, Table~S4, and $c=\big(1-(1-0.4688)\times 0.11\big)$ is a correction to allow for reduced efficacy in the 11\% of the population aged under 5 years.
Single and double vaccine doses were modeled by changing the waning of protection; protection was assumed to be equal between single and double dose until 52 weeks after vaccination, at which point the single dose becomes ineffective.

%%%%%%% 222222222222 %%%%%%%%%%

\subsection{Model~2}
\label{sec:model2}
Susceptible individuals are in compartments $S_{u\vaccCounter}(t)$, where $u\in\seq{1}{U}$ corresponds to the $U=10$ departments, and $\vaccCounter\in\seq{0}{4}$ describes vaccination status:
\begin{itemize}
  \item[$\vaccCounter=0$:] Unvaccinated or waned vaccination protection.
  \item[$\vaccCounter=1$:] One dose at age under five years.
  \item[$\vaccCounter=2$:] Two doses at age under five years.
  \item[$\vaccCounter=3$:] One dose at age over five years.
  \item[$\vaccCounter=4$:] Two doses at age over five years.
\end{itemize}

Individuals can progress to a latent infection $E_{u\vaccCounter}$ followed by symptomatic infection $I_{u\vaccCounter}$ with recovery to $R_{u\vaccCounter}$ or asymptomatic infection $A_{u\vaccCounter}$ with recovery to $R^A_{u\vaccCounter}$.
The force of infection depends on both direct transmission and an aquatic reservoir, $W_u(t)$, and is given by
\begin{equation}
\label{model2:lambda}
\lambda_{u}(t) = 0.5\big(1+\seasAmplitude \cos(2\pi t + \phaseParm)\big)
\frac{\beta_W\, W_u(t)}{ \Wsat  + W_u(t)} +
\transmission \left\{\sum_{\vaccCounter=0}^4 I_{u\vaccCounter}(t) + \asymptomRelativeInfect \sum_{\vaccCounter=0}^4 A_{u\vaccCounter}(t) \right\}
\end{equation}
The latent state is therefore described by the vector $\bm{X}^{(2)}(t) = \big(S_{u\vaccCounter}(t),\allowbreak E_{u\vaccCounter}(t),\allowbreak I_{u\vaccCounter}(t),\allowbreak A_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter}^A(t),\allowbreak W_u,\allowbreak u \in \seq{1}{U},\allowbreak \vaccCounter \in \seq{0}{4}\big)$.
The cosine term in Eq.~\myeqref{model2:lambda} accounts for annual seasonality, with a phase parameter $\phaseParm$.
The implementation of Model~2 in \cite{lee20} fixes $\phaseParm = 0$.

Individuals move from department $u$ to $v$ at rate $T_{uv}$, and aquatic cholera moves at rate $T^W_{uv}$.
The nonzero transition rates are
\begin{eqnarray}
\label{model2:mu_SE}
\mu_{S_{u\vaccCounter}E_{u\vaccCounter}} &=& \vaccineEfficacy_\vaccCounter \, \lambda
\\[-3pt]
\label{model2:mu_EI}
\mu_{E_{u\vaccCounter}I_{u\vaccCounter}} &=& \symptomFrac\muEI, \quad \mu_{E_{u\vaccCounter}A_{u\vaccCounter}} = (1-\symptomFrac)\muEI
\\[-3pt]
\label{model2:mu_IR}
\mu_{I_{u\vaccCounter}R_{u\vaccCounter}} &=& \mu_{A_{u\vaccCounter}R^A_{u\vaccCounter}} = \muIR
\\[-3pt]
\label{model2:RS}
\mu_{R_{u\vaccCounter}S_{u\vaccCounter}} &=& \mu_{R^A_{u\vaccCounter}S_{u\vaccCounter}} = \muRS
\\[-3pt]
\label{model2:transport}
\mu_{S_{u\vaccCounter}S_{{\varv}\vaccCounter}} &=& \mu_{E_{u\vaccCounter}E_{{\varv}\vaccCounter}} = \mu_{I_{u\vaccCounter} I_{{\varv}\vaccCounter}} = \mu_{A_{u\vaccCounter}A_{{\varv}\vaccCounter}} = \mu_{R_{u\vaccCounter}R_{{\varv}\vaccCounter}} = \mu_{R^A_{u\vaccCounter} R^A_{{\varv}\vaccCounter}} = T_{u\varv}
\\[-3pt]
\label{model2:omega1}
\mu_{S_{u1}S_{u0}} &=& \mu_{S_{u3}S_{u0}} = \omega_1
\\[-3pt]
\label{model2:omega2}
\mu_{S_{u2}S_{u0}} &=& \mu_{S_{u4}S_{u0}} = \omega_2
\\[-3pt]
\label{model2:to_W}
\mu_{\demography W_u} &=& \Wshed \left\{ \sum_{\vaccCounter=0}^4 I_{u\vaccCounter}(t) + \asymptomRelativeShed \sum_{\vaccCounter=0}^4 A_{u\vaccCounter}(t) \right\}
\\[-3pt]
\label{model2:from_W}
\mu_{W_u\demography} &=& \Wremoval
\\[-3pt]
\label{model2:water_transport}
\mu_{W_uW_{\varv}} &=& w_r T^W_{u\varv}
\end{eqnarray}
In \eqref{model2:transport} the spatial coupling is specified by a gravity model,
\begin{equation}
\label{model2:gravity}
T_{u\varv} = v_{\mathrm{rate}} \times \frac{\mathrm{Pop}_u \mathrm{Pop}_{\varv}}{D_{u\varv}^2},
\end{equation}
where $\mathrm{Pop}_u$ is the mean population for department $u$,
$D_{u\varv}$ is a distance measure estimating average road distance between rancomly chosen members of each population, and $v_{\mathrm{rate}}= 10^{-12}$ was treated as a fixed constant.
In \eqref{model2:water_transport}, $T^W_{u\varv}$ is a measure of river flow between departments.
The unit of $W_u(t)$ is cells per ml, with dose response modeled via a saturation constant of $\Wsat$ in \eqref{model2:lambda}.


%%%%%%% 333333333 %%%%%%%%%%

\subsection{Model~3}
\label{sec:model3}

The latent state is described as $\bm{X}^{(3)}(t) = \big(S_{u\vaccCounter}(t),\allowbreak I_{u\vaccCounter}(t),\allowbreak A_{u\vaccCounter}(t),\allowbreak R_{u\vaccCounter k}(t),\allowbreak W_u(t),\allowbreak u \in \seq{0}{U},\allowbreak \vaccCounter \in \seq{0}{4},\allowbreak k \in \seq{1}{3}\big)$.
Here, $\vaccCounter=0$ corresponds to unvaccinated, $\vaccCounter=2j-1$ corresponds to a single dose on the $j$th vaccination campaign in unit $u$ and $\vaccCounter=2j$ corresponds to receiving two doses on the $j$th vaccination campaign.
$k\in\seq{1}{3}$ models non-exponential duration in the recovered class before waning of immunity.
The force of infection is
\begin{eqnarray}
\label{eq:model3:foi}
\lambda_u(t) &=& \Wbeta{_u} \frac{W_u(t)}{1+W_u(t)} + \transmission_u \sum_{{\varv}\neq u}\big(I_{\varv 0}(t)+ \asymptomRelativeInfect A_{\varv 0}(t)\big)
\\[-3pt]
\label{eq:model3:SI}
\mu_{S_{u\vaccCounter}I_{u\vaccCounter}} &=& \symptomFrac \,  \lambda_u \big(1-\eta_{u\vaccCounter}(t)\big) \, d\Gamma/dt
\\[-3pt]
\label{eq:model3:SA}
\mu_{S_{u\vaccCounter}A_{u\vaccCounter}} &=& (1-\symptomFrac) \,  \lambda_u \big(1-\eta_{u\vaccCounter}(t)\big) \,  d\Gamma/dt
\\[-3pt]
\label{eq:model3:IR}
\mu_{I_{u\vaccCounter}R_{u\vaccCounter 1}} &=& \mu_{A_{u\vaccCounter}R_{u\vaccCounter 1}} = \muIR
\\[-3pt]
\label{eq:model3:IS}
\mu_{I_{u\vaccCounter}S_{u0}} &=& \muDeath + \choleraDeath
\\[-3pt]
\label{eq:model3:AS}
\mu_{A_{u\vaccCounter}S_{u0}} &=& \muDeath
\\[-3pt]
\label{eq:model3:RRnext}
\mu_{R_{u\vaccCounter 1}R_{u\vaccCounter 2}} &=& \mu_{R_{u\vaccCounter 2}R_{u\vaccCounter 3}} = 3\muRS
\\[-3pt]
\label{eq:model3:RS}
\mu_{R_{u\vaccCounter k}S_{u0}} &=& \muDeath + 3\muRS \, \mathbf{1}_{\{k=3\}}
\\[-3pt]
\label{eq:model3:water}
\mu_{{\demography}W_u} &=& \big[1 + \seasAmplitude \big(J(t))^r \big] D_i \, \Wshed \big[ I_{u0}(t)+ \asymptomRelativeShed A_{u0}(t) \big]
\\[-3pt]
\label{eq:model3:Decay}
\mu_{W_u\demography} &=& \Wremoval
\end{eqnarray}

As with Model~1, $d\Gamma_u(t)/dt$ is multiplicative Gamma-distributed white noise in \myeqref{eq:model3:SI} and \myeqref{eq:model3:SA}.
In \eqref{eq:model3:water}, $J_u(t)$ is a dimensionless measurement of precipitation that has been standardized by dividing the observed rainfall at time $t$ by the maximum recorded rainfall in department $u$ during the epidemic, and $D_u$ is the average population density.
Demographic stochasticity is accounted for by modeling non-cholera related death rate $\muDeath$ in each compartment, along with an additional death rate $\choleraDeath$ in \myeqref{eq:model3:IS} to account for cholera induced deaths among infected individuals.
All deaths are balanced by births into the susceptible compartment in \myeqref{eq:model3:AS} and \myeqref{eq:model3:RS}, thereby maintaining constant population in each department.

\section{Statistical Analysis}\label{sec:methods}

We consider model fitting (Sec.~\ref{sec:model_fitting}) followed by diagnostic investigations (Sec.~\ref{sec:model_diagnostics}) and forecasting (Sec.~\ref{sec:filter}).

<<FitModel1, echo=FALSE, message=FALSE, include=FALSE>>=
set.seed(636813)

NP_H1        <- switch(RUN_LEVEL, 100, 1000,   2000)
NMIF_H1      <- switch(RUN_LEVEL,  10,   50,    200)
NUM_TREND    <- switch(RUN_LEVEL,   5,   10,     90)
NPROF        <- switch(RUN_LEVEL,   4,    4,     24)
NP_EVAL      <- switch(RUN_LEVEL, 200, 1000,   5000)
NREPS_EVAL   <- switch(RUN_LEVEL, cores, 20,  cores)


if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/haiti1_fit.rds")) {
  h1_MIF <- readRDS("model1/run_level_3/haiti1_fit.rds")
} else {
  h1_MIF <- bake(
    file = paste0("model1/", rl_dir, "haiti1_fit.rds"), {
      fit_haiti1(
        NP = NP_H1,
        NMIF = NMIF_H1,
        NUM_TREND = NUM_TREND,
        NPROF = NPROF,
        NREPS_EVAL = NREPS_EVAL,
        NP_EVAL = NP_EVAL,
        ncores = cores
      )
    }
  )
}

h1_prof_res <- h1_MIF %>%
  group_by(betat) %>%
  summarize(logLik = max(ll))

mcap_results <- pomp::mcap(h1_prof_res$logLik, h1_prof_res$betat, span = 1)

# Get the best set of parameters from MIF2 search.
p1 <- h1_MIF %>%
  filter(ll == max(ll)) %>%
  select(-ll, -ll.se) %>%
  unlist()

mod1_ll <- h1_MIF %>%
  filter(ll == max(ll)) %>%
  select(ll) %>%
  unlist()

rm(NP_H1, NMIF_H1, NUM_TREND, NPROF, NREPS_EVAL, NP_EVAL, h1_MIF)
gc()

@


<<FitModel3, echo=FALSE, include=FALSE, message=FALSE>>=
set.seed(636813)

if (RUN_LEVEL == 1){

  # Global search, All params
  SEARCH1 <- list(
    NP = 50, NMIF = 3, NREPS = 3,
    NREPS_EVAL = 3, NP_EVAL = 100
  )

  # Local search, unit specific params
  SEARCH2 <- list(
    TOP_N = 1, NP = 50, NMIF = 3, NREPS = 3, NREPS_EVAL = 3,
    NP_EVAL = 100
  )

  # Number of searches
  N_SEARCHES <- 2L

  # THESE SEARCHES AREN'T CONDUCTED
  SEARCH3 <- list(
    TOP_N = 1, NP = 20, NMIF = 3, NREPS = 3, NREPS_EVAL = 3, NP_EVAL = 25
  )

  SEARCH4 <- list(
    TOP_N = 1, NP = 20, NMIF = 3, NREPS = 3, NREPS_EVAL = 3, NP_EVAL = 25
  )

  SEARCH5 <- list(
    TOP_N = 1, NP = 20, NMIF = 3, NREPS = 3, NREPS_EVAL = 3, NP_EVAL = 25
  )
} else if (RUN_LEVEL == 2) {

  # Global search, all parameters
  SEARCH1 <- list(
    NP = 200, NMIF = 10, NREPS = 36, NREPS_EVAL = 36, NP_EVAL = 500
  )

  # Local search, unit specific
  SEARCH2 <- list(
    TOP_N = 2, NP = 200, NMIF = 20, NREPS = 18, NREPS_EVAL = 36, NP_EVAL = 500
  )

  # Local search, shared parameters
  SEARCH3 <- list(
    TOP_N = 2, NP = 200, NMIF = 10, NREPS = 18, NREPS_EVAL = 36, NP_EVAL = 500
  )

  # Local search, unit specific
  SEARCH4 <- list(
    TOP_N = 2, NP = 200, NMIF = 10, NREPS = 18, NREPS_EVAL = 36, NP_EVAL = 500
  )

  # Local search, all parameters
  SEARCH5 <- list(
    TOP_N = 2, NP = 200, NMIF =  5, NREPS = 18, NREPS_EVAL = 36, NP_EVAL = 500
  )

  # Number of searches
  N_SEARCHES <- 5L
} else if (RUN_LEVEL == 3) {

  # Global search, all parameters
  SEARCH1 <- list(
    NP = 1500, NMIF = 100, NREPS = 144, NREPS_EVAL = 36, NP_EVAL = 2000
  )

  # Local search, unit specific parameters
  SEARCH2 <- list(
    TOP_N = 4, NP = 2000, NMIF = 100, NREPS = 18, NREPS_EVAL = 36, NP_EVAL = 4000
  )

  # Local search, shared parameters
  SEARCH3 <- list(
    TOP_N = 3, NP = 4000, NMIF = 50, NREPS = 24, NREPS_EVAL = 36, NP_EVAL = 5000
  )

  # Local search, unit specific parameters
  SEARCH4 <- list(
    TOP_N = 3, NP = 4000, NMIF = 50, NREPS = 24, NREPS_EVAL = 36, NP_EVAL = 5000
  )

  # Local search, all parameters
  SEARCH5 <- list(
    TOP_N = 3, NP = 4000, NMIF = 25, NREPS = 24, NREPS_EVAL = 36, NP_EVAL = 5000
  )

  # Total number of searches
  N_SEARCHES <- 5L
} else {
  stop("Run level must be 1, 2 or 3.")
}

haiti3_fit <- bake(
  file = paste0("model3/", rl_dir, "haiti3_fit.rds"), {
    fit_haiti3(
      search1 = SEARCH1,
      search2 = SEARCH2,
      search3 = SEARCH3,
      search4 = SEARCH4,
      search5 = SEARCH5,
      n_searches = N_SEARCHES,
      ncores = cores
    )
  }
)

rm(SEARCH1, SEARCH2, SEARCH3, SEARCH4, SEARCH5, N_SEARCHES)
gc()
@


<<Load Model Parameters, echo=FALSE, results='hide'>>=
# load('model3/output/PanelLocalAll_PF.rda')  # Final model 3 evaluations
# load('model3/output/PanelLocalAll.rda')  # Final model 3 parameters (MIF search)


# haiti3_fit contains ll estimate for many parameter sets, so we want to find
# the highest value
mod3_ll <- max(
  haiti3_fit[[length(haiti3_fit)]]$logLiks[, 'logLik'],
  na.rm = TRUE
)

# MIF_mod3 <- local_MIF2_search
# rm(mif_logLik, local_MIF2_search, tLocal)


# Fit model 2. Note that this takes the same amount of time at all run
# levels.
h2_fit <- bake(
  file = paste0("model2/", rl_dir, "model2_fit.rds"), {
    fit_haiti2()
  },
  timing = FALSE
)

@

<<table-input,echo=FALSE,eval=T, include=FALSE, message=FALSE>>=

# best_m3 <- ll_mod3 %>%
#   arrange(-logLik) %>%
#   slice_head(n = 1) %>%
#   pull(which)

best_m3 <- haiti3_fit[[length(haiti3_fit)]]$logLiks %>%
  filter(logLik == max(logLik, na.rm = TRUE)) %>%
  pull(which)

p3 <- haiti3_fit[[length(haiti3_fit)]]$params[best_m3, ]
gc()


stew(file = "models.rda", {
  h1 <- haiti1_joint()
  coef(h1) <- p1

  h2_epi <- haiti2(region = 'before', cutoff = 10000, measure = 'log')

  coef(h2_epi) <- h2_fit$h2_params

  h3 <- haiti3_panel(start_time = "2010-10-23", B0 = TRUE)
  coef(h3) <- p3
  pp3 <- pparams(h3)
  p3u <- pp3$specific
  p3s <- pp3$shared
})
@


\subsection{Model Fitting}\label{sec:model_fitting}

Proposed mechanistic structures form a family of statistical models indexed by a parameter vector $\paramVec$.
Different values of $\paramVec$ can result in qualitative differences in the predicted behavior of the system.
The complex nature of biological systems necessitates a search for modeling assumptions that combine insightful simplicity with fidelity to biological reality.
For example, many models commonly used in epidemiology are motivated by reasoning about a homogeneous mixing population \citep{bansal07} which is simultaneously an avenue for powerful simplification and a source of model misspecification.
Other common considerations include whether the proposed model should be stochastic or deterministic; whether the model should have change points in parameter values or should otherwise make adjustments for changes through time in the dynamic system; and whether the proposed model should include any spatial heterogeneity at a scale permissible by the observed data.
In addition, elements of $\paramVec$ can either be chosen as constants, based on scientific reasoning and previous knowledge, or calibrated to observed data.
Suitable methodology for calibrating model parameters may depend on other modeling decisions.
% While this section is focused on the elements of $\paramVec$ that are calibrated to data, we note that the decision of which elements to fix and which to estimate has consequences for model interpretability, as discussed in Sec.~\ref{sec:science}.

All three model considered in this study describes cholera dynamics via unobservable states that evolve dynamically with time.
Despite their similarities, these models represent a diverse selection of possible modeling assumptions: namely, the use of stochastic (Models~1 and 2) or deterministic (Model~2) equations; spatially-heterogeneous meta-population (Models~2 and 3) or spatially-aggregated (Model~1) structure; and the use of covariates (Model~3) versus mathematical equations (Models~1 and 2) to describe a seasonal mechanism.
All these structures can be described in the framework of partially observed Markov process (POMP) models, with the understanding that the deterministic Model~2 is a degenerate case of a stochastic model.
In the following subsections we describe our approach to fitting these mechanistic models.

\subsubsection{Model~1}

One approach to modeling a dynamic system is through probabilistic models.
With a probabilistic model, we suppose the existence of a joint density $f_{\bm{X}^{(\modelCounter)}_{\seq{0}{N}}, \bm{Y}^{(\modelCounter)}_{\seq{1}{N}}}$, with $\bm{X}^{(\modelCounter)}_{\seq{0}{N}}$ denoting the unobservable Markov process of model $\modelCounter$ at times $\seq{0}{N} = \{0, 1, \ldots, N\}$, and $\bm{Y}^{(\modelCounter)}_{\seq{1}{N}}$ denoting the observable process of the system at times $\seq{1}{N}$.
Under this framework, the observed data $y_{\seq{1}{n}}^*$ are assumed to be a single realization of the model $y_{\seq{1}{n}}^* \sim f_{\bm{X}^{(\modelCounter)}_{\seq{0}{N}}, \bm{Y}^{(\modelCounter)}_{\seq{1}{N}}}(x_{\seq{0}{N}}, y_{\seq{1}{N}}; \paramVec)$, where $\paramVec$ is a parameter vector that indexes the model.
Using a probabilistic model results in several advantages, including the ability to account for variability present in the system.
% which is of great interest to scientists and policy makers.
Furthermore, because each draw from the joint distribution represents a potential outcome of the dynamic system, best/worst case scenarios under the assumptions of the model can be easily obtained via simulation.
The added benefits of a stochastic model are coupled with statistical difficulty of fitting the parameter vector $\paramVec$ to observed data.

There exist several algorithms, both frequentist and Bayesian, that can be used to obtain estimates of the parameters in stochastic dynamic models.
% including the EM algorithm, Kalman Filter (and extensions) \citep{evensen09}, and iterated filtering algorithms \citep{ionides15}.
In order to retain the ability to propose models that are scientifically meaningful rather than only those that are simply statistically convenient, we restrict ourselves to parameter estimation techniques that have the plug-and-play property, which is that the fitting procedure only requires the ability to simulate the latent process instead of evaluating transition densities \citep{breto09,he10}.
Plug-and-play algorithms include Bayesian approaches like ABC and PMCMC \citep{toni09,andrieu10}, but here we use frequentist methods to maximize model likelihoods.
To our knowledge, the only plug-and-play frequentist methods that can maximize the complete model likelihood are iterated filtering algorithms, which modify the well-known particle filter \citep{arulampalam02} by performing a random walk for each parameter and particle.
These perturbations are carried out iteratively over multiple filtering operations, using the collection of parameters from the previous filtering pass as the parameter initialization for the next iteration, and decreasing the random walk variance at each step.

The ability to maximize the likelihood allows for likelihood-based inference, like performing statistical tests for potential improvements to the model.
% TODO: We should address somewhere why we included the process noise in Model 1. Anna suggested this below:
% We first demonstrate this capability by proposing the inclusion of additional stochasticity in the latent process through the force of infection, similar to that in Model~3. Eq.~\myeqref{model1:lambda} becomes
% \begin{equation}
% \label{model1:lambda_wn}
% \lambda(t) = \Big(\sum_{k=0}^{K}  I_{k}(t) + \asymptomRelativeInfect \sum_{k=0}^{K} A_{k}(t) \Big)^\nu d\Gamma_u(t)/dt \transmission(t)/N
% \end{equation}
% where $d\Gamma_u(t)/dt$ is multiplicative Gamma-distributed white noise.
%
% Part of the appeal of mechanistic models lies in their ability to illustrate a dynamical system.
% Inherent is dynamical systems is randomness, and, in the context of epidemiology, this randomness is a crucial component to the system itself.
% Useful and statistically sound incidence forecasting and prediction of vaccine efficacy are not only dependent upon a reasonable reflection of the natural system in the model; it is necessary for the model to also reflect the quantitative characteristics of the processes in question.
% This need, itself, is dependent upon inclusion of sufficient stochasticity in the model \citep{breto09}.
% For now, we motivate this decision by its presence in Model~3 and leave the more rigorous assessment of this adjustment to the supplement.
% The above can no doubt be improved.
% \arc{I think that evaluating the inclusion of both the noise and the trend parameter in the main text may distract from the main point of this manuscript, which is why I mentioned putting it in the supplement.}
% \eic{The dynamic noise issue is important, but is well described elsewhere so can perhaps be handled mostly by references. Maybe we don't need to do more than mention that Lee et al constrained the noise parameter to be zero, but we found considerable advantage from including the parameter (XXX log units of likelihood) consistent with previous investigations (refs).}
% Transition phrase here?
We demonstrate this capability by proposing a linear trend $\transmissionTrend$ in transmission in Eq.~\myeqref{model1:beta}:
\begin{equation}
\label{model1:betat}
\log\transmission(t) = \sum_{j=1}^6 \transmission_s s_j(t) + \transmissionTrend\bar{t}
\end{equation}
Where $\bar{t}$ is the linear mapping $\bar{t}: [0, N] \rightarrow [-1, 1]$ of the time $t$.
The proposal of a linear trend in transmission is a result of observing an apparent decrease in reported cholera infections from 2012-2019 in Fig.~\ref{CholeraData}.
% The inclusion of a possible trend in transmission is due to observing an apparent continual decrease in reported cholera infections from 2012-2019 can be seen in Fig.~\ref{CholeraData}.
While several factors may contribute to this decrease, one explanation is that case-area targeted interventions (CATIs), which included education sessions, increased monitoring, household decontamination, soap distribution, and water chlorination in infected areas \citep{rebaudet19CATI}, may have greatly reduced cholera transmission \citep{rebaudet21}.

We perform a statistical test to determine whether or not the data indicate the presence of a linear trend in transmissibility.
To do this, we perform a profile-likelihood search on the parameter $\transmissionTrend$ and obtain a confidence interval via a Monte Carlo Adjusted Profile (MCAP) \citep{ionides17}.
Model~1 was implemented in \cite{lee20} by fitting two distinct phases: an epidemic phase from October 2010 through March 2015, and an endemic phase from March 2015 onward.
We similarly allow the re-estimation of process and measurement overdispersion parameters ($\sigmaProc^2$ and $\obsOverdispersion$), and require that the latent Markov process $X(t)$ carry over from one phase into the next.
The resulting confidence interval for $\transmissionTrend$ is $(\Sexpr{myround(mcap_results$ci[1], digits = 3)}, \Sexpr{myround(mcap_results$ci[2], digits = 3)})$, with the full results displayed in Fig.~\ref{fig:betat}.
These results are suggestive that the inclusion of a trend in transmission rate improves the quantitative ability of Model~1 to describe the observed data.
The reported results for Model~1 in the remainder of this article were obtained with the inclusion of the parameter $\transmissionTrend$.

\begin{figure}[ht]
\centering
<<Beta_trend_Figure, fig.height=2.5, fig.width=3.7, fig.align='center'>>=
ggplot() +
  geom_point(data = h1_prof_res, aes(x = betat, y = logLik)) +
  geom_line(data = mcap_results$fit, aes(x = parameter, y = smoothed), col = 'blue') +
  # geom_line(data = mcap_results$fit, aes(x = parameter, y = quadratic), col = 'red') +
  geom_vline(xintercept = mcap_results$ci[1], linetype = 'dashed') +
  geom_vline(xintercept = mcap_results$ci[2], linetype = 'dashed') +
  # geom_vline(xintercept = mcap_results$quadratic_max, col = 'red') +
  geom_vline(xintercept = mcap_results$mle, col = 'blue') +
  labs(x = "Linear Trend in Transmission", y = 'Log Likelihood') +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 10))
@
\caption{\label{fig:betat}
Monte Carlo adjusted profile of $\transmissionTrend$.
The blue curve is the profile, the blue line indicates the MLE, and the dashed lines indicate the confidence interval.
}
\end{figure}

Model~1 is implemented using the \code{pomp} package \cite{pomp2}, which contains an implementation of the IF2 algorithm that was used to compute the likelihood profile.
% Performing a profile confidence search of a parameter also results in a MLE calculation, as the value that maximizes the profile likelihood also maximizes the entire likelihood surface \citep{murphy2000}.
Simulations from the fitted model compared to the observed data are given in Fig~\ref{fig:mod1fit}.

\begin{figure}[ht]
\centering
<<Model_1_Sims_Figure, fig.height=2.4, fig.width=4.7, fig.align='center'>>=
sims <- simulate(h1, nsim = 500, format = 'data.frame', seed = 3448931)

quants <- sims %>%
  mutate(is_data = .id == 'data') %>%
  filter(!is_data) %>%
  select(.id, week, cases) %>%
  group_by(week) %>%
  summarize(
    q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
    q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
    q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

ggplot() +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  geom_line(data = quants, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = quants, aes(x = date, ymin = q025 + 1, ymax = q975 + 1), alpha = 0.5) +
  theme(axis.title.x = element_blank(),
        axis.text = element_text(size = 8),
        axis.title.y = element_text(size = 10))+
  ylab('Reported cholera cases') +
  geom_vline(xintercept = lubridate::weeks(232) + lubridate::ymd("2010-10-16"), linetype = 'dashed') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '1 year'))
@
\caption{\label{fig:mod1fit}
Simulations from Model~1 compared to reported cholera cases.
The black curve is observed data, the blue curve is median of 500 simulations from the fitted model, and the vertical dashed line represents break-point when parameters are refit.
}
\end{figure}

<<Clean-up Model 1 Simulations, echo=FALSE, include=FALSE>>=
rm(sims, quants)
gc()
@

\subsubsection{Model~2}

Model~2 is a deterministic model, which can be viewed as a special case of a POMP (or SpatPOMP) with no randomness in the dynamic process or measurement.
By combining a deterministic process model with a simple Gaussian measurement model, Model~2 reduces model fitting to a least squares calculation over parameters in a set of differential equations.
Deterministic compartment models have a long history in the field of infectious disease epidemiology \citep{kermack1927,brauer2017,varghese21},
and can be justified by asymptotic considerations in a large-population limit \citep{dadlani2020,ndii17}.

\cite{lee20} fit two versions of model~2 based on a presupposed change in cholera transmission from a epidemic phase to endemic phase that occurred in March, 2014.
The inclusion of a change-point in model states and parameters increases the flexibility of the model and hence the ability to fit the observed data.
The increase in model flexibility, however, results in hidden states that are inconsistent between model phases.
A naive solution to this would be to estimate only model parameters in separate phases and not the hidden states, but this may also result in hidden states in one phase of the model that are inconsistent with the other phase of the outbreak, since only the data from one phase is used to fit model parameters.

The inclusion of a model break-point in \cite{lee20} is perhaps due to a challenging feature of fitting a deterministic model via least squares: discrepancies between model trajectories and observed case counts in highly infectious periods of a disease outbreak will result in greater penalty than the discrepancies between model trajectories and observed case counts in times of relatively low infectiousness, resulting in a bias towards accurately describing periods of high infectiousness.
This issue is particularly troublesome for this case study: the inability to accurately fit times of low infectiousness may result in poor model forecasts, as few cases of cholera were observed the last few years epidemic.

To combat this issue, we fit the model to log-transformed case counts, where the difference between periods of high and low infectiousness is less drastic.
Other solutions include changing the measurement model to include overdispersion, as was done in Models~1 and~3.
Here we chose to fit the model to transformed case counts rather than adding overdispersion to the measurement model with the goal of making the fewest changes to the model in \cite{lee20} as possible.
In practice, however, the inclusion of overdispersion in the measurement process may be preferred \jwc{REFS???}.

We implemented this model using the \code{spatPomp} \code{R} package \citep{asfaw21github}.
The model was then fit using the subplex algorithm, implemented in the  \code{subplex} package \citep{king2020Subplex}.
A comparison of the trajectory of the fitted model to the data is given in Fig~\ref{fig:mod2Traj}.

<<Model 2 Trajectory, echo=FALSE>>=
h2_epi_traj <- trajectory(h2_epi, params = h2_fit$h2_params, format = 'data.frame')
h2_epi_traj$Ctotal <- rowSums(h2_epi_traj[, paste0("C", 1:10)]) * h2_fit$h2_params['Rho']


h2_traj <- h2_epi_traj %>%
  select(year, Ctotal) %>%
  mutate(
    date = as.Date(lubridate::round_date(lubridate::date_decimal(year), unit = 'day'))
  )
@


\subsubsection{Model~3}
Model~3 is also of a probabilistic model.
In this model, both the latent and observable processes can be factored into department specific processes which interact with each other.
% We denote the latent and measurement processes for Model~3 as $\bm{X}^{(3)}(t_{0:N}) = X^{(3)}_{1:U, 0:N}$, and $\bm{Y}^{(3)}(t_{1:N}) = Y_{1:U, 1:N}$ \eic{IS THIS NEEDED HERE?}.
The decision to model the system via metapopulation models versus a model aggregated to a larger spatial scale is one of great scientific interest.
Evidence for the former approach has been provided in previous studies \citep{king15}, including the specific case of heterogeneity between Haitian departments in cholera transmission \citep{collins14}.
Note that this evidence alone does not automatically discredit conclusions drawn via nationally aggregated models, as there are also reasons to prefer a simple model over a complex one \citep{saltelli20,green15}.

Fitting scientifically flexible metapopulation models is a challenging statistical problem.
In particular, parameter estimation techniques based on the particle filter become computationally intractable as the number of spatial units increase.
This is a result of the approximation error of particle filters growing exponentially in the dimension of the model \citep{rebeschini15,park20}.

Parameters that must be fit in Model~3 are primarily shared between each department, the exception to this being the parameters $\beta_{W_u}$, and $\beta_u$, which are unique for each department $u \in \seq{1}{10}$.
To avoid the parameter estimation issue in high-dimensional models, \citet{lee20} simplified the problem by fitting independent department-level models to the data.
The shared parameters were calibrated using the cholera incidence data from Artibonite, and the department-specific parameters ($\beta_{W_u}$ and $\beta_u$) were fit using the data from their respective department.
Reducing a spatially coupled model to individual units in this fashion requires special treatment of any interactive mechanisms between spatial units, such as found in Eq.~\myeqref{eq:model3:foi}.
In particular, when considering a model for department $u$, the values $I_\nu(t)$ and $A_\nu(t)$, are unknown for $u \neq v \in \seq{1}{10}$.
A first order approximation of Eq.~\myeqref{eq:model3:foi} for each department $v \in \seq{1}{10}$ can be obtained using the weekly number of observed cholera cases in each department:
\begin{equation}
  I_v(t) + A_v(t) \approx \frac{365}{7\reportRate}y^*_v(t)\left(\frac{1}{\muDeath + \choleraDeath + \muIR} + \frac{1 - \symptomFrac}{\symptomFrac(\muDeath + \muIR)}\right)\label{eq:model3:approx}
\end{equation}
This approximation leads to department-specific models that are conditionally independent given the reported number of cholera infections in the remaining departments.
Here, we refer to a collection of POMP models that are independent across units as a PanelPOMP.

In the case of a PanelPOMP, an extension of the IF2 algorithm, known as Panel Iterated Filtering (PIF) \citep{breto20}, can be used to obtain the MLE, which solves the curse of dimensionality for this class of models.
A major advantage of this algorithm is that PIF can be used to fit both unit-specific parameters and shared parameters; in this way, the calibration of shared parameters involves all of the available data instead of just an arbitrarily chosen subset.

We fit the PanelPOMP version of Model~3 using a slight modification of the PIF algorithm, which we call the Block Panel Iterated Filter (BPIF).
Let $l_{\text{P}}(y_{1:N}^*; \paramVec)$ denote the log-likelihood function of the PanelPOMP version of Model~3.
The output of the BPIF algorithm is the MLE $\tilde{\paramVec} = \argmax_{\paramVec} l_{\text{P}}(y_{1:N}^*; \paramVec)$.
The BPIF algorithm is implemented in the \code{panelPomp} package \citep{breto20panelPomp} as the argument \code{block = TRUE} in the \code{mif2} function.
Pseudo-code for this algorithm is provided in Algorithm~S1 in the supplementary material.

% While the reported parameters were obtained using the decoupled representation of Model~3, simulations for the various vaccination campaigns were obtained using fully coupled version of the model, implemented using the \code{spatPomp} package.

<<Model 3 PanelSims,echo=FALSE>>=
h3_panel_sims <- switch(RUN_LEVEL, 20, 100, 500)

departements <-
  c(
    'Artibonite',
    'Centre',
    'Grande_Anse',
    'Nippes',
    'Nord',
    'Nord-Est',
    'Nord-Ouest',
    'Ouest',
    'Sud',
    'Sud-Est'
  )

registerDoRNG(5317865)

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/panelPompSims.rds")) {
  h3_panel_sims <- readRDS("model3/run_level_3/panelPompSims.rds")
} else {
  h3_panel_sims <- bake(
    file = paste0('model3/', rl_dir, 'panelPompSims.rds'), {
      foreach(dep = departements, .combine = rbind) %dopar% {
        SIRB <- unitobjects(h3)[[dep]]
        shared <- h3@shared
        specific <- h3@specific[, dep]

        pomp::simulate(
          SIRB,
          params = c(shared, specific),
          nsim = h3_panel_sims,
          format = 'data.frame'
        ) -> sims

        sims$dep <- dep
        sims
      }
    }
  )
}

h3_panel_quantiles <- h3_panel_sims %>%
  mutate(
    date = as.Date(
      lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
    )
  ) %>%
  group_by(dep, date) %>%
  summarise(
    q05 = quantile(cases, 0.025, na.rm = T),
    mean = mean(cases, na.rm = T),
    q50 = quantile(cases, 0.5, na.rm = T),
    q95 = quantile(cases, 0.975, na.rm = T)
  ) %>%
  ungroup() %>%
  mutate(
    dep = gsub("-", "_", dep)
  ) %>%
  filter(date >= as.Date(
    lubridate::round_date(
      lubridate::date_decimal(h3@unit.objects$Artibonite@t0), unit = 'day'
    )
  )
  )

plot_order <- c(
  'Artibonite',
  'Sud_Est',
  'Nippes',
  'Nord_Est',
  'Ouest',
  'Centre',
  'Nord',
  'Sud',
  'Nord_Ouest',
  'Grande_Anse'
)

@


% \begin{figure}[ht]
% ggplot() +
%   geom_line(data = h3_panel_quantiles, aes(x = date, y = q50 + 1), col = 'blue') +
%   geom_line(data = dep_plot_df, aes(x = date, y = cases + 1), col = 'black') +
%   geom_ribbon(data = h3_panel_quantiles, aes(x = date, ymin = q05 + 1, ymax = q95 + 1),
%               alpha = 0.4) +
%   facet_wrap(~factor(dep, levels = plot_order), nrow = 2,
%              labeller = dep_labeller) +
%   ylab('Reported cholera cases') +
%   theme(axis.title.x = element_blank()) +
%   scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years')) +
%   scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))
%
% \caption{\label{h3panelsims}
% Simulations from initial conditions using panelPOMP version of Model~3.
% The black curve represents true case count, the blue curve the median of 500 simulations from the model, and the grey ribbons representing $95\%$ confidence interval.
% }
% \end{figure}


<<Remove Model3 panelSims, cache=FALSE, include=FALSE, message=FALSE, echo=FALSE>>=
rm(h3_panel_quantiles, h3_panel_sims)
gc()
@

Fitting Model~3 in this fashion simplifies the problem of parameter estimation, but it also introduces additional technicalities that must be addressed.
One concern is that of obtaining model forecasts, which was the primary goal of \citet{lee20}.
% "primary output" -> "primary goal", based on Anna's suggestion.
The simplified PanelPOMP version of Model~3 relies on the observed cholera cases as a covariate, which are unavailable for use in forecasts.
To address this, we use the MLE obtained for the PanelPOMP approximation of Model~3 as an estimate for the parameters in fully coupled version of the model, which was implemented using the \code{spatPomp} package, similar to what was done in \cite{lee20}.
Simulations from the fully coupled version of the model using the estimated parameter vector $\tilde{\paramVec}$, are displayed in Fig.~\ref{h3spatsims}.

<<Model 3 SpatPOMP sims, echo=FALSE>>=
# load('output/FinalPanelAsSpatPomp_PF.rda')

h3_spat_nsim <- switch(RUN_LEVEL, 20, 100, 500)

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/SpatPompSims.rds")) {
  h3Spat_quants <- readRDS("model3/run_level_3/SpatPompSims.rds")
} else {
  h3Spat_quants <- bake(
    file = paste0('model3/', rl_dir, 'SpatPompSims.rds'), {
      h3Spat <- haiti3_spatPomp()
      coef(h3Spat) <- haitipkg:::panelParms_toSpatParms3(p3)

      h3Spat_sims <- simulate(
        h3Spat, nsim = h3_spat_nsim, format = 'data.frame'
      )

      h3Spat_sims %>%
        rename(dep = unitname) %>%
        group_by(dep, time) %>%
        summarise(
          q05 = quantile(cases, 0.025, na.rm = T),
          mean = mean(cases, na.rm = T),
          q50 = quantile(cases, 0.5, na.rm = T),
          q95 = quantile(cases, 0.975, na.rm = T)
        ) %>%
        ungroup() %>%
        mutate(
          date = lubridate::date_decimal(time)
        ) %>%
        filter(date >= as.Date(lubridate::round_date(lubridate::date_decimal(h3Spat@t0), unit = 'day'))) %>%
        mutate(date = as.Date(lubridate::round_date(date, unit = 'day')))
    },
    timing = FALSE
  )
}

@

\begin{figure}[ht]
<<Model3_SpatPOMP_sims_Figure, fig.height=3>>=
ggplot() +
  geom_line(data = h3Spat_quants, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_line(data = dep_plot_df, aes(x = date, y = cases + 1), col = 'black') +
  geom_ribbon(data = h3Spat_quants,
              aes(x = date, ymin = q05 + 1, ymax = q95 + 1),
              alpha = 0.4) +
  facet_wrap(~factor(dep, levels = plot_order), nrow = 2,
             labeller = dep_labeller) +
  labs(y = 'Log Reported Cholera Cases') +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2011-01-01"), as.Date("2019-01-01"), by = '2 years')) +
  theme(axis.title.x = element_blank()) +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))
@
\caption{\label{h3spatsims}
Simulations from initial conditions using the spatially coupled version of Model~3.
The black curve represents true case count, the blue line the median of 500 simulations from the model, and the grey ribbons representing $95\%$ confidence interval.
}
\end{figure}

<<Remove Model3 spatSims, cache=FALSE, include=FALSE, message=FALSE, echo=FALSE>>=
rm(h3Spat_quants, h3_spat_nsim)
gc()
@

% While simulations from the SpatPOMP version of Model~3 resemble the observed data, we note in Table~\ref{tab:likes} that the SpatPOMP version of Model~3 has a lower log-likelihood than the PanelPOMP version using the same model parameters.
Let $l_{\text{C}}(y_{1:N}^*; \paramVec)$ denote the log-likelihood of the fully coupled model, and let $\hat{\paramVec} = \argmax_{\paramVec} l_{\text{C}}(y_{1:N}^*; \paramVec)$.
If the goal of parameter estimation is to maximize the likelihood of the fully coupled model, the approach of estimating parameters using a decoupled model is clearly sub optimal, as $l_{\text{C}}(y_{1:N}^*; \hat{\paramVec}) \geq l_{\text{C}}(y_{1:N}^*; \tilde{\paramVec})$.
A natural question that arises is if the difference between $\hat{\paramVec}$ and $\tilde{\paramVec}$ is scientifically and statistically meaningful.
The answer to this question requires the ability to reliably estimate $\hat{\paramVec}$, which is model specific and beyond the scope of this article.

In this case study, a brief analysis of the fitted parameters $\tilde{\paramVec}$ provides some interesting insight.
Consider the unexpectedly large exponent on rainfall: $r = \Sexpr{signif(p3s["r"],3)}$ (Eq.~\ref{eq:model3:water}).
Because the value of rainfall $J(t) \in (0, 1)$, the large coefficient results in $J(t)^r \approx 0$.
This result may initially be surprising given the importance of rainfall as a seasonal diver of cholera infections in Haiti (see Sec.~\ref{sec:science}).
One plausible explanation of this result is that the fitting procedure---used to obtain parameter values in the independent department model given the reported cases in other departments---has learned an over-dependence on the reported cholera cases in other departments, and thus the seasonality of cholera incidence in department $u$ depends on the highly correlated covariate of the reported cholera cases in departments $1:U\setminus \{u\}$.
Whatever the case may be, this fitting procedure results in model likelihoods unable to improve upon a log-linear benchmark (see Sec.~\ref{sec:model_diagnostics} and Tab.~\ref{tab:likes}), and highlights the need for advancements in statistical methodology that permit inference on models with coupled metapopulation dynamics.

\subsection{Model Diagnostics} \label{sec:model_diagnostics}

Parameter calibration (whether Bayesian or frequentist) aims to find the best description of the observed data under the assumptions of the model.
Obtaining the best fitting set of parameters for a given model does not, however, guarantee that the model provides an accurate representation of the system in question.
Model misspecification, which may be thought of as the omission of a mechanism in the model that is an important feature of the dynamic system, is inevitable at all levels of model complexity.
To make progress, while accepting proper limitations, one must bear in mind the much-quoted observation of \citet{box79} that ``all models are wrong but some are useful.''
This is not just practical advice for applied statistics, but matches broader appreciation of the limitations of all scientific knowledge \eic{REF???}.
In this section, we provide some tools and suggestions for diagnosing mechanistic models with the goal of making the subjective assessment of model ``usefulness" more objective.
To do this, we will rely on the quantitative and statistical ability of the model to match the observed data, which we call the model's {\it goodness-of-fit}, with the guiding principle that a model which cannot adequately describe observed data may not be reliable for useful purposes.
Goodness-of-fit may provide evidence supporting the causal interpretation of one model versus another, but cannot by itself rule out the possibility of alternative explanations.

One common approach to assess a mechanistic model's goodness-of-fit is to compare simulations from the fitted model to the observed data.
Visual inspection may indicate defects in the model, or may  suggest that the observed data are a plausible realization of the fitted model.
While visual comparisons can be informative, they provide only a weak and informal measure of the goodness-of-fit of a model.
The study by \citet{lee20} provides an example of this: their models and parameter estimates resulted in simulations that visually resembled the observed data, yet resulted in model likelihoods that were---in some cases---remarkably smaller than likelihoods that can be achieved via the likelihood based optimization techniques that were used (see Table~\ref{tab:likes}).
Alternative forms of model validation should therefore be used in conjunction with visual comparisons of simulations to observed data.

Another approach is to compare a quantitative measure of the model fit (such as MSE, predictive accuracy, or model likelihood) among all proposed models.
These comparisons provide insight into how each model performs relative to the others.
To calibrate relative measures of fit, it is useful to compare against a model that has well-understood statistical ability to fit data, and we call this model a {\it benchmarks}.
Standard statistical models, interpreted as associative models without requiring any mechanistic interpretation of their parameters, provide suitable benchmarks.
Examples include linear regression, auto-regressive moving average time series models, or even independent and identically distributed measurements.
The benchmarks enable us to evaluate the goodness of fit that can be expected of a suitable mechanistic model.

Goodness-of-fit alone does not guarantee that a model provides a correct causal interpretation of the model.
Indeed, associative models are not constrained to have a causal interpretation, and typically are designed with the sole goal of providing a statistical fit to data.
Consequently, we should not require a candidate mechanistic model to beat all benchmarks.
However, a mechanistic model which falls far short against benchmarks is evidently failing to explain some substantial aspect of the data.
A convenient measure of fit should have interpretable differences that help to operationalize the meaning of far short.
Ideally, the measure should also have favorable theoretical properties.
Consequently, we focus on log-likelihood as a measure of goodness of fit, and we adjust for the degrees of freedom of the models to be compared by using the Akaike information criterion (AIC) \citep{aic74}.

In some cases, a possible benchmark model could be a generally accepted mechanistic model, but often no such model is available.
Because of this, we use a log-linear Gaussian ARMA model as an associative benchmark, as recommended by \citet{he10}.
The theory and practice of ARMA models is well developed, but the exponential growth and decay characteristic of biological dynamics suggests applying these linear models on a log scale.
Likelihoods of Models~1--3 and their respective ARMA benchmark models are provided in Table~\ref{tab:likes}.

It should be universal practice to present measures of goodness of fit for published models, and mechanistic models should be compared against benchmarks.
This alone would assist authors and readers to confront any major statistical limitations of the proposed mechanistic models.
In addition, the published goodness of fit provides a concrete measure for subsequent research to identify and remedy limitations in the analysis, or to update the investigation based on new data or new scientific understanding.
When combined with online availability of data and code, objective measures of fit provide a powerful tool to accelerate scientific progress, following the paradigm of the {\it common task framework} \citep[][Sec.~6]{donoho17}.
In our literature review of the Haiti cholera epidemic \eic{CHECK THIS. HOW/WHERE SHOULD WE DESCRIBE THIS?} no quantitative measures of goodness of fit, and no benchmark models, were considered in any of the ?? papers which calibrated a mechanistic model to data in order to obtain scientific conclusions.

The use of benchmarks may also be beneficial when developing models with varying spatial scale, where a direct comparison between models likelihoods is meaningless.
In such a case, a benchmark model could be fit to each spatial resolution being considered, and each model compared to their respective benchmark.
Large advantages (shortcomings) in model likelihood relative to the benchmark for a given spatial scale that are not present in other spatial scales may provide weak evidence for (against) the spatial resolution under consideration.
While these comparisons don't provide a formal statistical test of the amount of spatial resolution needed for a given model, they can be quite informative.

<<Table 2 Input, echo=FALSE, eval=TRUE>>=
NREPS_EVAL <- switch(RUN_LEVEL,   3,   8,    36)
NP_EVAL    <- switch(RUN_LEVEL, 100, 500, 10000)

# Objective function of model 2, epidemic phase
epi_ofun <- traj_objfun(
  h2_epi,
  params = h2_fit$h2_params
)

mod2_epi_ll <- -epi_ofun(par = h2_fit$h2_params)
# mod2_end_ll <- -end_ofun(par = h2_fit$end_params)


mod2_ll <- mod2_epi_ll - sum(log(h2_epi@data + 1), na.rm = TRUE)

# Evaluate and sum objective functions to get total model likelihood
# mod2_ll <- -(epi_ofun(par = h2_fit$epi_params) + end_ofun(par = h2_fit$end_params))

arima_lik2 <- 0

# model 2 AIC
mod2_n_params <- 16  # TODO: adjust Number of parameters fit in Model 2
mod2_aic <- 2 * mod2_n_params - 2 * mod2_ll

# model 1 AIC
mod1_n_params <- 13
mod1_aic <- 2 * mod1_n_params - 2 * mod1_ll

# Load pre-calculated model 3 likelihood
# mod3_ll <- ll_mod3 %>%
#   arrange(-logLik) %>%
#   slice_head(n = 1) %>%
#   pull(logLik)

# model 3 AIC
mod3_n_params <- 39
mod3_aic <- 2 * mod3_n_params - 2 * mod3_ll

h3Spat <- haiti3_spatPomp()
coef(h3Spat) <- haitipkg:::panelParms_toSpatParms3(p3)

# Calculate likelihood via block-particle filter
if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/SpatPomp_res.rds")) {
  mod3_spatPomp_res <- readRDS("model3/run_level_3/SpatPomp_res.rds")
} else{
  mod3_spatPomp_res <- bake(
    file = paste0('model3/', rl_dir, 'SpatPomp_res.rds'), {

      # Create a list to store results
      results <- list()

      # Perform block-particle filter
      h3_bpf <- foreach(i = 1:NREPS_EVAL, .combine = c) %dopar% {
        bpfilter(h3Spat, Np = NP_EVAL, block_size = 1,
                 params = haitipkg:::panelParms_toSpatParms3(p3))
      }


      ll <- logLik(h3_bpf)
      results$ll <- logmeanexp(ll[!is.na(ll)], se = FALSE)

      ### Get likelihood for subset of the data

      # Find appropriate sub-set
      in_subset_cols <- h3_bpf[[1]]@times >= as.numeric(lubridate::decimal_date(as.Date("2014-03-01")))
      in_subset_rows <- h3_bpf[[1]]@unit_names != 'Ouest'

      mod3_coupled_subset_evals <- c()
      for (i in 1:length(h3_bpf)) {
        mod3_coupled_subset_evals <- c(
          mod3_coupled_subset_evals,
          sum(h3_bpf[[i]]@block.cond.loglik[in_subset_rows, in_subset_cols])
        )
      }

      results$subset_ll <- logmeanexp(mod3_coupled_subset_evals)
      results
    },
    timing = FALSE
  )
}

mod3_spatPomp_n_params <- 39
mod3_spatPomp_aic <- 2 * mod3_spatPomp_n_params - 2 * mod3_spatPomp_res$ll
mod3_spatPomp_res$subset_aic <- 2 * mod3_spatPomp_n_params - 2 * mod3_spatPomp_res$subset_ll

rm(NREPS_EVAL, NP_EVAL)
@

<<CalculateARMA, echo=FALSE, message=FALSE, include=FALSE, cache=TRUE>>=

ARMA_benchmarks <- list()

###
### Model 1 ###
###

# Load aggregated data
m1_agg_data <- haiti1_agg_data()

# Fit ARMA(2, 1) on log-cases
m1_log_arma <- arima(log(m1_agg_data$cases + 1), order = c(2, 0, 1))

# Fit ARMA(2, 1) on natural scale of cases
m1_arma <- arima(m1_agg_data$cases, order = c(2, 0, 1))

# Check which one is better, and save benchmark as the better one
if (m1_log_arma$loglik - sum(log(m1_agg_data$cases + 1), na.rm = TRUE) >= m1_arma$loglik) {
  ARMA_benchmarks[['m1']] <- m1_log_arma$loglik - sum(log(m1_agg_data$cases + 1), na.rm = TRUE)
} else {
  ARMA_benchmarks[['m1']] <- m1_arma$loglik
}

rm(m1_agg_data, m1_log_arma, m1_arma)
gc()

###
### Model 2 ###
###

h2_data <- haiti2_data()

# model 2 ARIMA benchmark
h2_dep_names <- h2_data$department %>% unique()
m2_benchmark <- 0
for (i in 1:10){
  cases <- h2_data[h2_data$department == h2_dep_names[i], ]$cases
  log_cases <- log(cases + 1)

  m2_arma_dep_ll <- arima(cases, order=c(2, 0, 1))$loglik
  m2_log_arma_dep_ll <- arima(log_cases, order=c(2, 0, 1))$loglik - sum(log_cases, na.rm = TRUE)

  m2_benchmark <- m2_benchmark + max(m2_arma_dep_ll, m2_log_arma_dep_ll)
}

ARMA_benchmarks[['m2']] <- m2_benchmark

rm(m2_benchmark, h2_dep_names, cases, log_cases, m2_arma_dep_ll, m2_log_arma_dep_ll, h2_data)
gc()

###
### Model 3 ###
###

MODEL3_CASES <- haitiCholera %>%
  dplyr::rename(
    date = date_saturday, Grande_Anse = Grand.Anse,
    Nord_Est = Nord.Est, Nord_Ouest = Nord.Ouest,
    Sud_Est = Sud.Est
  ) %>%
  dplyr::mutate(date = as.Date(date)) %>%
  dplyr::select(-report) %>%
  dplyr::mutate(time_diff = date - lag(date))

jumps <- which(MODEL3_CASES$time_diff > lubridate::make_difftime(day = 7))

temp <- MODEL3_CASES
for (j in 1:length(jumps)) {
  temp <- rbind(
    temp[1:(jumps[j] + j - 2), ],
    rep(NA, ncol(temp)),
    temp[(jumps[j] + j - 1):nrow(temp), ]
  )
}

mod3_cases <- temp %>%
  dplyr::select(-time_diff)

mod3_departements <-
  c(
    'Artibonite',
    'Centre',
    'Grande_Anse',
    'Nippes',
    'Nord',
    'Nord-Est',
    'Nord-Ouest',
    'Ouest',
    'Sud',
    'Sud-Est'
  )

m3_benchmark <- 0
for (dep in mod3_departements) {

  # Get department cases
  cases <- as.numeric(mod3_cases[, gsub("-", "_", dep)])

  # Fit model on natural scale of cases
  m3_a201 <- arima(cases, order = c(2, 0, 1))

  # Fit model on log-scale of cases
  m3_log_a201 <- arima(log(cases + 1), order = c(2, 0, 1))

  m3_benchmark <- m3_benchmark + max(
    m3_log_a201$loglik - sum(log(cases + 1), na.rm = TRUE),
    m3_a201$loglik
  )
}

ARMA_benchmarks[['m3']] <- m3_benchmark
rm(cases, m3_a201, m3_log_a201, m3_benchmark, mod3_departements, dep,
   mod3_cases, temp, jumps, MODEL3_CASES, i, j)
gc()
@

<<Table 2 Lee Input, echo=FALSE, include=FALSE>>=
mod1_lee_n_params <- 20
mod1_lee_aic <- 2 * mod1_lee_n_params - 2 * mod1_lee_ll

# load('model2/output/mod2_leeFit.rda')
mod2_lee_aic <- 2 * h2_fit$leeFit_n_params - 2 * h2_fit$leeFit_ll
# mod2_lee_aic <- 2 * mod2_leeFit_num_parms - 2 * mod2_leeFit_ll
@

<<Lee3 Sub Calculations, echo=FALSE, include=FALSE, message=FALSE>>=
lee3_NP   <- switch(RUN_LEVEL, 50, 250, 5000)
lee3_NREP <- switch(RUN_LEVEL,  3,   8,   36)

registerDoRNG(49637421)

# Create Lee et al (2020a) version of model 3 on sub-data
lee3_sub <- haiti3_lee_et_al()

# This model is only fit for a subset of the data, so we will set data
# that are not part of this subset as NA. This will allow us to estimate
# the likelihood for only the values that were considered during the
# parameter estimation phase.
set_as_na <- lee3_sub@times < lubridate::decimal_date(as.Date('2017-06-10'))
temp_ouest_cases <- lee3_sub@data['casesOuest', ]
temp_ouest_cases[set_as_na] <- NA_real_
lee3_sub@data['casesOuest', ] <- temp_ouest_cases

lee3_all <- haiti3_lee_et_al(start_time = "2010-10-23")

rm(set_as_na, temp_ouest_cases)
gc()

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/lee3_results.rds")) {
  lee3_results <- readRDS("model3/run_level_3/lee3_results.rds")
} else {
  lee3_results <- bake(
    file = paste0('model3/', rl_dir, 'lee3_results.rds'), {

      # Create lists to save results
      lee3_results <- list()

      # Save how long it takes for sub-data
      lee3_results[["sub_pf_time"]] <- system.time({

        # Save all likelihood evaluations
        lee3_results[["sub_ll_evals"]] <- foreach(
          i = 1:lee3_NREP, .combine = c
        ) %dopar% {
          logLik(pfilter(lee3_sub, Np = lee3_NP))  # Get likelihood evaluations
        }
      })

      registerDoRNG(2789131)

      # Save how long it takes for all data
      lee3_results[["all_pf_time"]] <- system.time({

        # Save all likelihood evaluations
        lee3_results[["all_ll_evals"]] <- foreach(
          i = 1:lee3_NREP, .combine = c
        ) %dopar% {
          logLik(pfilter(lee3_all, Np = lee3_NP))  # Get likelihood evaluations
        }
      })

      lee3_results[["n_params"]] <- 29

      lee3_results
    },
    timing = FALSE
  )
}

rm(lee3_sub, lee3_all, lee3_NP, lee3_NREP)
gc()

mod3_lee_coupled_aic <- 2 * lee3_results$n_params - 2 * max(lee3_results$all_ll_evals)

mod3_lee_coupled_partial_aic <- 2 * lee3_results$n_params - 2 * max(lee3_results$sub_ll_evals)

@

\begin{table}[ht]
\centering

\begin{tabular}{|c|c|c|c|}
\hline
 & \thead{Model~1} & \thead{Model~2} & \thead{Model~3}
\\
\hline
\hline
\multirow{2}{*}{Log-likelihood} &
  $\Sexpr{myround(mod1_ll, 1)}$ &
  $\Sexpr{myround(mod2_ll, 1)}$ &
  $\Sexpr{myround(mod3_spatPomp_res$ll, 1)}$ \\
    & ($\Sexpr{myround(mod1_lee_ll, 1)}$)\footnotemark[1] &
  ($\Sexpr{myround(h2_fit$leeFit_ll, 1)}$) &
  ($\Sexpr{myround(max(lee3_results[["all_ll_evals"]]), 1)}$)\footnotemark[2]
\\
\hline
Number of &
  $\Sexpr{as.character(mod1_n_params)}$ &
  $\Sexpr{as.character(mod2_n_params)}$ &
  $\Sexpr{mod3_spatPomp_n_params}$ \\
 Fit Parameters &
 (\Sexpr{as.character(mod1_lee_n_params)}) &
 ($\Sexpr{as.character(h2_fit$leeFit_n_params)}$) &
 ($\Sexpr{as.character(lee3_results$n_params, 1)}$)
\\
\hline
\multirow{2}{*}{AIC} &
  $\Sexpr{myround(mod1_aic, 1)}$ &
  $\Sexpr{myround(mod2_aic, 1)}$ &
  $\Sexpr{myround(mod3_spatPomp_aic, 1)}$ \\
  & ($\Sexpr{myround(mod1_lee_aic, 1)}$)\footnotemark[1] &
  ($\Sexpr{myround(mod2_lee_aic, 1)}$) &
  ($\Sexpr{myround(mod3_lee_coupled_aic, 1)}$)\footnotemark[2]
\\
\hline
\thead{Log-ARMA(2,1) \\ Log-likelihood} &
  $\Sexpr{myround(ARMA_benchmarks$m1, 1)}$ &
  $\Sexpr{myround(ARMA_benchmarks$m2, 1)}$ &
  $\Sexpr{myround(ARMA_benchmarks$m3, 1)}$
\\
\hline
\end{tabular}
\caption{\label{tab:likes}Log-likelihood values for each models compared to their ARMA benchmarks.
Values in parenthesis are corresponding values using \citet{lee20} parameter estimates.  $\textsuperscript{1}$The reported likelihood is an upper bound of the likelihood of the \citet{lee20} model as it is the largest likelihood obtained using their parameter calibration regime. $\textsuperscript{2}$\cite{lee20} fit Model~3 to a subset of the data (March 2014 onward, excluding data from Ouest in 2015-2016).
On this subset, their model has a likelihood of $\Sexpr{myround(max(lee3_results$sub_ll_evals), 1)}$.
On this same subset, our model has a likelihood of $\Sexpr{myround(mod3_spatPomp_res$subset_ll, 1)}$.
}
\end{table}

Similar to comparing log-likelihoods across models, an additional powerful diagnosis tool is the comparison of conditional log-likelihoods.
Conditional likelihoods, defined as the density $f_{Y_k | Y_1, \ldots, Y_{k - 1}}\big(Y_k = y_k^* | y_{1:k-1}^*\big)$, provide a basic description of how well the proposed model can describe each data point, given the previous observations.
Comparing these results across models---including benchmark models---can help researchers identify potential model deficiencies, or errors in the observed data.
Additional tools for assessing the goodness-of-fit of a model include plotting the effective sample size of each observation \jwc{REF???} and comparing any statistic of the observed data to simulations from the model, which is sometimes referred to as diagnostic probes (for example, the autocorrelation function (ACF), as was done in \cite{king15}).

\subsection{Forecasts}\label{sec:filter}

The central goal of a forecast is to provide an accurate estimate of the future state of a system based on currently available data.
When a mechanistic model is used, forecasts may also provide estimates of the future effects of potential interventions.
Forecasting models are built using available scientific understanding, but forecasting can also be a way of testing new scientific hypotheses in real time \citep{lewis22}.
In order to provide trustworthy information, however, the reliability of the forecast should be undersood.
In particular, researchers should account for various forms of uncertainty present in model forecasts, and  calibrate the proposed model to observed data.

The most recently available information about a dynamic system should be used to obtain reliable forecasts.
Using state space model $m$, this means that in order to obtain a forecast up to time $N+s$, where $s\geq 0$ and $N$ is the index for the last available data, we should simulate from the conditional density $f_{\bm{X}^{(\modelCounter)}_{N:s}, \bm{Y}^{\modelCounter}_{N:s}}\big(x_{N:s}, y_{N:s}\mid \hat{\paramVec}, \bm{X}^{(\modelCounter)}_{N} = \bm{x}^{(\modelCounter)}_N\big)$.
The latent state $\bm{X}^{(\modelCounter)}_N$ is by definition unobservable, however, and therefore draws from the desired conditional density are unobtainable.
Informed estimates of $\bm{X}^{(\modelCounter)}_N$ given the observed data $\bm{Y}^{(\modelCounter)}_{1:N} = y_{1:N}^*$ can easily be obtained using the filtering distribution.
Let $\hat{\bm{X}}^{(\modelCounter), i}_N, \, i \in {1, 2, \ldots, J}$ be $\iid$ draws from the filtering distribution at time $N$, with density $\hat{\bm{X}}^{(\modelCounter), i}_N \sim f_{\bm{X}^{(\modelCounter)}_N|\bm{Y}^{(\modelCounter)}_{1:N}}(x_N \mid \hat{\paramVec}, y_{1:N}^*)$.
A single model forecast can then be obtained by simulating from the model $f_{\bm{X}^{(\modelCounter)}_{N:s}, \bm{Y}^{(\modelCounter)}_{N:s}}\big(x_{N:s}, y_{N:s}\mid \hat{\paramVec}, \bm{X}^{(\modelCounter)}_{N} = \hat{\bm{X}}^{(\modelCounter), i}_N\big)$.
Obtaining forecasts in this fashion allows us to obtain results that are consistent with the model assumptions and the observed data.
In this particular case study, projecting the future state of the cholera epidemic in Haiti starting from the draws from the filtering distribution allows the proposed models to benefit from the fact that very few cholera cases had been observed between 2018 and 2019, and that cases appear to be decreasing (i.e., the number of susceptible individuals may be small).
This alone partially explains the unrealistic forecasts of \cite{lee20}, which were obtained by simulating the model from initial conditions;
Table~S7 of their supplement material shows that of their simulations that were consistent with observing zero cases in 2019, all forecasts resulted in the elimination of cholera.
% \eic{THE QUESTION OF WHETHER TO USE A DETERMINISTIC MODEL REMAINS OF CURRENT INTEREST...}

Another consideration to make when obtaining model forecasts is that of parameter uncertainty.
It has been noted that the uncertainty in just a single parameter can lead to drastically different projections \citep{saltelli20}.
One possible approach to account for parameter uncertainty in model forecasts is by obtaining confidence intervals for each parameter, sampling parameters from the confidence intervals, simulating the model with the resulting parameter set, and then weighing the resulting model projections based on the likelihood of the given set of parameters, as was done in \citet{king15}.
\eic{THE ``OBVIOUS'' THING TO DO HERE IS A BAYESIAN APPROACH. IN SOME SENSE, KING15 IS EMPIRICAL BAYES. IT MAY TAKE CARE TO DISCUSS THIS WITHOUT OPENING UP A CAN OF WORMS - I CAN HAVE A GO AT MAKING SUGGESTIONS, NEXT TIME I'M WORKING ON THE MS.}

Note that in order to obtain projections that are consistent with the observed data, one must first be able to sample from the filtering distribution given each set of parameters.
This approach can be done for both deterministic and stochastic models, but requires a large number of computations, especially as the number of observations and model parameters increase.
Because the focus of this study is on model fitting and evaluation, we do not provide model projections accounting for parameter uncertainty.
Instead, we use the projections from point estimates to highlight a major deficiency of deterministic models, which is that the only variability in model projections is a result of parameter uncertainty, which leads to over-confidence in the projections.
This observation is consistent with those made in \cite{king15}, and suggests that stochastic models should be preferred over deterministic models.

To the credit of deterministic models, of the four fitted models in \cite{lee20}, Model~2 provides the most apparently accurate forecasts.
This perhaps demonstrates that while deterministic models describe the systems in a less realistic way, the relative ease in fitting these models potentially results in fewer modeling errors.
As the data analysis becomes more refined, however, the deficiencies of deterministic models become increasingly apparent.

Forecasting can be used to simulate the simulate various interventions on a system; this feature is useful to inform policy and was the primary goal of \citet{lee20}.
Outcomes of their study include estimates for the probability of cholera elimination and cumulative number of cholera infections under several possible vaccination scenarios.
Mimicking their efforts, we define cholera elimination as having less than one infection of cholera over at least 52 consecutive weeks in the 10-year projection period, and provide forecasts under the following vaccination scenarios:

\begin{itemize}
  \item[$V0$:] No additional vaccines are administered.
  \item[$V1$:] Vaccination limited to the departments of Centre and Artibonite, deployed over a two-year period.
  \item[$V2$:] Vaccination limited to three departments: Artibonite, Centre, and Ouest deployed over a two-year period.
  \item[$V3$:] Countrywide vaccination implemented over a five-year period.
  \item[$V4$:] Countrywide vaccination implemented over a two-year period.
\end{itemize}

<<Load Model 1 VaccScen Sims, message=FALSE, include=FALSE>>=

# Number of particles and simulations to use for pfilter and project from filter, respectively.
h1_scen_NP   <- switch(RUN_LEVEL, 50, 500, 5000)
h1_scen_sims <- switch(RUN_LEVEL, 20, 100,  500)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/S0_sims.rds")) {
  h1_S0_sims <- readRDS("model1/run_level_3/S0_sims.rds")
} else {
  h1_S0_sims <- bake(
    file = paste0("model1/", rl_dir, "S0_sims.rds"), {

      # Create the model
      s0 <- haiti1_joint(vacscen = 'id0')

      # Load coefficients
      coef(s0) <- p1

      # Get samples from filtering distribution
      s0_pf <- pfilter(s0, save.state = TRUE, Np = h1_scen_NP)

      # project vacc scenario, with filtering dist, and model.
      project_from_filter2(mod = s0, PF = s0_pf, nsims = h1_scen_sims)
    },
    timing = FALSE
  )
}

# Clean-up simulations for analysis/visualization
mod1_V0_sims <- agg_mod1_sims(h1_S0_sims)
mod1_V0_probs <- get_elimProbs(h1_S0_sims, model = 1)
rm(h1_S0_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/S2_sims.rds")) {
  h1_S2_sims <- readRDS("model1/run_level_3/S2_sims.rds")
} else {
  h1_S2_sims <- bake(
    file = paste0('model1/', rl_dir, 'S2_sims.rds'), {

      # Create the model
      s2 <- haiti1_joint(vacscen = "id2")

      # Set model parameters that need to be adjusted
      depts <- 2
      h1_par_names <- names(p1)
      h1_params_temp <- c(p1, rep(0.0, 5 * depts))
      names(h1_params_temp) <- c(
        h1_par_names,
        paste0("S", 1:depts, "_0"),
        paste0("E", 1:depts, "_0"),
        paste0("I", 1:depts, "_0"),
        paste0("A", 1:depts, "_0"),
        paste0("R", 1:depts, "_0")
      )
      coef(s2) <- h1_params_temp  # Set model parameters

      # Get samples from filtering distribution for Scenario 2 model:
      s2_pf <- pfilter(s2, save.state = TRUE, Np = h1_scen_NP)

      # project vacc scenario, with filtering dist, and model.
      project_from_filter2(mod = s2, PF = s2_pf, nsims = h1_scen_NP)
    },
    timing = FALSE
  )
}

# Clean-up simulations for analysis/visualization
mod1_V1_sims <- agg_mod1_sims(h1_S2_sims)
mod1_V1_probs <- get_elimProbs(h1_S2_sims, model = 1)
rm(h1_S2_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/S4_sims.rds")) {
  h1_S4_sims <- readRDS("model1/run_level_3/S4_sims.rds")
} else {
  h1_S4_sims <- bake(
    file = paste0('model1/', rl_dir, 'S4_sims.rds'), {

      # Create the model
      s4 <- haiti1_joint(vacscen = "id4")

      # Set model parameters that need to be adjusted
      depts <- 3
      h1_par_names <- names(p1)
      h1_params_temp <- c(p1, rep(0.0, 5 * depts))
      names(h1_params_temp) <- c(
        h1_par_names,
        paste0("S", 1:depts, "_0"),
        paste0("E", 1:depts, "_0"),
        paste0("I", 1:depts, "_0"),
        paste0("A", 1:depts, "_0"),
        paste0("R", 1:depts, "_0")
      )
      coef(s4) <- h1_params_temp  # Set model parameters

      # Get samples from filtering distribution for Scenario 2 model:
      s4_pf <- pfilter(s4, save.state = TRUE, Np = h1_scen_NP)

      # project vacc scenario, with filtering dist, and model.
      project_from_filter2(mod = s4, PF = s4_pf, nsims = h1_scen_NP)
    },
    timing = FALSE
  )
}

# Clean-up simulations for analysis/visualization
mod1_V2_sims <- agg_mod1_sims(h1_S4_sims)
mod1_V2_probs <- get_elimProbs(h1_S4_sims, model = 1)
rm(h1_S4_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/S3_sims.rds")) {
  h1_S3_sims <- readRDS("model1/run_level_3/S3_sims.rds")
} else {
  h1_S3_sims <- bake(
    file = paste0('model1/', rl_dir, 'S3_sims.rds'), {

      # Create the model
      s3 <- haiti1_joint(vacscen = "id3")

      # Set model parameters that need to be adjusted
      depts <- 10
      h1_par_names <- names(p1)
      h1_params_temp <- c(p1, rep(0.0, 5 * depts))
      names(h1_params_temp) <- c(
        h1_par_names,
        paste0("S", 1:depts, "_0"),
        paste0("E", 1:depts, "_0"),
        paste0("I", 1:depts, "_0"),
        paste0("A", 1:depts, "_0"),
        paste0("R", 1:depts, "_0")
      )
      coef(s3) <- h1_params_temp  # Set model parameters

      # Get samples from filtering distribution for Scenario 2 model:
      s3_pf <- pfilter(s3, save.state = TRUE, Np = h1_scen_NP)

      # project vacc scenario, with filtering dist, and model.
      project_from_filter2(mod = s3, PF = s3_pf, nsims = h1_scen_NP)
    },
    timing = FALSE
  )
}

# Clean-up simulations for analysis/visualization
mod1_V3_sims <- agg_mod1_sims(h1_S3_sims)
mod1_V3_probs <- get_elimProbs(h1_S3_sims, model = 1)
rm(h1_S3_sims)

# Simulate vaccination scenario from filtering distribution. The code
# below creates a model, samples from the filtering distribution, and then
# projects the vaccination scenario, under the specified model.
if (RUN_LEVEL != 3 & file.exists("model1/run_level_3/S1_sims.rds")) {
  h1_S1_sims <- readRDS("model1/run_level_3/S1_sims.rds")
} else {
  h1_S1_sims <- bake(
    file = paste0('model1/', rl_dir, 'S1_sims.rds'), {

      # Create the model
      s1 <- haiti1_joint(vacscen = "id1")

      # Set model parameters that need to be adjusted
      depts <- 10
      h1_par_names <- names(p1)
      h1_params_temp <- c(p1, rep(0.0, 5 * depts))
      names(h1_params_temp) <- c(
        h1_par_names,
        paste0("S", 1:depts, "_0"),
        paste0("E", 1:depts, "_0"),
        paste0("I", 1:depts, "_0"),
        paste0("A", 1:depts, "_0"),
        paste0("R", 1:depts, "_0")
      )
      coef(s1) <- h1_params_temp  # Set model parameters

      # Get samples from filtering distribution for Scenario 2 model:
      s1_pf <- pfilter(s1, save.state = TRUE, Np = h1_scen_NP)

      # project vacc scenario, with filtering dist, and model.
      project_from_filter2(mod = s1, PF = s1_pf, nsims = h1_scen_NP)
    },
    timing = FALSE
  )
}

# Clean-up simulations for analysis/visualization
mod1_V4_sims <- agg_mod1_sims(h1_S1_sims)
mod1_V4_probs <- get_elimProbs(h1_S1_sims, model = 1)
rm(h1_S1_sims, h1_scen_NP, h1_scen_sims)

gc()
@

<<Load Model 2 VaccScenarios, message=FALSE, include=FALSE>>=
# load('model2/output/mod2_vacc_scenarios.rda')

# Run Vaccination scenarios for Model 2. Note that the code runs the same
# speed for each run-level.
mod2_VaccScenarios <- bake(
  file = paste0("model2/", rl_dir, "VaccinationScenarios.rds"), {
    haiti2_vaccScenario(
      h2_params = h2_fit$h2_params
    )
  },
  timing = FALSE
)

@

<<Load Model 3 Vacc Scenarios, message=FALSE, include=FALSE>>=
NP_BPF <- switch(RUN_LEVEL, 50, 200, 1000)
NSIM   <- switch(RUN_LEVEL, 10,  50,  500)

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/h3_bpf.rds")) {
  h3_bpf <- readRDS("model3/run_level_3/h3_bpf.rds")
} else {
  h3_bpf <- bake(
    file = paste0("model3/", rl_dir, "h3_bpf.rds"),
    expr = {
      bpfilter(h3Spat, Np = NP_BPF, block_size = 1, save_states = TRUE)
    },
    timing = FALSE
  )
}

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/mod3_V0_res.rds")) {
  mod3_V0_res <- readRDS("model3/run_level_3/mod3_V0_res.rds")
} else {
  mod3_V0_res <- bake(
    file = paste0("model3/", rl_dir, "mod3_V0_res.rds"), {

      # Create a list to store results
      h3_V0_res <- list()

      # Get "noVacc" scenario parameters
      h3_S0_par <- get_model3_vacc_scenario_params(scenario = "noVacc")
      coef(h3Spat)[names(h3_S0_par)] <- h3_S0_par

      # Using parameters and filtered distribution, project system
      h3_V0_sims <- project_from_filter2(
        h3Spat, PF = h3_bpf, covarGen = project_rain,
        nsims = NSIM, seed = 6897154
      )

      # Store nationally aggregated results
      h3_V0_res$mod3_V0_sims <- agg_mod3_sims(h3_V0_sims)
      h3_V0_res$mod3_V0_probs <- get_elimProbs(h3_V0_sims, model = 3)

      # Return results
      h3_V0_res

    }
  )
}

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/mod3_V1_res.rds")) {
  mod3_V1_res <- readRDS("model3/run_level_3/mod3_V1_res.rds")
} else {
  mod3_V1_res <- bake(
    file = paste0("model3/", rl_dir, "mod3_V1_res.rds"), {

      # Create a list to store results
      h3_V1_res <- list()

      # Get "noVacc" scenario parameters
      h3_V1_par <- get_model3_vacc_scenario_params(scenario = "2dep")
      coef(h3Spat)[names(h3_V1_par)] <- h3_V1_par

      # Using parameters and filtered distribution, project system
      h3_V1_sims <- project_from_filter2(
        h3Spat, PF = h3_bpf, covarGen = project_rain,
        nsims = NSIM, seed = 6897154
      )

      # Store nationally aggregated results
      h3_V1_res$mod3_V1_sims <- agg_mod3_sims(h3_V1_sims)
      h3_V1_res$mod3_V1_probs <- get_elimProbs(h3_V1_sims, model = 3)

      # Return results
      h3_V1_res

    }
  )
}

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/mod3_V2_res.rds")) {
  mod3_V2_res <- readRDS("model3/run_level_3/mod3_V2_res.rds")
} else {
  mod3_V2_res <- bake(
    file = paste0("model3/", rl_dir, "mod3_V2_res.rds"), {

      # Create a list to store results
      h3_V2_res <- list()

      # Get "noVacc" scenario parameters
      h3_V2_par <- get_model3_vacc_scenario_params(scenario = "3dep")
      coef(h3Spat)[names(h3_V2_par)] <- h3_V2_par

      # Using parameters and filtered distribution, project system
      h3_V2_sims <- project_from_filter2(
        h3Spat, PF = h3_bpf, covarGen = project_rain,
        nsims = NSIM, seed = 6897154
      )

      # Store nationally aggregated results
      h3_V2_res$mod3_V2_sims <- agg_mod3_sims(h3_V2_sims)
      h3_V2_res$mod3_V2_probs <- get_elimProbs(h3_V2_sims, model = 3)

      # Return results
      h3_V2_res

    }
  )
}

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/mod3_V3_res.rds")) {
  mod3_V3_res <- readRDS("model3/run_level_3/mod3_V3_res.rds")
} else {
  mod3_V3_res <- bake(
    file = paste0("model3/", rl_dir, "mod3_V3_res.rds"), {

      # Create a list to store results
      h3_V3_res <- list()

      # Get "noVacc" scenario parameters
      h3_V3_par <- get_model3_vacc_scenario_params(scenario = "slowNation")
      coef(h3Spat)[names(h3_V3_par)] <- h3_V3_par

      # Using parameters and filtered distribution, project system
      h3_V3_sims <- project_from_filter2(
        h3Spat, PF = h3_bpf, covarGen = project_rain,
        nsims = NSIM, seed = 6897154
      )

      # Store nationally aggregated results
      h3_V3_res$mod3_V3_sims <- agg_mod3_sims(h3_V3_sims)
      h3_V3_res$mod3_V3_probs <- get_elimProbs(h3_V3_sims, model = 3)

      # Return results
      h3_V3_res

    }
  )
}

if (RUN_LEVEL != 3 & file.exists("model3/run_level_3/mod3_V4_res.rds")) {
  mod3_V4_res <- readRDS("model3/run_level_3/mod3_V4_res.rds")
} else {
  mod3_V4_res <- bake(
    file = paste0("model3/", rl_dir, "mod3_V4_res.rds"), {

      # Create a list to store results
      h3_V4_res <- list()

      # Get "noVacc" scenario parameters
      h3_V4_par <- get_model3_vacc_scenario_params(scenario = "fastNation")
      coef(h3Spat)[names(h3_V4_par)] <- h3_V4_par

      # Using parameters and filtered distribution, project system
      h3_V4_sims <- project_from_filter2(
        h3Spat, PF = h3_bpf, covarGen = project_rain,
        nsims = NSIM, seed = 6897154
      )

      # Store nationally aggregated results
      h3_V4_res$mod3_V4_sims <- agg_mod3_sims(h3_V4_sims)
      h3_V4_res$mod3_V4_probs <- get_elimProbs(h3_V4_sims, model = 3)

      # Return results
      h3_V4_res

    }
  )
}
@

Simulations from probabilistic models (Models~1 and~3) represent possible trajectories of the dynamic system under the scientific assumptions of the models.
In this case study, estimates of the probability of cholera elimination can therefore be obtained as the proportion of simulations from the fitted model that result in cholera elimination.
The results of these projections are summarized in Figs.~\ref{fig:Mod1Scenarios}--\ref{fig:elimProbs}.
These results suggest that cholera elimination was likely, even without increased vaccination efforts, which is consistent with observed reality \citep{trevisin22}.

Probability of elimination estimates of this form are not meaningful for deterministic models, as the trajectory of these models only represent the mean behavior of the system rather than individual potential outcomes.
We therefore do not provide probability of elimination estimates under Model~2.
Still, trajectories obtained by Model~2 are consistent with the simulation results of Models~1 and~3, and suggest that cholera was in the process of being eliminated from Haiti.

In additional to probability of elimination estimates, we provide estimates for the cumulative number of infections under each vaccination scenario from February 2019 -- February 2024.
Notably, the median number of cumulative cholera infections under the no-vaccination scenario using Models 1 and 3 were $\Sexpr{prettyNum(round(as.numeric(mod1_V0_probs$cumInf['q50'])), big.mark = ",")}$ and $\Sexpr{prettyNum(round(as.numeric(mod3_V0_res$mod3_V0_probs$cumInf['q50'])), big.mark = ",")}$, respectively.
While there is remaining time during this projection period in which new cholera infections can be detected, up to this point our estimates are far more consistent with the observed number of reported cholera cases than the corresponding estimates from \cite{lee20}, which were approximately $400,000$ and $1,000,000$.

<<Create Model 1 VaccScen plots>>=
gg_m1_V0 <- ggplot() +
  geom_line(data = mod1_V0_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V0_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title.x = element_blank()) +
  ggtitle("V0") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V1 <- ggplot() +
  geom_line(data = mod1_V1_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V1_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V1") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V2 <- ggplot() +
  geom_line(data = mod1_V2_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V2_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V2") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V3 <- ggplot() +
  geom_line(data = mod1_V3_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V3_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V3") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m1_V4 <- ggplot() +
  geom_line(data = mod1_V4_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod1_V4_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V4") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

@

\begin{figure}[ht]
<<Plot_Model1_Scenarios, fig.height=2>>=
lay <- rbind(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5))
gridExtra::grid.arrange(
  gg_m1_V0, gg_m1_V1, gg_m1_V2, gg_m1_V3, gg_m1_V4,
  layout_matrix = lay
)
@
\caption{\label{fig:Mod1Scenarios}
Simulations of Model~1 under each vaccination scenario.
Blue line indicates the median of model simulations, and ribbon represents $95\%$ of simulations.
The various vaccination campaigns made no practical difference in the median scenario, but a drastic difference in the extreme cases.
}
\end{figure}

\begin{figure}[ht]
<<Mod2Fit_and_Scenarios_Figure, fig.height=2, fig.width=5, fig.align='center'>>=
ggplot() +
  geom_line(data = h2_traj, aes(x = date, y = Ctotal + 1), col = 'blue') +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  geom_line(data = filter(mod2_VaccScenarios$all_trajs,
                          year >= max(h2_traj$year)),
            aes(x = as.Date(date), y = ReportedCases + 1, color = scenario)) +
  theme(axis.title.x = element_blank(),
        legend.title = element_blank(),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 8)) +
  ylab("Reported cholera cases") +
  scale_color_manual(values = c("V0" = '#f4a582',
                                "V1" = '#d6604d',
                                "V2" = '#b2182b',
                                "V3" = '#92c5de',
                                "V4" = '#4393c3')) +
  # geom_vline(xintercept = yearsToDate(2014.161), linetype = 'dashed') +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2010-01-01"), as.Date("2030-01-01"), by = '2 years'))
@
\caption{\label{fig:mod2Traj}
Simulated trajectory of Model~2 (blue curve) and projections under the various vaccination scenarios.
Reported cholera incidence is shown in black.}
\end{figure}

<<Create Model 3 VaccScen Plots>>=
gg_m3_V0 <- ggplot() +
  geom_line(data = mod3_V0_res$mod3_V0_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V0_res$mod3_V0_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title.x = element_blank()) +
  ggtitle("V0") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V1 <- ggplot() +
  geom_line(data = mod3_V1_res$mod3_V1_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V1_res$mod3_V1_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
  theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V1") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V2 <- ggplot() +
  geom_line(data = mod3_V2_res$mod3_V2_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V2_res$mod3_V2_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V2") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V3 <- ggplot() +
  geom_line(data = mod3_V3_res$mod3_V3_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V3_res$mod3_V3_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V3") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))

gg_m3_V4 <- ggplot() +
  geom_line(data = mod3_V4_res$mod3_V4_sims, aes(x = date, y = q50 + 1), col = 'blue') +
  geom_ribbon(data = mod3_V4_res$mod3_V4_sims, aes(x = date, ymin = q05 + 1, ymax = q95 + 1), alpha = 0.5) +
  geom_line(data = true_agg_cases, aes(x = date, y = ReportedAll + 1)) +
  ylab("Log reported cases") +
    theme(axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  ggtitle("V4") +
  scale_x_date(date_labels = "'%y") +
  scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x)))
@


\begin{figure}[ht]
<<Plot_Model3_Scenarios, fig.height=2>>=
lay <- rbind(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5))
gridExtra::grid.arrange(
  gg_m3_V0, gg_m3_V1, gg_m3_V2, gg_m3_V3, gg_m3_V4,
  layout_matrix = lay
)
@
\caption{\label{fig:Mod3Scenarios}
Simulations of Model~3 under each vaccination scenario.
Blue line indicates the median of model simulations, and ribbon represents $95\%$ of simulations.
The various vaccination campaigns made no practical difference in the median scenario, but a drastic difference in the extreme cases.
}
\end{figure}

% \begin{figure}[ht]
% \centering
%
% mod1_cumInf <- data.frame(
%   'scenario' = paste0("V", 0:4),
%   'low' = c(
%     as.numeric(mod1_V0_probs$cumInf['q025']),
%     as.numeric(mod1_V1_probs$cumInf['q025']),
%     as.numeric(mod1_V2_probs$cumInf['q025']),
%     as.numeric(mod1_V3_probs$cumInf['q025']),
%     as.numeric(mod1_V4_probs$cumInf['q025'])
%   ),
%   'high' = c(
%     as.numeric(mod1_V0_probs$cumInf['q975']),
%     as.numeric(mod1_V1_probs$cumInf['q975']),
%     as.numeric(mod1_V2_probs$cumInf['q975']),
%     as.numeric(mod1_V3_probs$cumInf['q975']),
%     as.numeric(mod1_V4_probs$cumInf['q975'])
%   ),
%   'med' = c(
%     as.numeric(mod1_V0_probs$cumInf['q50']),
%     as.numeric(mod1_V1_probs$cumInf['q50']),
%     as.numeric(mod1_V2_probs$cumInf['q50']),
%     as.numeric(mod1_V3_probs$cumInf['q50']),
%     as.numeric(mod1_V4_probs$cumInf['q50'])
%   )
% )
%
% mod3_cumInf <- data.frame(
%   'scenario' = paste0("V", 0:4),
%   'low' = c(
%     as.numeric(mod3_V0_probs$cumInf['q025']),
%     as.numeric(mod3_V1_probs$cumInf['q025']),
%     as.numeric(mod3_V2_probs$cumInf['q025']),
%     as.numeric(mod3_V3_probs$cumInf['q025']),
%     as.numeric(mod3_V4_probs$cumInf['q025'])
%   ),
%   'high' = c(
%     as.numeric(mod3_V0_probs$cumInf['q975']),
%     as.numeric(mod3_V1_probs$cumInf['q975']),
%     as.numeric(mod3_V2_probs$cumInf['q975']),
%     as.numeric(mod3_V3_probs$cumInf['q975']),
%     as.numeric(mod3_V4_probs$cumInf['q975'])
%   ),
%   'med' = c(
%     as.numeric(mod3_V0_probs$cumInf['q50']),
%     as.numeric(mod3_V1_probs$cumInf['q50']),
%     as.numeric(mod3_V2_probs$cumInf['q50']),
%     as.numeric(mod3_V3_probs$cumInf['q50']),
%     as.numeric(mod3_V4_probs$cumInf['q50'])
%   )
% )
%
% mod1_cumInf$model <- "1"
% mod3_cumInf$model <- "3"
%
% all_cumInf <- rbind(mod1_cumInf, mod3_cumInf)
% all_cumInf$scenario <- factor(all_cumInf$scenario, levels = rev(c("V0", "V1", "V2", "V3", "V4")))
%
%
% ggplot(all_cumInf, aes(y = scenario)) +
%   geom_point(aes(x = med)) +
%   geom_linerange(aes(xmin = low, xmax = high)) +
%   scale_x_continuous(
%     labels = scales::unit_format(scale = 1e-6, accuracy = 1),
%     limits = c(0, 5e6)
%   ) +
%   xlab("Cumulative infections (million)") +
%   theme(axis.title.y = element_blank(),
%         axis.title.x = element_text(size = 10),
%         axis.text = element_text(size = 8)) +
%   facet_wrap(~model, ncol = 1,
%              labeller = as_labeller(
%                c("1" = "Model 1", "3" = "Model 3", "2" = "Model 2")
%              ))
%
% \caption{\label{fig:cumInfPlot}
% Simulated cumulative infections under various vaccination scenarios from February, 2019, to February, 2024.
% Point estimates (median cumulative incidence) and $95\%$ error bars are included.
% x-axis scale is intended to ease comparison with Figure~4 of \cite{lee20}.
% }
% \end{figure}

<<Compute Elimination Probs, echo=FALSE>>=
mod1_V0_probs$ElimTime$scenario = "V0"
mod1_V1_probs$ElimTime$scenario = "V1"
mod1_V2_probs$ElimTime$scenario = "V2"
mod1_V3_probs$ElimTime$scenario = "V3"
mod1_V4_probs$ElimTime$scenario = "V4"

mod1_probElims <- rbind(
  mod1_V0_probs$ElimTime,
  mod1_V1_probs$ElimTime,
  mod1_V2_probs$ElimTime,
  mod1_V3_probs$ElimTime,
  mod1_V4_probs$ElimTime
)

mod3_V0_res$mod3_V0_probs$ElimTime$scenario = "V0"
mod3_V1_res$mod3_V1_probs$ElimTime$scenario = "V1"
mod3_V2_res$mod3_V2_probs$ElimTime$scenario = "V2"
mod3_V3_res$mod3_V3_probs$ElimTime$scenario = "V3"
mod3_V4_res$mod3_V4_probs$ElimTime$scenario = "V4"

mod3_probElims <- rbind(
  mod3_V0_res$mod3_V0_probs$ElimTime,
  mod3_V1_res$mod3_V1_probs$ElimTime,
  mod3_V2_res$mod3_V2_probs$ElimTime,
  mod3_V3_res$mod3_V3_probs$ElimTime,
  mod3_V4_res$mod3_V4_probs$ElimTime
)

mod1_probElims$mod <- "Model 1"
mod3_probElims$mod <- "Model 3"
mod3_probElims$time <- as.Date(lubridate::round_date(lubridate::date_decimal(mod3_probElims$time), unit = 'day'))

all_prob_elims <- dplyr::bind_rows(mod1_probElims, mod3_probElims)
@

\begin{figure}[ht]
<<Elimination_Probs_Figure, fig.height=2.5>>=
ggplot(tidyr::drop_na(all_prob_elims), aes(x = time, y = elim_prob, col = mod)) +
  geom_line() +
  facet_wrap(~scenario, nrow = 1) +
  scale_color_manual(values = c("Model 1" = "#377eb8", "Model 3" = "#e41a1c")) +
  ylab("Probability of Elimination (%)") +
  theme(axis.title.x = element_blank(),
        legend.position = 'bottom',
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 35, hjust = 1),
        legend.margin=margin(c(-5, 0, -3, 0)),
        legend.box.margin = margin(c(-5, 0, -3, 0))) +
  scale_x_date(date_labels = "'%y", breaks = seq.Date(from = as.Date("2010-01-01"), as.Date("2030-01-01"), by = '2 years'))
@
\caption{\label{fig:elimProbs}
Probability of elimination across simulations for a 10 year period.
Compare to Figure 3A of \cite{lee20}.}
\end{figure}

%%% table tttttttttt

\begin{table}
% \small
\resizebox{\textwidth}{!}{
\hspace{-10mm}\begin{tabular}{|l@{}|l@{}t@{}|l@{}t@{}|l@{}t@{}|}
\hline
Mechanism & Model 1 && Model 2 && Model 3 &
\\
\hline
\hline
Infection (day)
  & {\fixed $\muIR^{-1}=\Sexpr{myround(7/p1["gamma"],1)}$}
  & \eqref{model1:toR}
  & {\fixed $\muIR^{-1}=\Sexpr{myround(365/h2_fit$h2_params["gamma"],1)}$}
  & \eqref{model2:mu_IR}
  & {\fixed $\muIR^{-1}=\Sexpr{myround(365/p3["gamma"],1)}$ }
  & \eqref{eq:model3:IR}
\\
Latency (day)
  & {\fixed $\muEI^{-1}=\Sexpr{myround(7/p1["sigma"],1)}$}
  & \eqref{model1:EA}
  & {\fixed $\muEI^{-1}=\Sexpr{myround(365/h2_fit$h2_params["gammaE"],1)}$}
  & \eqref{model2:mu_EI}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
\\
\hline
Seasonality
  & $\begin{array}{l}
    \hspace{-1mm} \transmission_{1:6}=(
      \Sexpr{myround(p1["beta1"],1)},
      \Sexpr{myround(p1["beta2"],1)},
    \\
      \Sexpr{myround(p1["beta3"],1)},
      \Sexpr{myround(p1["beta4"],1)},
      \Sexpr{myround(p1["beta5"],1)},
      \Sexpr{myround(p1["beta6"],1)})
    \end{array}$
  & \eqref{model1:beta}
  & {\fixed $\seasAmplitude=0.4$ }
  & \eqref{model2:lambda}
  & $\begin{array}{l} \seasAmplitude=\Sexpr{myround(p3s["lambdaR"],2)} \\ r=\Sexpr{signif(p3s["r"],3)} \end{array}$
  & \eqref{eq:model3:water}
\\
\hline
$\begin{array}{l}
\text{Immunity} \\
\text{(year)}
\end{array}$
  & {\fixed $\muRS^{-1}=\Sexpr{myround(1/p1["alpha"]/52,1)}$}
  & \eqref{model1:RS}
  & {\fixed
    $\begin{array}{l}
    \muRS^{-1}= \Sexpr{signif(1/h2_fit$h2_params["sigma"],2)} \\
     \omega_1^{-1} = \Sexpr{myround(1/h2_fit$h2_params["Omega1"],1)} \\
    \omega_2^{-1} = \Sexpr{myround(1/h2_fit$h2_params["Omega2"],1)}
    \end{array}$
    }
  & $\begin{array}{l}
    \eqref{model2:RS} \\
    \eqref{model2:omega1} \\
    \eqref{model2:omega1}
    \end{array} $
  & {\fixed $\muRS^{-1}=\Sexpr{myround(1/p3s["rho"],1)}$}
  & \eqref{eq:model3:RRnext}
\\
\hline
Vaccine efficacy
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\begin{array}{l}
      \hspace{-1mm} \fixed{\vaccineEfficacy_{1:4} = (
        \Sexpr{myround(1 - h2_fit$h2_params["VE1"] * 0.4688, 2)},
        \Sexpr{myround(1 - h2_fit$h2_params["VE2"] * 0.4688, 2)}} \\
        \fixed{\Sexpr{myround(1 - h2_fit$h2_params["VE1"], 2)},
        \Sexpr{myround(1 - h2_fit$h2_params["VE2"], 2)}})
        \end{array}$
  & \eqref{model2:mu_SE}
  & \fixed{$\eta_{ud}(t)$}
  &
  \\
Birth/death (yr)
  & \fixed{$\begin{array}{l}
    \muBirth^{-1} = \Sexpr{myround(1/p1["mu"]/52,1)} \\
    \muDeath^{-1} = \Sexpr{myround(1/p1["delta"]/52,1)}
    \end{array}$}
  & \eqref{model1:death}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & {\fixed $\muDeath^{-1} = \Sexpr{myround(1/p3s["mu"],1)}$}
  & \eqref{eq:model3:IS}
\\
Symptomatic frac.
  & {\fixed $\symptomFrac_z(t)=c\theta^*(t-\tau_d)$}
  & (\ref{model1:EI}-\ref{model1:EA})
  & {\fixed $\symptomFrac=\Sexpr{myround(h2_fit$h2_params["k"],1)}$}
  & \eqref{model2:mu_EI}
  & {\fixed $\symptomFrac=\Sexpr{myround(p3s["sigma"],2)}$}
  & \eqref{eq:model3:SA}
\\
$\begin{array}{l}
\text{Asymptomatic} \\
\text{infectivity}
\end{array}$
  & {\fixed $\asymptomRelativeInfect=0.05$ }
  & \eqref{model1:lambda}
  & $\begin{array}{l}
      {\fixed \asymptomRelativeInfect =0.001 } \\
      {\fixed \asymptomRelativeShed = \Sexpr{1e-7} }
    \end{array}$
  & \begin{tabular}{l}
      \eqref{model2:lambda} \\
      \eqref{model2:to_W}
    \end{tabular}
  &  $\begin{array}{l}
      {\fixed \asymptomRelativeInfect =1 } \\
      {\fixed \asymptomRelativeShed = \Sexpr{myround(p3s["XthetaA"],3)} }
    \end{array}$
  & \begin{tabular}{l}
      \eqref{eq:model3:foi} \\
      \eqref{eq:model3:water}
    \end{tabular}
\\
\hline
$\begin{array}{l}
\text{Human to}\\
\text{human}\end{array}$
  & $\transmission_{1:6}$ as above
  & \eqref{model1:lambda}
  & $\transmission=$\Sexpr{signif(h2_fit$h2_params["Beta"],3)}
  & \eqref{model2:lambda}
  & $\begin{array}{l}
      \hspace{-1mm} \transmission_{1:10}=(
        \Sexpr{myround(p3u["foi_add",1]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",2]*1e6,2)},
      \\
	\Sexpr{myround(p3u["foi_add",3]*1e6,2)},
        \Sexpr{myround(p3u["foi_add",4]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",5]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",6]*1e6,2)},
      \\
	\Sexpr{myround(p3u["foi_add",7]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",8]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",9]*1e6,2)},
	\Sexpr{myround(p3u["foi_add",10]*1e6,2)}
	)
      \\
      \times 10^{-6}
    \end{array}$
  & \eqref{eq:model3:foi}
\\
\hline
$\begin{array}{l}
\text{Water to}\\ \text{human}
\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\begin{array}{l}
    {\fixed \Wsat = \Sexpr{signif(h2_fit$h2_params["Sat"],2)} }
    \\
    \beta_W= \Sexpr{signif(h2_fit$h2_params["BetaW"],3)}
    \end{array}$
  & \eqref{model2:lambda}
  & $\begin{array}{l}
      \hspace{-1mm} \Wbeta{_{1:10}}= (
        \Sexpr{myround(p3u["betaB",1],2)}, \Sexpr{myround(p3u["betaB",2],2)},
       \\
        \Sexpr{myround(p3u["betaB",3],2)}, \Sexpr{myround(p3u["betaB",4],2)},
        \Sexpr{myround(p3u["betaB",5],2)}, \Sexpr{myround(p3u["betaB",6],2)},
       \\
        \Sexpr{myround(p3u["betaB",7],2)}, \Sexpr{myround(p3u["betaB",8],2)},
        \Sexpr{myround(p3u["betaB",9],2)}, \Sexpr{myround(p3u["betaB",10],2)}
      ) \end{array}$
  & \eqref{eq:model3:foi}
\\
\hline
$\begin{array}{l}
\text{Human to}\\
\text{water}\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\Wshed = $ \Sexpr{signif(h2_fit$h2_params["Mu"],3)}
  & \eqref{model2:to_W}
  & $\Wshed= \Sexpr{signif(p3s["thetaI"],3)}$
  & \eqref{eq:model3:water}
\\
$\begin{array}{l}
\text{Water} \\
\text{survival (wk)}
\end{array}$
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\Wremoval^{-1} = $ \Sexpr{signif(52/h2_fit$h2_params["Delta"],3)}
  & \eqref{model2:from_W}
  & $\Wremoval^{-1}=\Sexpr{myround(52/p3s["mu_B"],2)}$
  & \eqref{eq:model3:Decay}
\\
Mixing exponent
  & $\mixExponent=\Sexpr{myround(p1["nu"],2)}$
  & \eqref{model1:lambda}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
\\
$\begin{array}{l}
\text{Process} \\
\text{noise} (\text{wk}^{1/2})
\end{array}$
  & $\sigmaProc=(\Sexpr{myround(sqrt(p1["sig_sq_epi"]),2)}, \Sexpr{myround(sqrt(p1["sig_sq_end"]),2)})$
  & \eqref{model1:lambda}
  & \multicolumn{1}{c}{\hspace{16pt}\missing}
  &
  & $\sigmaProc= \Sexpr{myround(p3s["std_W"],3)}$
  & \eqref{eq:model3:SA}
\\
Reporting rate
  & $\reportRate=(\Sexpr{myround(p1["rho_epi"],3)}, \Sexpr{myround(p1["rho_end"],3)})$
  & (S16) % TODO: Make sure manual equations match!
  & {\fixed $\reportRate=\Sexpr{myround(h2_fit$h2_params["Rho"],2)}$}
  & (S17)
  & $\reportRate=\Sexpr{myround(p3s["epsilon"],2)}$
  & (S18)
\\
$\begin{array}{l}
\text{Observation} \\
\text{overdispersion}
\end{array}$
  & $\obsOverdispersion=(\Sexpr{round(p1["tau_epi"],2)}, \Sexpr{round(p1["tau_end"],2)})$
  & (S16)
  &
  &
  & $\obsOverdispersion=\Sexpr{myround(p3s["k"],2)}$
  & (S18)
\\
\hline
\end{tabular}}
\caption{References to the relevant equation are given in parentheses.
Parameters in blue were fixed based on scientific reasoning and not fitted to the data.
[N] denotes parameters added during our re-analysis, not considered by Lee et al.
Translations back into the notation of \citet{lee20} are given in Table~S1.
}
\end{table}

\subsection{Reckoning Scientific Understanding With Fitted Models}\label{sec:science}

The resulting mechanisms in a fitted model can be compared to current scientific knowledge about a system.
Synonymity between model based inference and our current understanding of a system may taken as a confirmation of both model based conclusions and our understanding of the system \jwc{REF???}.
On the other hand, discrepancies found in these comparisons may have the potential to spark new scientific breakthroughs \jwc{REFS???}.
The potential for new discoveries based on dynamic modeling outcomes is rooted in a model's ability to effectively approximate the system in question, and is therefore inexorably linked to a models goodness-of-fit.
As such, the perception of scientific conclusions made via dynamic modeling should be weighed by the model's goodness-of-fit.

In our analysis, we demonstrate how one may compare model mechanisms with current scientific understanding by examining the results of fitting the flexible cubic spline term in Model~1 (Eq.~\myeqref{model1:lambda}--\myeqref{model1:beta});
the cubic splines permit flexible estimation of seasonality in the force of infection.
After fitting the model, we explore potential patterns in the seasonal transmission rate by plotting the average value of $\transmission$ in a typical year.
Fig.~\ref{fig:h1SeasRain} shows that the estimated seasonal transmission rate $\transmission$ mimics the rainfall dynamics in Haiti, despite Model~1 not having access to rainfall data.
This result provides evidence that rainfall played an important role in cholera transmission in Haiti, consistent with previous studies \citep{lemaitre19,eisenberg13};
the estimated seasonality also features an increased transmission rate during the Fall, which was noticed at an earlier stage of the epidemic \citep{rinaldo12}.

<<Get Model 1 Seasonality>>=
plot_model_fun <- function(mod) {
  params <- coef(mod)

  beta1 <- params['beta1']
  beta2 <- params['beta2']
  beta3 <- params['beta3']
  beta4 <- params['beta4']
  beta5 <- params['beta5']
  beta6 <- params['beta6']
  betat <- params['betat']

  get_beta <- function(time) {

    covar <- t(mod@covar@table)
    covar_df <- as.data.frame(covar)
    covar_df$times <- mod@covar@times

    betas <- matrix(c(beta1, beta2, beta3, beta4, beta5, beta6), nrow = 1)
    si <- matrix(unlist(covar_df[time, -7]), ncol = 1)
    if (time <= 430) {
      return(as.numeric(exp(betas %*% si + betat * ((time - 215) / (430-215)))))
    } else {
      return(as.numeric(exp(betas %*% si + betat)))
    }
  }

  return(get_beta)
}

get_beta1 <- plot_model_fun(h1)

h1_seas_df <- data.frame(
  'week' = 431:800,
  'date' = lubridate::ymd("2010-10-16") + lubridate::weeks(431:800),
  'trans' = purrr::map_dbl(431:800, get_beta1)
) %>%
  mutate(year = lubridate::year(date)) %>%
  filter(year == 2020) %>%
  mutate(trans_std = (trans - min(trans)) / (max(trans) - min(trans)))

std_rain <- function(x) {
  # This function simply standardizes the rain for us.
  x / max(x)
}

df_rain <- haitiRainfall %>%
  dplyr::summarize(
    date = date, dplyr::across(Artibonite:`Sud-Est`, std_rain)
  ) %>%
  pivot_longer(
    data = .,
    cols = -date,
    names_to = 'dep',
    values_to = 'rainfall'
  ) %>%
  mutate(
    year = lubridate::year(date),
    week = lubridate::week(date)
  ) %>%
  group_by(year, week) %>%
  summarize(national_weekly_rain = sum(rainfall)) %>%
  ungroup() %>% # Not needed, but good practice
  mutate(
    date_week_start = lubridate::ymd(paste0(2014, "-01-01")) + lubridate::weeks(week - 1),
    week_date = as.Date(date_week_start, format = "%m-%d"),
    std_national_rain = (national_weekly_rain - min(national_weekly_rain, na.rm = TRUE)) / (max(national_weekly_rain, na.rm = TRUE) - min(national_weekly_rain, na.rm = TRUE))
  )

mean_rain <- df_rain %>%
  group_by(week_date) %>%
  summarize(mean_rain = mean(std_national_rain)) %>%
  ungroup() %>%
  mutate(mean_rain = (mean_rain - min(mean_rain)) / ((max(mean_rain) - min(mean_rain))))

gg_rain <- df_rain %>%
  filter(week != 1, week != max(week)) %>%
  ggplot(aes(x = week_date, y = std_national_rain, group = year, col = year)) +
  geom_line() +
  theme_bw() +
  guides(color = 'none') +
  ylab('Standardized Weekly\nNational Rain') +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 7.5),
        axis.title.y = element_text(size = 9))

gg_trans <- ggplot() +
  geom_line(data = h1_seas_df, aes(x = date, y = trans_std), linetype = 'dashed') +
  geom_line(
    data = mean_rain,
    aes(x = week_date + lubridate::years(6), y = mean_rain),
    col = '#2166ac'
  ) +
  scale_x_date(date_labels = "%b", date_breaks = '1 month') +
  ylab('Standardized Seasonal\nContact Rate') +
  theme(axis.title.x = element_blank(),
        axis.text = element_text(size = 7.5),
        axis.title.y = element_text(size = 9))

@

\begin{figure}[ht]
\centering
<<Model1_Seasonality_Figure, fig.height=2.9, fig.width=5, fig.align='center'>>=
cowplot::plot_grid(gg_rain, gg_trans, align = "v", ncol = 1)
@
\caption{\label{fig:h1SeasRain}
(Top) weekly rainfall in Haiti, lighter colors representing more recent years.
(Bottom) estimated seasonality in the transmission rate (dashed line) plotted alongside mean rainfall (solid line).
}
\end{figure}

While dynamic models may potentially result in meaningful scientific insights, one must be careful not to assume the results of a single analysis as a de-facto feature of the system.
It is important instead to recognize and assess the modeling simplifications and assumptions that were used in order to arrive at the conclusions.
In epidemiological studies, for example, it is unreasonable to expect that our quantitative understanding of individual-level processes will perfectly match model parameters that were fit to population-level case counts \jwc{REF??}.
This makes the direct interpretation of such parameters difficult.

Our case study provides an example of this in the parameter estimate for the duration of natural immunity due to cholera infection $\muRS^{-1}$.
Under the framework of Model~2, the best estimate for this parameter is $\Sexpr{signif(1/h2_fit$h2_params["sigma"],2)}$, suggesting that individuals have effectively permanent immunity to cholera once infected, and thus the pool of susceptible individuals decreases over time.
The interpretation of this result is muddled due to the fact that the data ranged from 2010-2019, and therefore estimates of immunity longer than 10 years---the upper end of previous estimates of natural immunity \citep{king08}---effectively result in the same model dynamics.
The depletion of susceptible individuals may also be attributed to confounding mechanisms that are unaccounted for in the model---such as vaccination and other non-pharmaceutical interventions that reduce cholera transmission \citep{trevisin22, rebaudet21}---that were not accounted for in the model.
Perhaps the best interpretation of this results, then, is that the model most adequately describes the observed data by having a steady decrease in the number of susceptible individuals; in this case, the fact that the parameter estimate $\muRS$ is inconsistent with the expected result can be taken as evidence of model mispecification.


\section{Advising policy via dynamic modeling}\label{sec:advice}

\eic{I THINK IT IS INTERESTING THAT THE MODEL-SUPPORTED SUGGESTION BY \citep{andrews11} HAS LARGELY PROVED ACCURATE (VACCINATION, NON-PHARMACEUTICAL INTERVENTIONS, AND INFRASTRUCTURE IMPROVEMENTS ALL HAVE A ROLE AND SHOULD BE FOLLOWED SIMULTANEOUSLY)DESPITE LEGITIMATE CONCERNS ABOUT WHETHER THE SCIENTIFIC SUPPORT FOR THE MODEL IS STRONG ENOUGH TO ROBUSTLY GUIDE POLICY \citep{grad12}.
THE MAIN CLAIM OF ANDREWS11 IS THAT FORECASTS AND POLICY NOT SUPPORTED BY A MODEL SEEM EVEN MORE UN-ROOTED THAN RECOMMENDATIONS BASED ON A WEAKLY SUPPORTED MODEL.
THE CONCLUSION FROM ALL THIS, WHICH I THINK IS SUPPORTED BY OUR RE-ANALYSIS OF LEE ET AL, IS THAT MECHANISTIC MODELS FITTED TO DATA ARE A STRONG WAY TO ENFORCE CONSISTENT THINKING, BUT WEAKER AS A WAY TO CHECK THAT YOU ARE ACTUALLY RIGHT DUE TO THE LIMITATIONS OF INFERRING CAUSAL MECHANISMS FROM OBSERVATIONAL DATA.
POLICY-MAKERS SHOULD FOLLOW AN APPROACH WHICH IS SELF-CONSISTENT AND CONSISTENT WITH PREVIOUS EVIDENCE THAT MIGHT WORK (WHICH MODELS CAN HELP WITH) BUT THEY MUST BE AGILE AS NEW EVIDENCE EMERGES.
MODELS, TOGETHER WITH DATA ANALYSIS, CAN ALSO HELP REVEAL WHEN THE EVIDENCE SUPPORTING A P0LICY IS BREAKING DOWN, FORCING A RE-THINK.
}

A model which aspires to provide quantitative guidance for assessing interventions should provide a quantitative statistical fit for available data.
However, strong statistical fit does not guarantee a correct causal structure: it does not even necessarily require the model to assert a causal explanation.
A causal interpretation is strengthened by corroborative evidence.
For example, reconstructed latent variables (such as numbers of susceptible and recovered individuals) should make sense in the context of alternative measurements of these variables;
parameter values which fit the data should make sense in the context of alternative lines of evidence about the phenomena being modeled.
% \jwc{Perhaps add comment on our final parameter estimates, and remove the following sentences in this paragraph.}
% A model is necessarily a simplified approximation of a complex process, and so it is unreasonable to expect models (describing and fitted to population-level processes) to perfectly match quantitative understanding of individual-level processes.
% Such discrepancies could lead to biases when interventions (modeled as effects on individuals) are included in the population-level, unless there are historical occurrences of the intervention which enable the consequence to be calibrated at the population level.

If a mechanistic model including a feature (such as a representation of a mechanism, or the inclusion of a covariate) fits better than mechanistic models without that feature, and also has competitive fit compared to associative models, this may be taken as evidence supporting the scientific relevance of the feature.
As for any analysis of observational data, we must be alert to the possibility of confounding.
For a covariate, this shows up in a similar way to regression analysis: the covariate under investigation could be a proxy for some other unmodeled or unmeasured covariate.
For a mechanism, the model feature could in principle explain the data by helping to account for some different unmodeled phenomenon.
% Assessing potential confounding is part of building an argument for a causal interpretation of a fitted model.
In the context of our analysis, the estimated trend in transmission rate could be explained by any trending variable (such as hygiene improvements, or changes in population behavior), resulting in confounding from colinear covariates.
Alternatively, the trend could be attributed to a decreasing reporting rate rather than decreasing transmission rate, resulting in confounded mechanisms.
The robust statistical conclusion is that a model which allows for change fits better than one which does not---we argue that a decreasing transmission rate is a plausible way to explain this, but the incidence data themselves do not provide enough information to pin down the mechanism.


\section{Discussion}\label{sec:discussion}

The ongoing global COVID-19 pandemic has provided a clear example on how government policy may be affected by the conclusions of scientific models.
This article demonstrates that fitting appropriate scientific models remains a challenging statistical task, and therefore great care is needed when fitting scientific models for policy recommendations.
We provided a few suggestions that may aid the fitting of mechanistic models such as comparing model likelihoods to a benchmark.
Improved model fits allows for meaningful statistical inference that may provide valuable insight on a dynamic system and may improve the accuracy of model-based projections.
% TODO: "Improved model fits", or "Improved model fitting", based on Anna's suggestion.
Caution is nonetheless needed when making policy based on modeling conclusions, as model misspecification may invalidate conclusions.
% TODO: Citation above?

Various suggestions been made about why \citet{lee20} failed to accurately predict the eventual eradication of cholera from Haiti, including model misspecification, overly difficult elimination criteria, and a potential conflict of interest \citep{rebaudetComment20,henrys20}.
Here, we instead argue that careful attention to important statistical details could have correctly resulted in the conclusion of imminent cholera elimination.
We acknowledge the benefit of hindsight: our demonstration of a statistically principled route to obtain better-fitting models with more accurate predictions does not rule out the possibility of discovering other models that fit well yet predict poorly.

We used the same data and models, and even much of the same code, as \citet{lee20}, and yet ended up with drastically different conclusions.
At a minimum, we have shown that the conclusions are sensitive to details in how the data analysis is carried out, and that attention to statistical fit (including numerical issues such as likelihood maximization) can lead to improved policy guidance.

We acknowledge there are limitations to this study;
one example was the inability to fit model parameters to the fully-coupled version of Model~3.
Promising theoretical and methodological developments \citep{ning21ibpf,ionides22} based on the Block Particle Filter \citep{rebeschini15} may potentially be used to obtain better parameter estimates for Model~3 in future work.

Inference for mechanistic time series models offers opportunities for understanding and controlling complex dynamic systems.
This case study has investigated issues requiring attention when applying powerful new statistical techniques that can enable statistically efficient inference for a general class of partially observed Markov process models.
Care must be taken to ensure that the computationally intensive numerical calculations are carried out adequately.
Once that is accomplished, care is required to assess what causal conclusions can properly be inferred given the possibility of alternative explanations consistent with the data.
Studies that combine model development with thoughtful data analysis, supported by a high standard of reproducibility, build knowledge about the system under investigation.
Cautionary warnings about the difficulties inherent in understanding complex systems \citep{saltelli20,ioannidis20} should motivate us to follow best practices in data analysis, rather than avoiding the challenge.

\subsection{Reproducibility and Extendability}

\citet{lee20} published their code and data online, and this reproducibility facilitated our work.
By design, the models were coded and analyzed independently, leading to differing implementation decisions.
Robust data analysis requires not only reproducibility but also extendability: if one wishes to try new model variations, or new approaches to fitting the existing models, or plotting the results in a different way, this should be not excessively burdensome.
Scientific results are only trustworthy so far as they can be critically questioned, and an extendable analysis should facilitate such examination \citep{gentleman07}.

We provide a strong form of reproducibility, as well as extendability, by developing our analysis in the context of an \code{R} package, \code{haitipkg}.
Using a software package mechanism supports documentation, standardization and portability that promote extendability.
In the terminology of \citet{gentleman07}, the source code for this article is a {\it dynamic document} combining code chunks with text.
In addition to reproducing the article, the code can be extended to examine alternative analysis to that presented.
The dynamic document, together with the R packages, form a {\it compendium}, defined by \citet{gentleman07} as a distributable and executable unit which combines data, text and auxiliary software (the latter meaning code written to run in a general-purpose, portable programming environment, which in this case is R).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support information, if any,             %%
%% should be provided in the                %%
%% Acknowledgements section.                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{acks}[Acknowledgments]
% The authors would like to thank ...
%\end{acks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{funding}
This work was supported by National Science Foundation grants DMS-1761603 and DMS-1646108.
%
% The second author was supported in part by ...
\end{funding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, including data   %%
%% sets and code, should be provided in     %%
%% {supplement} environment with title      %%
%% and short description. It cannot be      %%
%% available exclusively as external link.  %%
%% All Supplementary Material must be       %%
%% available to the reader on Project       %%
%% Euclid with the published article.       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Eliminating cholera in Haiti: Supplement}
\sdescription{
This document contains additional details for Models~1--3, as well as a translation table that facilitates comparisons between these models and those described in \cite{lee20}.
The supplement also demonstrates our capability to faithfully replicate the results
of \cite{lee20}.
}
\end{supplement}

\begin{supplement}
\stitle{\code{haitipkg}}
\sdescription{
This \code{R} package is contained in a public GitHub repository: zjiang2/haitipkg. The package contains all of the data and code used to create and fit the models, as well as other useful functions that were used in this article.
}
\end{supplement}

\begin{supplement}
\stitle{\code{jesseuwheeler/haiti}}
\sdescription{
This GitHub repository contains the \code{.Rnw} files that were used to create this document and the supplement material.
}
\end{supplement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  imsart-nameyear.bst  will be used to                   %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%  MR numbers will be added by VTeX.                      %%
%%                                                         %%
%%  Use \cite{...} to cite references in text.             %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% if your bibliography is in bibtex format, uncomment commands:
\bibliographystyle{imsart-nameyear} % Style BST file
\bibliography{bib-haiti}       % Bibliography file (usually '*.bib')

\end{document}

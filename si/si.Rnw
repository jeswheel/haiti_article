\documentclass[11pt]{article}

\pdfminorversion=4

\newcommand\secTitleSpace{\hspace{3mm}}

<<packages, include=FALSE>>=
library(tidyverse)
library(pomp)
library(haitipkg)
library(spatPomp)
library(doParallel)
library(doRNG)

cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset = NA))
if(is.na(cores)) cores <- detectCores()
# cores <- 20
registerDoParallel(cores)
theme_set(theme_bw())

if (!dir.exists("../model3/si")) {
  dir.create("../model3/si", recursive = TRUE)
}
@

<<set-opts, include=FALSE>>=
options(
  scipen = 2,
  help_type = "html",
  stringsAsFactors = FALSE,
  continue = "+  ",
  width = 70,
  useFancyQuotes = FALSE,
  reindent.spaces = 2,
  xtable.comment = FALSE
)
@

<<knitr-opts, include=FALSE, purl=FALSE>>=
library(knitr)
opts_knit$set(concordance=TRUE)
opts_chunk$set(
  progress=TRUE,prompt=TRUE,highlight=FALSE,
  tidy=TRUE,
  tidy.opts=list(
    keep.blank.line=FALSE
  ),
  comment="",
  warning=FALSE,
  message=FALSE,
  error=TRUE,
  echo=FALSE,
  cache=FALSE,
  strip.white=TRUE,
  results="markup",
  fig.path="figure/",
  fig.lp="fig:",
  fig.align="center",
  fig.show="asis",
  dev="pdf",
  dev.args=list(
    bg="transparent",
    pointsize=9
  )
)

myround <- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

@

%% copied from pnas-new.cls
\usepackage{amsmath,amsfonts,amssymb,graphicx}
\usepackage{pict2e}
\usepackage{curve2e}

%%%% parameters %%%%%%%%%%5
\newcommand\Wsat{W_{\mathrm{sat}}}
\newcommand\muIR{\mu_{IR}}
\newcommand\muEI{\mu_{EI}}
\newcommand\transmission{\beta}
\newcommand\seasAmplitude{a}
\newcommand\rainfallExponent{r}
\newcommand\muRS{\mu_{RS}}
\newcommand\vaccineEfficacy{\theta}
\newcommand\muBirth{\mu_S}
\newcommand\muDeath{\delta}
\newcommand\choleraDeath{\delta_{C}}
\newcommand\symptomFrac{f}
\newcommand\asymptomRelativeInfect{\epsilon}
\newcommand\asymptomRelativeShed{\epsilon_{W}}
\newcommand\Wbeta[1]{\beta_{W#1}}
\newcommand\Wremoval{\delta_W}
\newcommand\Wshed{\mu_W}
\newcommand\mixExponent{\nu}
\newcommand\sigmaProc{\sigma_{\mathrm{proc}}}
\newcommand\reportRate{\rho}
\newcommand\obsOverdispersion{\psi}
\newcommand\phaseParm{\phi}
\newcommand\transmissionTrend{\zeta}
\newcommand\Binit{\xi}
\newcommand\vaccClass{Z}
\newcommand\vaccCounter{z}
\newcommand\modelCounter{m}
\newcommand\Whur[1]{\beta_{W#1}^{hm}}
\newcommand\hHur[1]{h_{#1}^{hm}}
\newcommand\Iinit{I_{0,0}}
\newcommand\Einit{E_{0, 0}}
\newcommand\reportChange{\delta}
\newcommand\NBintercept{\alpha}
\newcommand\NBsize{\varphi}

\newcommand\missing{\hspace{6mm} ---}
\newcommand\fixed{\color{blue}}
\newcommand\demography{\bullet}

\newcommand\mytitle{Informing policy via dynamic models: Cholera in Haiti}
\newcommand\comma{{\hspace{-0.25mm},\hspace{-0.2mm}}}

%%%%%%%%%%%%%%%% START OF USER DEFINITIONS %%%%%%%%%%%%%%%5

\newcommand\siOnly[1]{} %% redefined for the supplement
\newcommand\msOnly[1]{#1} %% redefined for the supplement

\newcommand\code[1]{\texttt{#1}}
\usepackage{xurl}
% \usepackage[breaklinks]{hyperref}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
% \usepackage{setspace}

%% EDITING MACROS
\usepackage{color}
\usepackage[section]{placeins}
\usepackage[normalem]{ulem}% to use \sout in feedback commands
\usepackage{bm}
% orange for EI
\definecolor{orange}{rgb}{1,0.5,0}
\newcommand\ei[2]{\sout{#1} \textcolor{orange}{#2}}
\newcommand\eic[1]{\textcolor{orange}{[#1]}}
% green for JW
\definecolor{green}{rgb}{0,0.5,0}
\newcommand\jw[2]{\sout{#1} \textcolor{green}{#2}}
\newcommand\jwc[1]{\textcolor{green}{[#1]}}
% purple for JJ
\definecolor{purple}{rgb}{0.5,0,1}
\newcommand\jj[2]{\sout{#1} \textcolor{purple}{#2}}
\newcommand\jjc[1]{\textcolor{purple}{[#1]}}
% cyan for AR
\definecolor{cyan}{rgb}{0,.5,.5}
\newcommand\ar[2]{\sout{#1} \textcolor{cyan}{#2}}
% \newcommand\arc[1]{\textcolor{cyan}{#1}}
% light brown for KT
\definecolor{lightbrown}{rgb}{0.5,0.5,0}
\newcommand\kt[2]{\sout{#1} \textcolor{lightbrown}{#2}}
\newcommand\ktc[1]{\textcolor{lightbrown}{#1}}

%% basic POMP definitions
\newcommand\Xspace{{\mathbb X}}
\newcommand\Yspace{{\mathbb Y}}
\newcommand\Thetaspace{\R^{\Thetadim}}
\newcommand\Xunitspace{{\scriptsize{\mathcal X}}}
\newcommand\Yunitspace{{\tiny{\mathcal Y}}}
\newcommand\hatTheta{\widehat{\Theta}}
\newcommand\Xdim{{\mathrm{dim}}(\Xspace)}
\newcommand\Ydim{{\mathrm{dim}}(\Yspace)}
\newcommand\Thetadim{P}
\newcommand\thetadim{p}
\newcommand\timeSet{{\mathbb T}}
\newcommand\data[1]{#1^*}

\newcommand\unitSpecific{\hspace{0.1mm}\mathrm{us}}
\newcommand\shared{\hspace{0.15mm}\mathrm{sh}}

% for algorithm pseudocode
\SetKwFor{For}{for}{:}{end}

%% for particle filters
\newcommand\unit{u}
\newcommand\altUnit{\tilde u}
\newcommand\Unit{U}
\newcommand\rep{i}
\newcommand\Rep{\mathcal{I}}
\newcommand\island{\rep}
\newcommand\Island{\Rep}
%\newcommand\txtisland{replicate}
\newcommand\altIsland{\tilde i}
\usepackage[mathscr]{euscript}

\renewcommand\time{n}
\newcommand\myvec[1]{\boldsymbol{#1}}
\newcommand\altTime{\tilde n}
\newcommand\Time{N}
\newcommand\Np{J}
\newcommand\np{j}
\newcommand\altNp{k}
\newcommand\altAltNp{\tilde j}
\newcommand\resampleIndex{r}
\newcommand\unitWeight{w}

%% for all iterated filtering algorithms
\newcommand\Nit{M}
\newcommand\nit{m}

%% customized math macros
\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\dd[1]{\mathrm{d}{#1}}
\newcommand\given{{\,\vert\,}}
\newcommand\equals{{{\,}={\,}}}
\newcommand\myequals{\hspace{0.5mm}{=}\hspace{0.5mm}}
\newcommand\myto{{\;:\;}}
\newcommand\seq[2]{{#1}\!:\!{#2}}
\newcommand\mydot{{\,\cdot\,}}
\newcommand\cp[2]{N_{\mathrm{#1}\mathrm{#2}}}
\newcommand\giventh{{\hspace{0.5mm};\hspace{0.5mm}}}
\newcommand\normal{\mathcal{N}}
\newcommand\argequals{{\,=\,}}
\newcommand\lags{c}
\newcommand\maxlag{\overline{c}}
\newcommand\nlfList{C}
\newcommand\bigO{\mathcal{O}}
\newcommand\loglik{\lambda}
\newcommand\loglikMC{\MC{\loglik}}
\newcommand\R{\mathbb{R}}
\newcommand\param{\,;}
\newcommand\mycolon{{\hspace{0.6mm}:\hspace{0.6mm}}}
\newcommand\MC[1]{#1^{\,\mbox{\tiny MC}}}
\newcommand\EMC{{\E}}
\newcommand\Var{\mathrm{Var}}
\newcommand\var{\Var}
\newcommand\Cov{\mathrm{Cov}}
\newcommand\cov{\Cov}
\newcommand\iid{\mathrm{iid}}
\newcommand\dist{d}
\newcommand\transpose{\top}

\newcommand\gnsep{,}

\def\lik{L}
\def\loglik{\ell}

%% THEOREM-LIKE ENVIRONMENTS
\usepackage{amsthm}
\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{cor}{Corrolary}
\newtheorem{fact}{Fact}
\newtheorem{assumption}{Assumption}
\newtheorem{assumptionB}{Assumption}
\renewcommand\theassumption{A\arabic{assumption}}
\renewcommand\theassumptionB{B\arabic{assumptionB}}

\makeatletter
\AtBeginDocument{%
  \expandafter\renewcommand\expandafter\subsection\expandafter{%
    \expandafter\@fb@secFB\subsection
  }%
}
\makeatother

%% FOR PSEUDOCODE SETUP
\newcommand\mystretch{\rule[-2mm]{0mm}{5mm} }
\newcommand\asp{\hspace{4mm}}
\newcommand\codeIndent{\hspace{4mm}}

%%%%%%%%%%%%%%%% END OF USER DEFINITIONSS %%%%%%%%%%%%%%%

\bibliographystyle{apalike}
\usepackage{natbib}
\usepackage{fullpage}
% \usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
% \usepackage[labelfont=bf,labelsep=period]{caption} % For some reason this doesn't work with .Rnw file, but it does with just the tex file, so after compiling you can edit this so that it works.
\renewcommand{\figurename}{Fig}

\newcommand\myeqref[1]{(\ref{#1})}

\renewcommand{\contentsname}{Supplementary Content}
\renewcommand{\refname}{Supplementary References}
\renewcommand\thefigure{S-\arabic{figure}}
\renewcommand\thetable{S-\arabic{table}}
\renewcommand\thesection{S\arabic{section}}
\renewcommand\theequation{S\arabic{equation}}
\renewcommand\theprop{S\arabic{prop}}
\renewcommand\thelemma{S\arabic{lemma}}
\renewcommand\thealgocf{S\arabic{algocf}}

\setcounter{tocdepth}{1}

\begin{document}

\date{\today}
\title{Supplement to ``{\mytitle}''}
  \author{Jesse Wheeler, AnnaElaine L. Rosengart, Zhuoxun Jiang, \\ Kevin Tan, Noah Treutle and Edward L. Ionides
 \\ \hspace{.2cm}\\
    Department of Statistics, University of Michigan\\
}

\newcommand{\blind}{1}

\if1\blind
{
\maketitle
}
\fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf \mytitle}
\end{center}
  \bigskip
  \bigskip
}\fi

\tableofcontents

\newpage

\section{Model Diagrams}

Each of the dynamic models considered in this manuscript can be fully described using the model descriptions in the manuscript, coupled with the additional information described in Sections 2 and 3 of this supplement.
Despite this, diagrams of dynamic systems are often helpful to understand the equations.
In this section, we give three diagrams representing Models~1--3, respectively.
Because the models are defined by their mathematical equations and numeric implementation, these diagrams are not unique visual representations of the model.
Alternative representations that may be helpful in understanding the models explored in this paper were made by \citet{lee20sup}.

\subsection{Model~1}

\begin{figure}[!h]
%%%%% SEAIR diagram
\begin{center}
  \resizebox{0.9\textwidth}{!}{
    \Large
    \setlength{\unitlength}{5pt}
    \begin{picture}(100,95)(0,15)
      \thicklines

      % Unvaccinated Compartments
      \put(10,63){\framebox(6,6){$\mathrm{S}_0$}}
      \put(34,63){\framebox(6,6){$\mathrm{E}_0$}}
      \put(58,71){\framebox(6,6){$\mathrm{A}_0$}}
      \put(82,63){\framebox(6,6){$\mathrm{R}_0$}}
      \put(58,55){\framebox(6,6){$\mathrm{I}_0$}}

      % Births
      \put(1, 66){\vector(1, 0){8}}
      \put(4, 68){$\muBirth$}

      % S_0 -> E_0
      \put(17,66){\vector(1,0){16}}
      \put(22,67){$\lambda(t)$}

      % E_0 -> A_0
      \put(41,67.5){\vector(2.1,1){16}}
      % \put(37, 72){$\muEI\big(1-\symptomFrac_{\vaccCounter}(t)\big)$}

      % E_0 -> I_0
      \put(41,65.5){\vector(2.1,-1){16}}

      % A_0 -> R_0
      \put(65,74){\vector(2.1,-1){16}}

      % I_0 -> R_0
      \put(65,59){\vector(2.4,1){16}}

      % R_0 -> S_0
      \cbezier(84,70)(75, 95)(23,95)(14,70)
      \put(14,70){\vector(-1, -2){0.3}}

      % Unvaccinated death rates
      \put(13, 70){\vector(0, 1){10}}
      \put(11, 74){$\muDeath$}

      \put(37, 70){\vector(0, 1){10}}
      \put(35, 74){$\muDeath$}

      \put(61, 78){\vector(0, 1){9}}
      \put(59, 80){$\muDeath$}

      \put(61, 62){\vector(1.5, 2){6}}
      \put(61.5, 65.5){$\muDeath$}

      \put(85, 70){\vector(0, 1){10}}
      \put(83, 74){$\muDeath$}

      % Vaccinated Compartments
      \put(10,37){\framebox(6,6){$\mathrm{S}_\vaccCounter$}}
      \put(34,37){\framebox(6,6){$\mathrm{E}_\vaccCounter$}}
      \put(58,45){\framebox(6,6){$\mathrm{A}_\vaccCounter$}}
      \put(82,37){\framebox(6,6){$\mathrm{R}_\vaccCounter$}}
      \put(58,29){\framebox(6,6){$\mathrm{I}_\vaccCounter$}}

      % S -> E
      \put(17,40){\vector(1,0){16}}
      \put(22,41){$\lambda(t)$}

      % E -> A
      \put(41,40.5){\vector(2.1,1){16}}

      % E -> I
      \put(41,39.5){\vector(2.1,-1){16}}

      % A -> R
      \put(65,48){\vector(2.1,-1){16}}

      % I -> R
      \put(65,32){\vector(2.1,1){16}}

      % R -> S
      \cbezier(84, 36)(75, 10)(23, 10)(14, 36)
      \put(14,36){\vector(-1, 2){0.3}}

      % Vaccinated death rates
      \put(13, 36){\vector(0, -1){10}}
      \put(11, 31){$\muDeath$}

      \put(37, 36){\vector(0, -1){10}}
      \put(35, 31){$\muDeath$}

      \put(61, 28.5){\vector(0, -1){9}}
      \put(59, 24){$\muDeath$}

      \put(61, 44){\vector(1.5, -2){6}}
      \put(62, 38){$\muDeath$}

      \put(85, 36){\vector(0, -1){10}}
      \put(83.2, 30.2){$\muDeath$}

      % S_0 -> S_\vaccCounter
      \multiput(13,62)(0,-2){9}{\line(0,-1){1}}
      \put(13, 43.5){\vector(0, -1){0}}

      % E_0 -> E_\vaccCounter
      \multiput(37,62)(0,-2){9}{\line(0,-1){1}}
      \put(37, 43.5){\vector(0, -1){0}}

      % R_0 -> R_\vaccCounter
      \multiput(85,62)(0,-2){9}{\line(0,-1){1}}
      \put(85, 43.5){\vector(0, -1){0}}

      % A_0 -> A_\vaccCounter
      \multiput(57.5,72)(-1.9, 0){3}{\line(-1, 0){0.9}}
      \put(52.2, 72){\line(-2.1, -1){1}}
      \multiput(51,71)(0, -2){11}{\line(0, -1){1}}
      \put(52.2, 49){\line(-2.1, 1){1}}
      \multiput(54,49)(1.9, 0){2}{\line(-1, 0){0.9}}
      \put(57, 49){\vector(1, 0){0}}

      % I_0 -> I_\vaccCounter
      \multiput(65, 57.5)(1.9, 0){3}{\line(1, 0){0.9}}
      \put(70.6, 57.5){\line(2.1, -1){1}}
      \multiput(72, 56.5)(0, -2){13}{\line(0, -1){1}}
      \put(72, 30.9){\line(-2, -1.2){0.9}}
      \multiput(65.6, 30.2)(1.9, 0){3}{\line(1, 0){0.9}}
      \put(64.5, 30.2){\vector(-1, 0){0}}

    \end{picture}
  }
\end{center}
\caption{A flow diagram for the SEAIR model.}\label{fig:flow_diagram}
\end{figure}

\subsection{Model~2}

\begin{figure}[!h]
%%%%% SEAIR diagram
\begin{center}
  \resizebox{\textwidth}{!}{
    \Large
    \setlength{\unitlength}{5pt}
    \begin{picture}(100,95)(0,15)
      \thicklines

      % Unvaccinated Compartments
      \put(10,63){\framebox(6,6){$\mathrm{S}_{u0}$}}
      \put(34,69){\framebox(6,6){$\mathrm{E}_{u0}$}}
      \put(58,77){\framebox(6,6){$\mathrm{A}_{u0}$}}
      \put(82,77){\framebox(6,6){$\mathrm{R^A}_{u0}$}}
      \put(82,61){\framebox(6,6){$\mathrm{R^I}_{u0}$}}
      \put(58,61){\framebox(6,6){$\mathrm{I}_{u0}$}}

      % % S_0 -> E_0
      \put(17,66){\vector(3,1.1){16}}
      % \put(22,67){$\lambda(t)$}

      % % E_0 -> A_0
      \put(41,72.5){\vector(2.1,1){16}}
      % % \put(37, 72){$\muEI\big(1-\symptomFrac_{\vaccCounter}(t)\big)$}

      % % E_0 -> I_0
      \put(41,71.5){\vector(2.1,-1){16}}

      % % A_0 -> R_0
      \put(65,80.5){\vector(1,0){16}}

      % A_0 -> W_u
      % \multiput(65, 79.8)(1, -1){8}{\line(1, -1){0.25}}

      % I_0 -> W_u
      % \multiput(65, 64.2)(1, 1){8}{\line(1, 1){0.25}}

      % Long verticle dotted line
      % \multiput(72.9, 72)(0, -1.41){13}{\line(0, -1){0.25}}

      % Long backwards horizontal dotted line
      % \multiput(73, 54)(-1.41, 0){22}{\line(0, -1){0.25}}
      % \multiput(73, 34)(0, 1.41){15}{\line(0, 1){0.25}}
      % \put(42, 53.9){\vector(-1, 0){0.2}}

      % A_uz -> W_u
      \put(58,39){\framebox(6,6){$\mathrm{A}_{u\vaccCounter}$}}
      \qbezier[25](60.5, 46)(51, 52)(42, 53)
      \put(42, 53){\vector(-1, 0.1){0.5}}

      % A_uz -> W_u
      \qbezier[25](60.5, 60)(51, 54)(42, 54)
      \put(42, 54){\vector(-1, -0.1){0.5}}

      % I_uz -> W_u
      \qbezier[30](60.5,30)(48, 45)(40, 50)
      \put(40, 50){\vector(-1, 0.8){0.5}}

      % A_u0 -> W_u
      \qbezier[29](60.5,76)(48, 61)(40.6, 57)
      \put(40.8, 57.1){\vector(-1, -0.7){0.5}}


      % A + I_\vaccCounter -> W_u
      % \multiput(65, 41.8)(1, -1){8}{\line(1, -1){0.25}}
      % \multiput(65, 26.2)(1, 1){8}{\line(1, 1){0.25}}


      % W_u -> backwards diags.
      % \multiput(33, 57)(-1, 1.2){10}{\line(-1, 1.2){0.1}}
      % \put(24.5, 67){\vector(-1, 1.2){0.5}}
      % \multiput(33, 51)(-1, -1.2){10}{\line(-1, -1.2){0.1}}
      % \put(24, 40){\vector(-1, -1.2){0.5}}
      %\put(65, 80){\vector(2, -1){8}}

      % % I_0 -> R_0
      \put(65,63.5){\vector(1,0){16}}

      % % Vaccinated Compartments
      \put(10,37){\framebox(6,6){$\mathrm{S}_{u\vaccCounter}$}}
      \put(34,31){\framebox(6,6){$\mathrm{E}_{u\vaccCounter}$}}
      \put(58,39){\framebox(6,6){$\mathrm{A}_{u\vaccCounter}$}}
      \put(82,39){\framebox(6,6){$\mathrm{R^A}_{u\vaccCounter}$}}
      \put(58,23){\framebox(6,6){$\mathrm{I}_{u\vaccCounter}$}}
      \put(82,23){\framebox(6,6){$\mathrm{R^I}_{u\vaccCounter}$}}

      % R_0 -> S_0
      \cbezier(85,22)(80, 8)(18,8)(12,36)
      \put(12.1,35.5){\vector(-1, 3.5){0.3}}

      \cbezier(85,38)(80, 12)(18,15)(14,36)
      \put(14.1,35.5){\vector(-1, 3.5){0.3}}


      % R_0 -> S_0
      \cbezier(85,84)(80, 100)(18,100)(12,70)
      \put(12.1,70.5){\vector(-1, -3.5){0.3}}

      \cbezier(85,68)(80, 95)(18, 92)(14,70)
      \put(14.1,70.5){\vector(-1, -3.5){0.3}}

      % % S -> E
      \put(17,40){\vector(3,-1.1){16}}
      % \put(22,41){$\lambda(t)$}

      % % E -> A
      \put(41,34.5){\vector(2.1,1){16}}

      % % E -> I
      \put(41,33.5){\vector(2.1,-1){16}}

      % % A -> R
      \put(65,42.5){\vector(1,0){16}}

      % % I -> R
      \put(65,25.5){\vector(1,0){16}}

      % S_0 -> S_\vaccCounter
      \multiput(12,62)(0,-2){9}{\line(0,-1){1}}
      \put(12, 44){\vector(0, -1){0}}

      % S_0 -> S_\vaccCounter
      % Wanning vaccine efficacy
      \multiput(14,44)(0,2){9}{\line(0,1){1}}
      \put(14, 62){\vector(0, 1){0}}


      \put(37, 54){\circle{7}}
      \put(35, 53){$\mathrm{W_u}$}

      \qbezier[10](32.5, 54)(29.5, 54)(26.5, 54)
      \put(26, 54){\vector(-1, 0){0.1}}
      \put(28, 55){$\Wremoval$}

    \end{picture}
  }
\end{center}
\caption{A flow diagram for the SEAIR model 2. This is a constant population model, there are no births/deaths. Vaccinations are assumed to only be given to susceptible individuals, and vaccine immunity wanes only with susceptible vaccinated individuals.}\label{fig:flow_diagram2}
\end{figure}

\newpage

\subsection{Model~3}

\begin{figure}[!h]
%%%%% SEAIR diagram
\begin{center}
  \resizebox{\textwidth}{!}{
    \Large
    \setlength{\unitlength}{5pt}
    \begin{picture}(100,85)(0,15)

    % COMPARTMENTS
    % \put(36, 47){\framebox(6, 6){$\mathrm{W}_{u}$}}

    \put(39, 50){\circle{6}}
    \put(37, 49){$\mathrm{W}_{u}$}

    \put(8, 55){\framebox(6, 6){$\mathrm{S_{u0}}$}}
    \put(8, 39){\framebox(6, 6){$\mathrm{S_{u\vaccCounter}}$}}
    % \put(22, 45){\circle{2}}
    % \put(22, 55){\circle{2}}
    \put(36, 62.5){\framebox(6, 6){$\mathrm{A_{u}}$}}
    \put(36, 31.5){\framebox(6, 6){$\mathrm{I_{u}}$}}
    \put(56, 55){\framebox(6, 6){$\mathrm{R^1_{u0}}$}}
    \put(71, 55){\framebox(6, 6){$\mathrm{R^2_{u0}}$}}
    \put(86, 55){\framebox(6, 6){$\mathrm{R^3_{u0}}$}}
    \put(56, 39){\framebox(6, 6){$\mathrm{R^1_{u\vaccCounter}}$}}
    \put(71, 39){\framebox(6, 6){$\mathrm{R^2_{u\vaccCounter}}$}}
    \put(86, 39){\framebox(6, 6){$\mathrm{R^3_{u\vaccCounter}}$}}
    % \dottedline{3}(0,5)(70,5)

    % INDIVIDUAL MOVEMENT
    % \put(15, 58){\line(2.4, -1){5.5}}
    % \put(15, 42){\line(2.4, 1){5.5}}

    % \put(23, 46.3){\vector(1, 1.45){12}}
    % \put(23, 53.7){\vector(1, -1.45){12}}

    % \put(23.5, 55.7){\vector(1.1, 1){11}}
    % \put(23.5, 44.3){\vector(1.1, -1){11}}

    % Backward Diags, W_u
    % \multiput(35, 51)(-1, .3){12}{\line(-1, .3){0.2}}
    % \put(24, 54.4){\vector(-1, 0.3){0.5}}
    % \multiput(35, 49)(-1, -0.3){12}{\line(-1, -0.3){0.15}}
    % \put(24, 45.6){\vector(-1, -0.3){0.5}}

    % S_u0 -> A_u
    \put(15, 58.5){\vector(3, 1.2){20}}

    % S_u0 -> I_u
    \put(15, 57.5){\vector(1, -1.1){20}}

    % S_uz -> I_u
    \put(15, 41.5){\vector(3, -1.2){20}}

    % S_uz -> A_u
    \put(15, 42.5){\vector(1, 1.1){20}}

    % I_u -> R_u0
    \put(43, 66){\vector(2, -1){12}}

    % A_u -> R_uz
    \multiput(43, 64)(1.25, -1.6){12}{\line(1, -1.3){0.7}}
    \put(57, 46){\vector(1, -1.3){0.2}}

    \put(43, 36){\vector(1.7, 2){15}}

    \multiput(60, 54)(0, -2){4}{\line(0, -1){1}}
    \put(60, 46){\vector(0, -1){0.5}}

    \multiput(74, 54)(0, -2){4}{\line(0, -1){1}}
    \put(74, 46){\vector(0, -1){0.5}}

    \multiput(89, 54)(0, -2){4}{\line(0, -1){1}}
    \put(89, 46){\vector(0, -1){0.5}}

    \put(63, 58){\vector(1, 0){7}}
    \put(78, 58){\vector(1, 0){7}}

    \put(63, 42){\vector(1, 0){7}}
    \put(78, 42){\vector(1, 0){7}}

    \multiput(39, 61.5)(0, -1){8}{\line(0, -1){0.3}}
    \put(39,54){\vector(0, -1){0.5}}

    \multiput(39, 38.5)(0, 1){8}{\line(0, 1){0.3}}
    \put(39,46){\vector(0, 1){0.5}}

    \cbezier(89,62)(80, 85)(20, 85)(12,62)
    \put(12.2,62.5){\vector(-1, -3.5){0.3}}

    \cbezier(89,38)(80, 15)(20, 15)(12,38)
    \put(12.2,37.5){\vector(-1, 3.5){0.3}}

    \multiput(11, 54)(0, -2){4}{\line(0, -1){1}}
    \put(11, 46){\vector(0, -1){0.5}}

    % DEMOGRAPHY
    % -> S
    \put(0, 58){\vector(1, 0){7}}
    \put(1, 59){$\mathrm{\mu_{\demography S_{u0}}}$}

    % S Death
    \put(10, 62){\vector(0, 1){7}}
    \put(8, 64){$\mathrm{\muDeath}$}
    \put(10, 38){\vector(0, -1){7}}
    \put(8, 34){$\mathrm{\muDeath}$}

    % A + I death
    \put(39, 69.5){\vector(0, 1){7}}
    \put(37, 71.5){$\mathrm{\muDeath}$}

    \put(39, 31.5){\vector(0, -1){7}}
    \put(30, 27.5){$\mathrm{\muDeath + \choleraDeath}$}

    % R Death
    \put(59, 62){\vector(0, 1){7}}
    \put(57, 64){$\mathrm{\muDeath}$}

    \put(74, 62){\vector(0, 1){7}}
    \put(72, 64){$\mathrm{\muDeath}$}

    \put(59, 38){\vector(0, -1){7}}
    \put(57, 34){$\mathrm{\muDeath}$}

    \put(74, 38){\vector(0, -1){7}}
    \put(72, 34){$\mathrm{\muDeath}$}

    \put(93, 42){\vector(1, 0){7}}
    \put(95.5, 43){$\mathrm{\muDeath}$}

    \put(93, 58){\vector(1, 0){7}}
    \put(95.5, 59){$\mathrm{\muDeath}$}

    % W death
    % \put(43, 50){\vector(1, 0){7}}
    \multiput(43, 50)(1, 0){7}{\line(1, 0){0.2}}
    \put(50.2, 50){\vector(1, 0){0.5}}
    \put(44, 51){$\mathrm{\Wremoval}$}
    % \qbezier[12](10, 20)(20, 30)(25, 20)

    \end{picture}
  }
\end{center}
\caption{A flow diagram for the SAIR model 3. This model assumes a constant population while also including a mechanism for births/deaths; all deaths are balanced by births into the unvaccinated susceptible compartment, so the birth rate $\mu_{\demography S_{u0}}$ corresponds to the sum total deaths from the remaining compartments. The model assumes that symptomatic individuals will not be vaccinated, hence no vaccination arrow exiting the $I_u0$ compartment.}\label{fig:flow_diagram3}
\end{figure}

\section{Markov chain and differential equation interpretations of compartment flow rates}

In Sections 2.1, 2.2 and 2.3 of the main article, we define compartment models in terms of their flow rates.
For a discrete population model, these rates define a Markov chain.
For a continuous and deterministic model, the rates define a system of ordinary differential equations.
Here, we add additional details to clarify the mapping from a collection of rate functions to a fully specified process.
Our treatment follows \citet{breto09}.

A general compartment model is a vector-valued process $X(t)=(X_1(t),\dots,X_c(t))$ denoting the (integer or real-valued) counts in each of $c$ compartments, where $t$ is any continuous value in the interval  $\left[t_0, \infty\right)$ for some real valued starting time $t_0$.
The compartments may also have names, but to set up general notation we simply refer to them by their numerical index.
The basic characteristic of a compartment model is that $X(t)$ can be written in terms of the flows $N_{ij}(t)$ from $i$ to $j$.
A flow into compartment $i$ from outside the system is denoted by $N_{\demography i}$, and a flow out of the system from compartment $i$ is denoted by $N_{i\demography}$.
We call $\demography$ a source/sink compartment, though it is an irregular compartment since $X_{\demography}(t)$ is not defined.
These flows are required to satisfy a ``conservation of mass'' identity:
\begin{equation}
X_i(t)=X_i(t_0)+N_{\demography i}(t) - N_{i\demography}(t) + \sum_{j\neq
i}N_{ji}(t)-\sum_{j\neq i}N_{ij}(t). \label{eq:conservation}
\end{equation}
Each {\em flow} $N_{ij}(t)$ is associated with a {\em rate} function
$\mu_{ij}=\mu_{ij}(t,X(t))$, where we include the possibility that $i$ or $j$ takes value $\demography$.

There are different ways to use a collection of rate functions to build a fully specified model.
We proceed to describe the ones we use in this paper: via a system of ordinary differential equations (Sec.~\ref{subsec:ode}), a simple Markov counting system (Sec.~\ref{subsec:smcs}), and an overdispersed Markov counting system (Sec.~\ref{subsec:odmcs}). Other representations include stochastic differential equations driven by Gaussian noise or Gamma noise \citep{bhadra11}.

\subsection{Ordinary differential equation (ODE) interpretation}
\label{subsec:ode}

A basic deterministic specification is
\begin{equation}
\label{eq:ode1}
dN_{ij}/dt = \mu_{ij}\big(t,X(t) \big) X_i(t), \hspace{3mm} i\in \seq{1}{c}, \hspace{3mm} j\in \seq{1}{c} \cup\{\demography\}, \hspace{3mm} i\neq j,
\end{equation}
where $\mu_{ij}\big(t,X(t)\big)$ is called a per-capita rate or a unit rate.
Flows into the system require special treatment since $X_i(t)$ in \myeqref{eq:ode1} is not defined for $i=\demography$.
Instead, we specify
\begin{equation}
\label{eq:ode2}
dN_{\demography i}/dt = \mu_{\demography i}\big(t,X(t) \big).
\end{equation}
This is the the interpretation and implementation used for Model~2 in our study.

\subsection{Simple Markov counting system interpretation}
\label{subsec:smcs}
A continuous time Markov chain can be specified via its infinitesimal transition probabilities.
A basic approach to this is to define
\begin{eqnarray}
\label{eq:smcs1}
\prob\big[ N_{ij}(t+\delta)-N_{ij}(t)=0 \given X(t)\big]
 &=& 1-\delta \mu_{ij}\big(t,X(t)\big)X_i(t) + o(\delta),
\\
\label{eq:smcs2}
\prob\big[ N_{ij}(t+\delta)-N_{ij}(t)=1 \given X(t)\big]
 &=& \delta \mu_{ij}\big(t,X(t)\big)X_i(t) + o(\delta),
\end{eqnarray}
for $i\in \seq{1}{c}$ and $j\in\seq{1}{c}\cup\{\demography\}$ with $i\neq j$.
As with the ODE case, we need special attention for flows into the system, and we define
\begin{eqnarray}
\label{eq:smcs3}
\prob\big[ N_{\demography i}(t+\delta)-N_{\demography i}(t)=0 \given X(t)\big]
 &=& 1-\delta \mu_{\demography i}\big(t,X(t)\big) + o(\delta),
\\
\label{eq:smcs4}
\prob\big[ N_{\demography i}(t+\delta)-N_{\demography i}(t)=1 \given X(t)\big]
 &=& \delta \mu_{\demography i}\big(t,X(t)\big) + o(\delta).
\end{eqnarray}
Together with the initial conditions $X(0)$, equations \myeqref{eq:smcs1}--\myeqref{eq:smcs4} define a Markov chain.
Each flow is a simple counting process, meaning a non-decreasing integer-valued process that only has jumps of size one.
We therefore call the Markov chain a simple Markov counting system (SMCS).
The infinitesimal mean of every flow is equal to its infinitesimal variance \citep{breto11} and so an SMCS is called equidispersed.
We note that the special case of Model~1 used by \cite{lee20} (with $\sigmaProc = 0$) is an SMCS.
To permit more general mean-variance relationships for a Markov counting system, we must permit jumps of size greater than one.
The utility of overdispersed models, where the infinitesimal variance of the flow exceeds the infinitesimal mean, has become widely recognized \citep{stocks20,he10}.

\subsection{Overdispersed Markov counting system interpretation}
\label{subsec:odmcs}

Including white noise in the rate function enables the possibility of an overdispersed Markov counting system \citep{breto11,breto09,he10}.
Since rates should be non-negative, Gaussian noise is not appropriate and gamma noise is a convenient option that has found various applications \citep{romero-severson15, subramanian20}.
Specifically, we consider a model given by
\begin{equation}
\label{eq:odmcs1}
\mu_{ij}\big(t,X(t)\big) = \bar\mu_{ij}\big(t,X(t)\big) \, d\Gamma_{ij}(t)/dt,
\end{equation}
where $\Gamma_{ij}(t)$ is a stochastic process having independent gamma distributed increments, with
\begin{equation}
\label{eq:odmcs2}
\E\big[\Gamma_{ij}(t)\big] = t, \quad \var\big[\Gamma_{ij}(t)\big] = \sigma_{ij}^2 t.
\end{equation}
Formally interpreting the meaning of \myeqref{eq:odmcs1} is not trivial, and we do so by constructing a Markov process $X(t)$ as the limit of the Euler scheme descrived in Section~\ref{sec:numerics}, below.
Therefore, the numerical scheme in Sec.~\ref{sec:numerics} can be taken as a definition of the meaning of \myeqref{eq:odmcs1}.
The Markov chain defined by the limit of this Euler scheme as the step size decreases is an overdispersed Markov counting system, with the possibility of instantaneous jumps of size greater than one \citep{breto11}.

\section{Numerical solutions to compartment models}
\label{sec:numerics}

Models may be fitted and their implications assessed via numerical solutions (i.e., simulations) from the model equations.
All the analyses we consider have this simulation-based property, known as plug-and-play or equation-free or likelihood-free.
The numerical solutions to the model are arguably of more direct scientific interest than the exact solutions to the postulated equations.
For ODE models, numerical methods are well studied and a standard numerical solution package such as \code{deSolve} in \code{R} is adequate for many purposes.
For SMCS and ODMCS models, exact schemes are feasible when the number of events is small, which may be the case for small populations.
However, for applicability to larger populations, we use instead the following Euler scheme.
Write $\delta$ for an Euler time step, and $\Delta N_{ij}$ for the numerical approximation to $N_{ij}(t+\delta)-N_{ij}(t)$ given $X(t)$.
For each $i$ and $j$ in $\seq{1}{c} \cup \{\demography\}$ with $i \neq j$, we draw independent Gamma distributed noise increments with mean $\delta$ and variance $\sigma_{ij}^2 \delta$, denoted using a mean-variance parameterization of the gamma distribution as
\begin{equation}
\label{eq:numerics1}
\Delta\Gamma_{ij} \sim \mathrm{gamma}(\delta, \sigma_{ij}^2 \delta).
\end{equation}
In the case of an SMCS model, $\sigma_{ij}=0$ for all $i$ and $j$, so we have $\Delta\Gamma_{ij}=\delta$.
Then, for $i\neq \demography$ and $j\neq i$, and writing
\begin{equation}
\label{eq:numerics2}
\mu_{ij}=\bar\mu_{ij}\big(t,X(t)\big) \Delta\Gamma_{ij} / \delta,
\end{equation}
we calculate transition probabilities
\begin{eqnarray}
\label{eq:numerics3}
p_{ij} &=& \exp\left\{-\sum_{k\in 1:c \, \cup \{\demography\}} \mu_{ik} \, \delta \right\}
\frac{\mu_{ij}}{\sum_{k\in 1:c\cup \{\demography\}} \mu_{ik}},
\\
\label{eq:numerics4}
p_{ii} &=& 1 - \sum_{j\neq i} p_{ij}.
\end{eqnarray}
These probabilities correspond to competing hazards for every individual in compartment $i$ to transition to some compartment $j$, interpreting $j=i$ to mean that the individual remains in $i$.
Then, $\big(\Delta N_{i1},\dots,\Delta N_{ic},\Delta N_{i\demography}\big)$ has the multinomial distribution where $X_i(t)$ individuals are allocated independently to $\seq{1}{c}\cup\{\demography\}$ with probabilities given by \eqref{eq:numerics3} and \eqref{eq:numerics4}.
We use the \code{reulermultinom} function in the \code{pomp} package to draw from this multionomial distribution.

Different treatments of demographic flows---such as birth, death, immigration and emigration---are possible.
For the case $i=\demography$, the treatment used by Model~1 is to set
\begin{equation}
\label{eq:numerics6}
\Delta N_{\demography j} \sim \mathrm{poisson}( \mu_{\demography j} \delta),
\end{equation}
an independent Poisson random variable with mean $\mu_{\demography j} \delta$.

Models~2 and 3 used an alternative approach, balancing the total number of flows in and out of the compartment, i.e., $\sum_{i}N_{\demography i}(t) = \sum_{i}N_{i \demography}(t)$, in order to make the model consistent with the known total population.
In this case, we formally model the death rate as a rate of returning to the susceptible class $S$, and use external transitions from $\demography$ into $S$ to describe only net population increase.

\section{Measurement Models}

Each POMP requires specification of a measurement model, which is a statistical description of how observations on the system are obtained.
In general, we used the same measurement models that were reported by \cite{lee20} unless specifically noted in the following subsections.


\subsection{Model~1}

In this model, the advantage afforded by vaccination is an increased probability that an infection is asymptomatic.
Therefore, under the assumptions of this model, all reported cases are assumed to be a fraction of individuals that transition from the exposed to the infected compartment, as noted in Eq.~\myeqref{model1:meas}:
\begin{equation}
  \label{model1:meas}
  Y_{n} \mid \Delta N_{E_{\cdot}I_{\cdot}}(n) = \reportChange \sim \text{NB}\left(\reportRate \reportChange, \obsOverdispersion \right),
\end{equation}
where $Y_n$ is the reported cholera cases at time $t_n \in t_1:t_N$ and $\Delta N_{E_{\cdot}I_{\cdot}}(n)$ is the sum total of individuals across each vaccination compartment $\vaccCounter \in 1:\vaccClass$ who moved from compartment $E_\vaccCounter$ to $I_\vaccCounter$ since observation $t_{n-1}$.
Here, $\text{NB}\left(\reportRate \reportChange, \obsOverdispersion \right)$ denotes a negative binomial distribution with mean $\reportRate \reportChange$ and variance $\reportRate \reportChange \big(1 + \frac{\reportRate \reportChange}{\obsOverdispersion}\big)$.

\subsection{Model~2}\label{sec:mod2Meas}

Model~2 was fit using reported case counts that were transformed using the natural logarithm.
We fit Model~2 using the subplex algorithm in the \code{subplex} package, using
a Gaussian measurement model (Eq.~\myeqref{model2:meas}) on the log transformed cases within each unit.
The final loss function that is maximized is the product of the likelihoods of the individual units, or the sum of the log-likelihood of the individual units.
The measurement model for individual units is given in Eq.~\myeqref{model2:meas}.
\begin{equation}
  \label{model2:meas}
  \log\big(Y_{u, n} + 1\big) \mid \Delta N_{E_{u\cdot}I_{u\cdot}}(n) = \reportChange_u \sim \text{N}\left(\log \big(\reportRate \reportChange_u + 1\big), \obsOverdispersion^2 \right),
\end{equation}
where $\Delta N_{E_{u\cdot}I_{u\cdot}}(n)$ is the sum total of individuals across vaccination compartment $\vaccCounter \in 0:4$ within unit $u$ who moved from compartment $E_{u\vaccCounter}$ to $I_{u\vaccCounter}$ since observation $t_{n-1}$.
Therefore, because the natural logarithm of observed case counts (plus one, to avoid taking the logarithm of zero) has a normal distribution, $Y_{u, n} + 1$ is assumed to follow a log-normal distribution with log-mean parameter $\log\big(\reportRate \Delta N_{E_{u\cdot}I_{u\cdot}}(n) + 1\big)$ and log-variance $\obsOverdispersion^2$.
We note that fitting a model with this measurement model is equivalent to fitting using least squares, with $\log(Y_{u, n} + 1)$ as the response variable.

This measurement model differs from that used by \citet{lee20}, who fit the model in two stages: epidemic and endemic phases.
Although their text and supplement material do not explicitly describe the measurement model used, inspection of the code provided with their submission suggests a change in measurement model between the epidemic and endemic phases.
% In the file \code{choleraEqsPublic.py}, \citet{lee20} create several functions, where each function represents a set of coupled differential equations that could potentially be used to model cholera incidence data.
% Each function returns a vector (or \code{numpy} array) that represents the change in each state variable for a single time step, including the variable \code{dC}, which tracks the number of new infections and is used to obtain the reported case counts.
% Following their comments and code, it appears that the function \code{choleraEqs10WithoutVaccinationNetwork} was used to describe the dynamics of the epidemic phase, and \code{choleraEqs11WithoutVaccinationNetwork} was used for the endemic stage (see Sec~\ref{sec:mod2rep} for more details).
% Because their models were fit using least squares, the code in these functions suggest that the measurement model for the epidemic phase is
The measurement model they used for the epidemic phase is
\begin{equation}
  \label{model2:measEpi}
  Y_{u, n} \mid \Delta N_{E_{u\cdot}I_{u\cdot}}(n) = \reportChange_u \sim \text{N}\left(\reportRate \reportChange_u, \obsOverdispersion^2 \right),
\end{equation}
% which is similar to our measurement model, the primary difference being that the measurement model is applied to raw case counts rather than log-transformed case counts.
The measurement model they used for the endemic phase modifies the epidemic model by counting both asymptomatically infected (A) and symptomatically infected (I) individuals in the case counts:
\begin{equation}
  \label{model2:measEnd}
  Y_{u, n} \mid \Delta N_{E_{u\cdot}I_{u\cdot}}(n) = \reportChange_{u1}, \Delta N_{E_{u\cdot}A_{u\cdot}}(n) = \reportChange_{u2} \sim \text{N}\left(\reportRate \big(\reportChange_{u1} + \reportChange_{u2}\big), \obsOverdispersion^2 \right),
\end{equation}
where the notation for $\Delta N_{E_{u\cdot}A_{u\cdot}}(n)$ is similar to $\Delta N_{E_{u\cdot}I_{u\cdot}}(n)$, described above.

\subsection{Model~3}

In this model, reported cholera cases are assumed to stem from individuals who develop symptoms and seek healthcare.
Therefore reported cases are assumed to come from an over-dispersed negative binomial model, given the increase in infected individuals:
\begin{equation}
  \label{model3:meas}
  Y_{u, n} \mid \Delta N_{S_{u \cdot}I_{u}}(t) = \reportChange_u \sim \text{NB}\left(\reportRate \reportChange_{u}, \obsOverdispersion \right),
\end{equation}
where $\Delta N_{S_{u \cdot}I_{u \vaccCounter}}(n)$ is the number of individuals who moved from compartment $S_{u \vaccCounter}$ to $I_{u}$ since observation $t_{n-1}$.

This measurement model is a minor change from that used by \cite{lee20}, which allowed for a change in the reporting rate on January 1st, 2018.
The fitted values of the reporting rate---before and after January 2018---were $0.97$ and $0.097$, respectively.
An instantaneous change from near perfect to almost non-existent reporting can be problematic, as it forces the model to explain the observed reduction in reported cases as a decrease in the reporting of cases, rather than a decrease in the prevalence of cholera.
This shift was justified by a ``change of the case definition that occurred on January 1st, 2018";
this claim was not cited, and we could find no evidence that such a drastic change in the reporting rate would be warranted.
We therefore do not allow a change in reporting rate when fitting Model~3.

\section{Initial Values}

To perform inference on POMP models, it is necessary to propose an initial probability density for the latent process $f_{X_0}(x_0;\theta)$, including the possibility that the initial values of the latent states are known, or are a non-random function of the unknown parameter vector, $\theta$.
This density is used to obtain initial values of the latent state when fitting and evaluating the model.
For each of the models considered in this analysis, the initial conditions are derived by enforcing the model dynamics on reported cholera cases.
It is also sometimes necessary to fit some initial value parameters in order to help determine initial values for weakly identifiable compartments.
In the following subsections, we mention initial value parameters that were fit for each model.

\subsection{Model~1}

For this model, the number of individuals in the Recovered and Asymptomatic compartments are set to zero, but the initial proportion of Infected and Exposed individuals is estimated as initial value parameters ($\Iinit$ and $\Einit$, respectively) using the IF2 algorithm, implemented as \texttt{mif2} in the \texttt{pomp} package.
Finally, the initial proportion of Susceptible individuals $S_{0, 0}$ is calculated as $S_{0, 0} = 1 - \Iinit - \Einit$.
This model for the initial values of the latent states matches that which was used by \citet{lee20}.

\subsection{Model~2}

Model~2 assumes that the initial values are a deterministic function of the reporting rate and the initial case reports, and so no initial value parameters need to be estimated.
Initial values for latent state compartments are chosen so as to enforce the model dynamics on the observed number of cases.
Specifically, the model sets $I_{u0}(0) = y^*_{1u} / \reportRate$ for each unit $u \in 1:10$, where $I_{u0}(t)$ is the number of infected individuals in unit $u$ at time $t$, vaccination scenario $z = 0$, $y^*_{tu}$ is the reported number of cases, and $\reportRate$ is the reporting rate.
This model for the initial values of the latent states matches that which was used by \citet{lee20}.

\subsection{Model~3}

The latent states of this model are initialized by enforcing the model dynamics on the incidence data from the start of the recorded cases until time $t_0$, requiring that some of the available data be used to determine the initial values of the latent states.
This is the same approach that was taken by \citet{lee20}, who used the value $t_0 = \text{2014-02-22}$; this choice of $t_0$ results in modeling roughly only $60\%$ of the available data, some of which is later discarded for alternative reasons \citep{lee20sup}.

We do not see any immediate reason that this model could not be extended to cover a larger range of the data, and chose the value $t_0 = \text{2010-11-13}$.
This choice of $t_0$ corresponds to using approximately $1\%$ of the available data to determine initial values of the latent states.
In addition to modeling a larger portion of the available data, this choice of $t_0$ corresponds to an important real-world event, as daily reports from each of the departments were not being sent to the health ministry until November 10, 2010 \citep{barzilay13};
this choice of $t_0$ therefore makes $\bm{Y}\big(t_1\big)$ the first week of data once daily reports are being sent to the health ministry.
The few observation times that exist before $t_0$ are used to initialize the model by enforcing model dynamics on these preliminary observations.
For convenience, we denote these observations as $t_{-3}, t_{-2}$ and $t_{-1}$; as before, we let $y_{u,-k} = Y_{u}\big(t_{-k}\big)$ denote the observed case count for unit $u$ at time point $t_{-k}$, where $k \in 1:3$.
Equations for the initial values of non-zero latent states are provided in Eqs.~\myeqref{eq:model3:I0}--\myeqref{eq:model3:B0}; these equations match those that were used by \citet{lee20}, the primary change being a change to the value of $t_0$.

\begin{eqnarray}
\label{eq:model3:I0}
I_{u}\big(t_0\big) &=& \frac{y^*_{u, -1}}{7\reportRate\left(\muIR + \left(\muDeath + \choleraDeath\right)/365\right)},
\\%[-3pt]
\label{eq:model3:A0}
A_{u}\big(t_0\big) &=& \frac{I_{u0}\big(t_0\big)\left(1 - \symptomFrac\right)}{\symptomFrac},
\\
\label{eq:model3:R0}
R_{u01}\big(t_0\big) &=& R_{u02}\big(t_0\big) = R_{u03}\big(t_0\big) = \left(\frac{\sum_{k = -3}^0 y^*_{u, k}}{\reportRate \symptomFrac} - \left(I_{u0}\big(t_0\big) + A_{u0}\big(t_0\big)\right)\right) / 3
\\
\label{eq:model3:S0}
S_{u0}\big(t_0\big) &=& \mathrm{Pop}_u - I_{u}\big(t_0\big) - A_{u}\big(t_0\big) - \sum_{k=1}^3 R_{u0k}\big(t_0\big)
\\
\label{eq:model3:B0}
B_{u}\big(t_0\big) &=& \big[1 + \seasAmplitude \tilde{J}^r \big] \mathrm{Den}_u \, \Wshed \big[ I_{u}(t_0)+ \asymptomRelativeShed A_{u_0}(t) \big]/\Wshed.
\end{eqnarray}

In Eq.~\myeqref{eq:model3:B0}, $\tilde{J} = 0.002376$ is the median adjusted rainfall over the observation period.
One important consideration to make with this parameter initialization model is when $y_{u, -1}^* = 0$, which occurs for units $u \in \{3, 4\}$, which correspond to the Grand'Anse and Nippes departments, respectively.
When this is the case, each of the infectious $I_u(t_0)$, asymptomatic $A_u(t_0)$, and bacterial reservoir $W_u(t_0)$ compartments have a value of zero.
Because Model~3 models cholera transmission primarily by means of the bacterial reservoir, this makes it nearly impossible for an outbreak to occur.
Therefore for units $u \in \{3, 4\}$, we introduce initial value parameters $I_{0,0}^3$ and $I_{0,0}^4$, and calibrate these parameter values using the data.
The resulting parameter estimates are used to obtain the remaining non-zero initial values of the latent states using Eqs.~\myeqref{eq:model3:A0}--\myeqref{eq:model3:B0}.

\section{Calibrating Model~3 to observed cases}\label{sec:mod3Cal}

<<CalibrateOrig3>>=
# Create vectors for the unit and shared parameters
unit_specific_names <- c("betaB", "foi_add")
shared_param_names <- c(
  "mu_B", "XthetaA", "thetaI", "lambdaR", "r", "std_W",
  "epsilon", "k"
)
est_param_names <- c(
  unit_specific_names, shared_param_names
)

# Add unit numbers to each parameter
est_param_names_expanded <- paste0(rep(est_param_names, each = 10), 1:10)

# Simple function that is used to set the rw_sd for the first search
# This function is convenient so that we don't have to type out the rw_sd
# for each unit parameter.
set_rw_1 <- function(x) {
  if (gsub("[[:digit:]]+$", "", x) %in% shared_param_names) {
    return(0.01)
  # } else if (x %in% paste0(rep(c("aHur", 'hHur'), each = 2), c(3, 9))) {
  #   return(expression(ifelse(time >= 2016.754 & time <= 2017, 0.015, 0)))
  # } else if (x %in% c("Iinit3", "Iinit4")) {
  #   return(expression(ivp(0.25)))
  } else if (grepl('^foi_add[[:digit:]]+$', x)) {
    return(0.015)
  } else if (grepl('^betaB[[:digit:]]+$', x)) {
    return(0.01)
  } else {
    return(0)
  }
}

set_rw_2 <- function(x) {
  if (gsub("[[:digit:]]+$", "", x) %in% shared_param_names) {
    return(0.003)
  # } else if (x %in% paste0(rep(c("aHur", 'hHur'), each = 2), c(3, 9))) {
  #   return(expression(ifelse(time >= 2016.754 & time <= 2017, 0.005, 0)))
  # } else if (x %in% c("Iinit3", "Iinit4")) {
  #   return(expression(ivp(0.125)))
  } else if (grepl('^foi_add[[:digit:]]+$', x)) {
    return(0.005)
  } else if (grepl('^betaB[[:digit:]]+$', x)) {
    return(0.003)
  } else {
    return(0)
  }
}

# First Search RW
reg_rw_1.sd <- lapply(est_param_names_expanded, set_rw_1)
names(reg_rw_1.sd) <- est_param_names_expanded
chol_rw_1.sd <- do.call(rw_sd, reg_rw_1.sd)

# Second Search RW
reg_rw_2.sd <- lapply(est_param_names_expanded, set_rw_2)
names(reg_rw_2.sd) <- est_param_names_expanded
chol_rw_2.sd <- do.call(rw_sd, reg_rw_2.sd)

SEARCH1 = list(
  NBPF = 100,
  NP = 2500,
  SPAT_REGRESSION = 0.05,
  NREPS = 144,
  NP_EVAL = 2500,
  NREPS_EVAL = 18,
  RW_SD = chol_rw_1.sd,
  COOLING = 0.5,
  KEEP_TRACES = FALSE,
  KEEP_LIKE_MAT = FALSE
)

SEARCH2 = list(
  TOP_N = 12,
  NBPF = 100,
  NP = 2500,
  SPAT_REGRESSION = 0.05,
  NREPS = 6,
  NP_EVAL = 2500,
  NREPS_EVAL = 18,
  RW_SD = chol_rw_2.sd,
  COOLING = 0.5,
  KEEP_TRACES = FALSE,
  KEEP_LIK_MAT = TRUE
)

haiti3_sub_fit <- bake(
  file = "../model3/si/subset_noparams.rds", {
    fit_haiti3(
      nsearches = 2L,
      search1 = SEARCH1,
      search2 = SEARCH2,
      search_rho = FALSE,
      search_gamma = FALSE,
      search_Iinit = FALSE,
      search_hur = FALSE,
      ncores = cores
    )
  }
)

# Find model index with best log-likelihood
best_m <- which.max(haiti3_sub_fit$search2$logLiks$logLik)

# Find parameters using best_m index
sub_params <- haiti3_sub_fit$search2$params[best_m, ]

# Get unit log-likelihoods for the best model.
all_liks <- as.data.frame(haiti3_sub_fit$search2$likMat)
# all_liks$which <- rep(1:(SEARCH3$TOP_N * SEARCH3$NREPS), each = SEARCH3$NREPS_EVAL)
all_liks$which <- rep(1:(12 * 6), each = 18)
best_mod <- filter(all_liks, which == best_m)

# Pivot longer to get estimate of log-likelihood for each unit.
unitLikesM3 <- best_mod %>%
  pivot_longer(
    cols = -which,
    names_to = "unit",
    names_prefix = "V",
    values_to = "logLik"
  ) %>%
  select(-which) %>%
  group_by(unit) %>%
  summarize(
    ll_est = logmeanexp(logLik),
    ll_se = logmeanexp(logLik, se = TRUE)[2]
  ) %>%
  ungroup() %>%
  mutate(unit = as.numeric(unit)) %>%
  arrange(unit)
@


<<M3_bench, echo=FALSE, message=FALSE, include=FALSE, cache=TRUE>>=
h3 <- haiti3_spatPomp()
deps <- h3@unit_names

mod3_cases <- as.data.frame(t(h3@data))
colnames(mod3_cases) <- deps
m3_inits <- c(7362, 1914, 22, 8, 2818, 82, 2499, 6953, 44, 7)  # fit by hand
names(m3_inits) <- deps

bench_cond_lls <- list()
for (dep in deps) {

  # Get department cases
  cases <- as.numeric(mod3_cases[, dep])
  mod <- ar1_NegBinom(data = cases, init = m3_inits[dep])

  bench_cond_lls[[dep]] <- haitipkg:::.ar1_NegBinom_cond_ll(
    theta = mod$theta, init = m3_inits[dep], data = cases, transform = FALSE
  )
}

bench_cond_lls <- as.data.frame(bench_cond_lls)

bench_lls <- apply(bench_cond_lls, 2, sum)

unitLikes_bench <- data.frame(
  unit = 1:10,
  # dep = names(bench_lls),
  ll = bench_lls
) %>%
  as_tibble()

bench_cond_lls$date <- lubridate::round_date(lubridate::date_decimal(h3@times), unit = 'day')
@

<<combine_bench_M3, echo=FALSE>>=

# Combine tables with model 3 and benchmark likelihoods.
bench_M3_joined <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  select(-ll_se) %>%
  mutate(unit = as.character(unit)) %>%
  pivot_longer(
    cols = -unit,
    names_to = "type",
    values_to = "ll"
  ) %>%
  mutate(unit = paste0(unit, ": ", deps[as.numeric(unit)]))
@

<<M3bpf, echo=FALSE, message=FALSE, include=FALSE>>=

# Change model parameters to the best set of parameters from previous
# search
coef(h3)[names(sub_params)] <- sub_params

registerDoRNG(40765101)  # for modular, reproducible results

# Perform block partical filter with fitted model, to get conditional
# log-likelihoods.
bpf_sub3_out <- bake(
  file = "../model3/si/bpf_sub_cond_LL.rds", {

    results <- list()

    bpf_out <- h3 %>% bpfilter(
      Np = 5000,
      block_size = 1,
      save_states = TRUE
    )

    results$block.cond.loglik <- bpf_out@block.cond.loglik
    results$times <- bpf_out@times  # Saved for convenience
    results
  },
  seed = 432235
)
@

In this section, we provide more detail on the process that was used to estimate the coefficients of Model~3.
In particular, we discuss why we decided to include additional model parameters---those that are associated with the behavior of the system during Hurricane Matthew---that were not considered by \cite{lee20}.
To calibrate this model, we used the iterated block particle filter (IBPF) method of \cite{ionides22}.
Due to the novelty of this algorithm, there does not exist published examples the IBPF algorithm used for data analysis outside of the papers in which the algorithm was introduced \citep{ning21ibpf,ionides22}, which is one motivation of the inclusion additional details related to fitting and diagnosing the model fit provided here.

\cite{lee20} were only able to estimate model parameters to a simplified version of Model~3 on a subset of the available data, as no method existed at the time of their publication to fit a fully coupled meta-population model to disease incidence data.
Building on their results, we fit the fully coupled version of Model~3 to nearly all available data, reserving only a few observations to use to calibrate the initial conditions of the model (see the previous section for more details).
In order to properly maximize the model likelihood, we found that it was necessary to use multiple searches for the MLE, periodically pruning away less successful searches.
The first collection of searches was performed by obtaining initial values for the parameters by uniformly sampling values from a predefined hypercube.
A subsequent refinement search used parameter values corresponding to the largest model likelihoods as starting parameter values.
The need for multiple searches does not appear to be uncommon, as a similar approach was used by \cite{ionides22}.

We use the technique described above to fit the fully coupled version of Model~3 proposed by \cite{lee20}.
The maximum likelihood we obtained after two rounds of searching was $\Sexpr{round(max(haiti3_sub_fit$search2$logLiks$logLik))}$, which is higher than the benchmark model ($\Sexpr{round(sum(unitLikes_bench$ll))}$).
While beating a simple associative benchmark is promising, this does not immediately rule out the possibility that the model is a poor description of the system.
Additional investigation of parameters estimates and their corresponding implications on model based conclusions should always be conducted.
For meta-population models, it is worth considering how well the model fits the data to each spatial unit.
% This can be done by looking at conditional log-likelihoods, which is part of the output of the \texttt{bpfilter} algorithm in the \texttt{spatPomp} package.
The likelihoods for each department, compared to the corresponding benchmark model, are displayed in Fig.~\ref{fig:h3UnitLikes}.
The figure demonstrates that while the log-likelihood of the fitted model outperforms the auto-regressive negative binomial benchmark model, Model~3 has lower likelihoods than the benchmark for some departments.

\begin{figure}[!h]
<<h3UnitLikes, fig.height=3, fig.width=5.5, echo=FALSE>>=
ggplot(bench_M3_joined, aes(x = ll, y = reorder(unit, ll))) +
  geom_point(aes(col = type)) +
  ylab("Unit") +
  xlab("Log Likelihood") +
  scale_color_manual(
    labels = c("ll" = "Benchmark", "ll_est" = "Model 3"),
    values = c("ll" = "#1b9e77", "ll_est" = "#d95f02")
  ) +
  theme(legend.title = element_blank(),
        axis.title.y = element_blank())
@
\caption{\label{fig:h3UnitLikes}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model prior to the inclusion of parameters that account for Hurricane Matthew.}
\end{figure}

In addition to considering the conditional log-likelihoods of each unit, one can consider conditional log-likelihoods of each observation.
When compared to a benchmark, this level of detail can provide useful information about which observations are well described by the model and which are not.
In Fig.~\ref{fig:condLL}, we plot the conditional log-likelihoods of Model~3 for each observation.
Typically it is most useful to compare the conditional log-likelihoods of the model under consideration to a benchmark, as plotting only conditional log-likelihoods without a comparison may not be helpful.
In this case, however, the same insight can be drawn using a figure without a benchmark comparison, so we do not include the benchmark in order to avoid the issue of over-plotting.

\begin{figure}[!h]
<<CondLLFig, echo=FALSE, message=FALSE, fig.height=3>>=

# Changing labels to human-readable labels.
deps[grepl("_", deps)] <- gsub("_", "-", deps[grepl("_", deps)])
deps[deps == "Grande-Anse"] <- "Grand\'Anse"
names(deps) <- as.character(1:10)
myLabeller <- as_labeller(deps)

# Plot conditional log-likelihoods for each unit
t(bpf_sub3_out$block.cond.loglik) %>%
  as.data.frame() %>%
  mutate(t = bpf_sub3_out$times) %>%
  pivot_longer(
    cols = -t,
    values_to = 'likelihood',
    names_prefix = "V",
    names_to = 'Unit'
  ) %>%
  mutate(date = as.Date(lubridate::date_decimal(t))) %>%
  ggplot(aes(x = date, y = likelihood)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Unit, labeller = myLabeller, scales = 'free_y',
             nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Conditional log-likelihoods")
@
\caption{\label{fig:condLL}Conditional log-likelihoods of Model~3 prior to the inclusion of the Hurricane Matthew related parameters.}
\end{figure}

Fig.~\ref{fig:condLL} reveals that the fitted model poorly describes certain features of the data.
For example, many departments (in particular Sud) have observations with lower conditional log-likelihoods near October 2016 than at other time points.
Further investigation of the model output reveals that the model is struggling to explain the sudden surge in cholera cases that occurred at this time, which coincides with the time that Hurricane Matthew struck Haiti.
While the model does include a mechanism to account for increased cholera transmission due to large rainfall events, the mechanism does not appear to be sufficient to capture the damaging effects of the hurricane, which had the greatest impact in the the Sud and Grand'Anse departments \citep{ferreirai16}.
This result led us to include parameters $\Whur{u}$ and $\hHur{u}$ in Eq.~23 of the main text, which allows for an increase in the transmission rate between environmental cholera and humans for in Sud and Grand'Anse during and after the hurricane.
The effect of the hurricane on cholera transmission is assumed to have an exponential decay, where the magnitude is determined by $\Whur{u}$ and the duration of the effect determined by $\hHur{u}$.

<<loadFinalModel3, echo=FALSE, include=FALSE, message=FALSE>>=

# This result was used in the manuscript, so it doesn't need to be recalculated.
h3_final_results <- readRDS("../model3/run_level_3/haiti3_fit.rds")

# Save best_m as index for the model parameters with the best likelihood.
# Then, save the model parameter values and the likelihood evaluations
best_m <- which.max(h3_final_results$search2$logLiks$logLik)
best_params <- h3_final_results$search2$params[best_m, ]
all_liks <- as.data.frame(h3_final_results$search2$likMat)
all_liks$which <- rep(1:72, each = 18)
best_mod <- dplyr::filter(all_liks, which == best_m)

# Get repeated estimates of unit log-likelihoods
unitLikesM3 <- best_mod %>%
  pivot_longer(
    cols = -which,
    names_to = "unit",
    names_prefix = "V",
    values_to = "logLik"
  ) %>%
  select(-which) %>%
  group_by(unit) %>%
  summarize(
    ll_est = pomp::logmeanexp(logLik),
    ll_se = pomp::logmeanexp(logLik, se = TRUE)[2]
  ) %>%
  ungroup() %>%
  mutate(unit = as.numeric(unit)) %>%
  arrange(unit)

# Combine model likelihood table to benchmarks
bench_M3_joined <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  select(-ll_se) %>%
  mutate(unit = as.character(unit)) %>%
  pivot_longer(
    cols = -unit,
    names_to = "type",
    values_to = "ll"
  ) %>%
  mutate(unit = paste0(unit, ": ", deps[as.numeric(unit)]))

bench_diffs <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  rename(ll_mod3 = ll_est, ll_bench = ll) %>%
  mutate(unit = unlist(myLabeller(unit)),
         ll_diff = ll_mod3 - ll_bench) %>%
  arrange(ll_diff)

floor(100 * sum(mod3_cases[, c(bench_diffs$unit[1:4])], na.rm = TRUE) / sum(mod3_cases, na.rm = TRUE))
@

We refit Model~3 after introducing these hurricane-related parameters using the same successive search scheme described above.
The resulting model has a log-likelihood value of $\Sexpr{round(max(h3_final_results$search2$logLiks$logLik), 1)}$.
% We also provide figures for department specific log-likelihoods and conditional log-likelihoods in Figs.~\ref{fig:finalUnitLL} and \ref{fig:finalCondLL}.
The addition of the Hurricane parameters seems to have resulted in a dramatic increase in conditional likelihoods around and after October 2016, as seen in Fig.~\ref{fig:finalCondLL}.
The inclusion of these parameters resulted in an overall increase of $\Sexpr{round(max(h3_final_results$search2$logLiks$logLik) - max(haiti3_sub_fit$search2$logLiks$logLik), 1)}$ log-likelihood units, which is a highly statistically significant difference.

<<finalModel3BPF, echo=FALSE, message=FALSE, include=FALSE>>=

# Set the model parameters to the best results from previous search
coef(h3)[names(best_params)] <- best_params

# block particle filter to get conditional log-likelihoods.
bpf_final3_out <- bake(
  file = "../model3/si/h3_final_bpf_cond_ll.rds", {

    results <- list()

    bpf_out <- h3 %>% bpfilter(
      Np = 5000,
      block_size = 1,
      save_states = TRUE
    )

    results$Sus_df <- foreach(i=1:length(h3@times), .combine = bind_rows) %do% {
      get_h3_S_quants(bpf_out, i)
    }

    results$block.cond.loglik <- bpf_out@block.cond.loglik
    results$times <- bpf_out@times  # saved for convenience
    results
  },
  seed = 786321
)
@

\begin{figure}[!h]
<<finalCondLLFig, echo=FALSE, message=FALSE, fig.height=3>>=
t(bpf_final3_out$block.cond.loglik) %>%
  as.data.frame() %>%
  mutate(t = bpf_final3_out$times) %>%
  pivot_longer(
    cols = -t,
    values_to = 'likelihood',
    names_prefix = "V",
    names_to = 'Unit'
  ) %>%
  mutate(date = as.Date(lubridate::date_decimal(t))) %>%
  ggplot(aes(x = date, y = likelihood)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Unit, labeller = myLabeller, scales = 'free_y',
             nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Conditional log-likelihoods")
@
\caption{\label{fig:finalCondLL}Conditional log-likelihoods of Model~3 after adding and estimating the parameters related to Hurricane Matthew.}
\end{figure}

Now that the model with additional parameters has been calibrated to the incidence data, we plot the conditional log-likelihood of each department compared to a benchmark model in Fig.~\ref{fig:finalUnitLL}.
Interestingly, the difference in log-likelihoods between Model~3 and the benchmark model is smallest in the departments \Sexpr{bench_diffs$unit[1]}, \Sexpr{bench_diffs$unit[2]}, \Sexpr{bench_diffs$unit[3]} and \Sexpr{bench_diffs$unit[4]}.
Each of these departments also exhibited the most sustained cholera transmission, defined by having the fewest number of weeks with no recorded cholera cases.
Specifically, these four departments have zero cholera cases recorded in less than $4\%$ of the available data, and all remaining departments---except for Nord-Ouest, which has approximately $9.5\%$ of cases that are zeros and also exhibits the next smallest difference in log-likelihoods---have zero cases recorded in at least $14\%$ of the available weekly data.
This result suggests that the quantitative advantage Model~3 has over its respective benchmark is primarily due to the model's ability to describe a resurgence of cases after a department records a week of zero cholera cases.
This result may be unsurprising in the context of the models that we are comparing.
Because the cholera transmission in individual departments likely depends on the national prevalence of cholera and the Vibrio cholerae bacteria, our benchmark model that relies exclusively on the previous number of case within any given unit has a difficult time predicting a resurgence of cases.

\begin{figure}[!h]
<<finalUnitLLFig, fig.height=3, fig.width=5.5, echo=FALSE>>=
ggplot(bench_M3_joined, aes(x = ll, y = reorder(unit, ll))) +
  geom_point(aes(col = type)) +
  ylab("Unit") +
  xlab("Log Likelihood") +
  scale_color_manual(
    labels = c("ll" = "Benchmark", "ll_est" = "Model 3"),
    values = c("ll" = "#1b9e77", "ll_est" = "#d95f02")
  ) +
  theme(legend.title = element_blank(),
        axis.title.y = element_blank())
@
\caption{\label{fig:finalUnitLL}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model after adding and estimating parameters related to Hurricane Matthew.}
\end{figure}

The difference in log-likelihoods between Model~3 and its benchmark model for each individual units suggests that Model~3 has a relatively poor fit for the four units with the most sustained cholera transmission.
The simple four parameter benchmark has a higher likelihood than Model~3 for the \Sexpr{bench_diffs$unit[1]} and \Sexpr{bench_diffs$unit[2]} departments, and also has log-likelihoods only a few units smaller than Model~3 for units \Sexpr{bench_diffs$unit[3]} and \Sexpr{bench_diffs$unit[4]}.
This is particularly worrisome given that these four departments account for more than $\Sexpr{floor(100 * sum(mod3_cases[, c(bench_diffs$unit[1:4])], na.rm = TRUE) / sum(mod3_cases, na.rm = TRUE))}\%$ of the total number of reported cholera cases.

\subsection{Examining the Hidden States of the Calibrated Model}

For mechanistic models, beating a suitable statistical benchmark does not alone guarantee that the model provides an accurate description of a dynamic process.
Indeed, a good statistical fit does not require the model to assert a causal explanation.
For example, reconstructed latent variables should make sense in the context of alternative measurements of these variables \citep{grad12}.
We demonstrate this principle by examining the latent state of the calibrated model.
In particular, we examine the compartment of susceptible individuals under various scenarios.

Recall that the filtering distribution for the calibrated version of Model~3 at time $t_k$ is defined as the distribution of the latent state at time $t_k$ given the data from times $t_{1}:t_{k}$, i.e. $f^{(3)}_{\bm{X}_k|\bm{Y}_{1:k}}(\bm{x}_{k} | \bm{y}^*_{1:k} ; \hat\theta)$.
In general, one may expect simulations from the filtering distribution of a model with a good statistical fit to result in hidden states that are highly consistent with the observed data because the filtering distribution is conditioned on the observed data.
Fig.~\ref{fig:h3Sus} shows the percentage of individuals that are in the susceptible compartment from various simulations of the model:
simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
This figure shows that simulations from initial conditions tends to result in a much more rapid depletion of susceptible individuals at the start of the epidemic than simulations from the filtering distribution, suggesting the calibrated model has a propensity to predict larger outbreaks than what is typically seen in the data.
This also appears true in later stages of the time period of observed data, where the data suggests that there is a replenishment of the susceptible compartment, but the Model simulations typically retain a smaller proportion of susceptible individuals.
This result demonstrates that the calibrated model favors a more rapid growth in cholera cases than what is typically seen in the observed data, which explains why the model fails to predict the absence of cases between February, 2019 and October, 2022.

<<h3SusFigSetup, echo=FALSE, message=FALSE, include=FALSE>>=

# This bake perfoms the following:
#  (1) simulates the model from initial conditions
#  (2) get the values of the hidden states at the end of each simulation, and
#      saves these for later use.
#  (3) Obtain quantiles for the Susceptible pool from the simulations.
init_conditions_results <- bake(
  file = "../model3/si/InitSimsQuants.rds", {

    results <- list()  # To return final output of bake

    # Simulate from the fitted model
    h3Spat_sims <- simulate(
      h3, nsim = 2000, format = 'data.frame'
    )

    # Get states at the end of the simulations
    h3_spat_end <- h3Spat_sims %>%
      filter(time == max(time))

    # Save hidden state values at the end of the simulation. Saved as a matrix
    # in the same format as the output from a block-particle filter, so that
    # it can be used as the input to project_from_filter.
    results[["end_conditions"]] <- h3_spat_end %>%
      select(-time, -cases) %>%
      mutate(
        unit = case_when(
          unitname == "Artibonite" ~ 1,
          unitname == "Centre" ~ 2,
          unitname == "Grande_Anse" ~ 3,
          unitname == "Nippes" ~ 4,
          unitname == "Nord" ~ 5,
          unitname == "Nord_Est" ~ 6,
          unitname == "Nord_Ouest" ~ 7,
          unitname == "Ouest" ~ 8,
          unitname == "Sud" ~ 9,
          unitname == "Sud_Est" ~ 10
        )
      ) %>%
      select(-unitname) %>%
      pivot_wider(
        data = .,
        names_from = unit,
        values_from = c(S, I, A, R_one, R_two, R_three, VSd, VR1d, VR2d,
                        VR3d, VSdd, VR1dd, VR2dd, VR3dd, VSd_alt, VR1d_alt,
                        VR2d_alt, VR3d_alt, VSdd_alt, VR1dd_alt, VR2dd_alt,
                        VR3dd_alt, C, B, Doses, totInc),
        names_sep = ""
      ) %>%
      select(-.id) %>%
      as.matrix() %>%
      t()

    # Get the quantiles for the Susceptible pool from the simulations
    results[["SQuants"]] <- h3Spat_sims %>%
      rename(dep = unitname) %>%
      group_by(dep, time) %>%
      summarise(
        Q025 = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.025),
        Q50  = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.5),
        Q975 = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.975)
      ) %>%
      ungroup() %>%
      rowwise() %>%
      mutate(
        unit_num = which(h3@unit_names == dep),
        date = lubridate::date_decimal(time),
        Q025 = Q025 / h3@params[paste0("H", unit_num)],
        Q50  = Q50  / h3@params[paste0("H", unit_num)],
        Q975 = Q975 / h3@params[paste0("H", unit_num)]
      ) %>%
      filter(
        date >= as.Date(
          lubridate::round_date(lubridate::date_decimal(h3@t0), unit = 'day')
        )
      ) %>%
      mutate(date = as.Date(lubridate::round_date(date, unit = 'day')))

    results
  },
  seed = 834687
)

# Get the output of the "bake" call for plotting.
h3Spat_quants <- init_conditions_results[["SQuants"]]

# This bake call uses the ending hidden states from above to forecast cholera
# cases in the future under the V0 vaccination scenario
mod3_V0_initcond_res <- bake(
  file = '../model3/si/vacc_sim_from_init.rds', {
    h3_V0_res <- list()

    # Get "noVacc" scenario parameters
    h3_S0_par <- get_model3_vacc_scenario_params(scenario = "noVacc")
    h3Spat <- h3
    p3 <- best_params
    coef(h3Spat) <- p3
    coef(h3Spat)[names(h3_S0_par)] <- h3_S0_par

    # store the end states that will be used as initial values
    h3_bpf_end_states <- init_conditions_results$end_conditions

    # Using parameters and end state values, obtain projections
    h3_V0_sims <- project_from_filter(
      h3Spat, end_states = h3_bpf_end_states, covarGen = project_rain,
      nsims = 1000, seed = 184697
    )

    # Store nationally aggregated results
    h3_V0_res$mod3_V0_sims <- agg_mod3_sims(h3_V0_sims)
    h3_V0_res$mod3_V0_probs <- get_elimProbs(h3_V0_sims, model = 3)

    # Get quantiles of observed cases for each department and time
    h3_V0_res$UnitQuants <- h3_V0_sims %>%
      mutate(
        date = as.Date(  # Get date object from decimal time
          lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
        )
      ) %>%
      group_by(unitname, date) %>%
      summarize(
        Q025 = quantile(cases, probs = 0.025),
        Q50 = quantile(cases, probs = 0.5),
        Q975 = quantile(cases, probs = 0.975)
      ) %>%
      ungroup() %>%
      rename(dep = unitname)

    # Get quantiles of susceptible individuals for each department and time
    h3_V0_res$SQuants <- h3_V0_sims %>%
      mutate(
        date = as.Date(
          lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
        )
      ) %>%
      group_by(unitname, date) %>%
      mutate(
        # Get the population for each unit
        pop = h3@params[paste0("H", which(first(unitname) == unit_names(h3)))]
      ) %>%
      summarize(  # Calculate the quantiles
        Q025 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.025
        ),
        Q50 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.5
        ),
        Q975 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.975
        )
      ) %>%
      ungroup() %>%
      rename(dep = unitname)

    h3_V0_res
  }
)

# This result was already used in the manuscript; it doesn't need to
# be recalculated.
mod3_V0_res <- readRDS("../model3/run_level_3/mod3_V0_res.rds")
@

\begin{figure}[!h]
<<h3SusFig, echo=FALSE, message=FALSE, fig.height=3>>=
ggplot() +
  # Sims from initial condition
  geom_line(data = h3Spat_quants, aes(x = date, y = Q50), col = 'red') +
  geom_ribbon(
    data = h3Spat_quants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'red', alpha = 0.5
  ) +
  # projections from initial condition, using filtered sims function
  geom_line(data = mod3_V0_initcond_res$SQuants, aes(x = date, y = Q50), col = 'red') +
  geom_ribbon(
    data = mod3_V0_initcond_res$SQuants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'red', alpha = 0.5
  ) +
  # Sims from filtered distribution
  geom_line(
    data = bpf_final3_out$Sus_df,
    aes(x = as.Date(lubridate::date_decimal(time)), y = Q50),
    col = 'blue'
  ) +
  geom_ribbon(
    data = bpf_final3_out$Sus_df,
    aes(x = as.Date(lubridate::date_decimal(time)), ymin = Q025, ymax = Q975),
    fill = 'blue', alpha = 0.5
  ) +
  # Projections from filtered distribution
  geom_line(data = mod3_V0_res$SQuants, aes(x = date, y = Q50), col = 'blue') +
  geom_ribbon(
    data = mod3_V0_res$SQuants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'blue', alpha = 0.5
  ) +
  geom_vline(xintercept = max(h3Spat_quants$date), linetype = 'dashed') +
  facet_wrap(~dep, scales = 'free_y', nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Total Susceptible (%)")
@
\caption{\label{fig:h3Sus}Percentage of individuals that are in the susceptible compartment.
Simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
The dashed line represents the end of the observed data.}
\end{figure}

\clearpage

\section{Replication of \cite{lee20}}

In this article we claimed that we were able to obtain better fits to the observed data using the same models that were proposed by \cite{lee20}.
Along with visual comparisons to the data, this claim was supported by comparing likelihoods and AIC values in Table~2 in the manuscript.
Because model likelihoods were not provided by \cite{lee20}, it is necessary to replicate these models in order to obtain likelihood estimates.
Here we would like to thank the authors of \cite{lee20}, who provided detailed descriptions of their models, which enabled us to build on their work.
In the following subsections, we use our \code{R} package \code{haitipkg} to reproduce some of the results of \cite{lee20}.
This reproduction allows us to estimate the likelihoods of the \cite{lee20} version of Models~1--3, and also provides a demonstration of the importance and usefulness of reproducible research.

\subsection{Model~1 Replication}

The model was implemented by a team at Johns Hopkins Bloomberg School of Public Health (hereafter referred to as the Model~1 authors) in the \code{R} programming language using the \code{pomp} package \citep{king16}.
Original source code is publicly available with DOI: 10.5281/zenodo.3360991.
The final results reported by the Model~1 authors were obtained by using several different parameter sets rather than a single point estimate.
According to the supplement materials, this was because model realizations from a single parameter set retained substantial variability, but multiple realizations from a collection of parameter sets resulted in a reasonable visual fit to the data.
We are also inclined to believe that the use of multiple parameter values was in part intended to account for parameter uncertainty---the importance of which was discussed in the main text---an effort by the Model~1 authors that we applaud.
Simulations from each of the parameter sets, however, were treated with equal importance when being used to diagnose the model fit and make inference on the system.
This is problematic given Figures S8 and S9 of the supplement material, which suggest that some parameter sets that were used for inference may have been several hundred units of log-likelihood lower than other parameter sets that were simultaneously used to make forecasts.
Such a large difference in log-likelihoods is well beyond the threshold of statistical uncertainty determined by Wilks' theorem, resulting in the equal use of statistically inferior parameter sets in order to make forecasts and conduct inference on the system.

To fully reproduce the results of the Model~1 authors, it is necessary to use the exact same set of model parameters that were originally used to obtain the results presented by \cite{lee20}.
Because these parameter sets were not made publicly available, we relied on the source code provided by the Model~1 authors to approximately recreate the parameter set.
Due to software updates since the publication of the source code, we were unable to produce the exact same set of parameters.
Running the publicly available source code, however, resulted in a set of parameters that are visually similar to those used by the Model~1 authors (See Figures~\ref{fig:PlotEpiDist} and \ref{fig:plotEndParams}).
Furthermore, simulations using the set of parameters produced by the source code appear practically equivalent to those displayed by \cite{lee20} (See Figure~\ref{fig:plotMod1Sims}).

<<Simulate Lee Model 1, echo=FALSE, cache=TRUE>>=
h1_epi <- haiti1()
N_LEE_SIMS <- 20

registerDoRNG(18599687)
foreach(
  i = 1:length(h1LeeStartsEpi),
  .combine = rbind
) %dopar% {
  guess <- h1LeeStartsEpi[i, ]
  sims <- simulate(h1_epi, nsim = N_LEE_SIMS, format = 'data.frame',
                   params = guess)  # 20 sims for each set of parameters
  sims$param_set <- i
  sims %>%
    dplyr::select(.id, week, cases, param_set)
} -> all_sims
@

<<PlotEpiDist, echo=FALSE, fig.cap="Compare to Figure S8 of Lee et al. (2020) supplement.", fig.width=7, fig.height=7, fig.align='center'>>=
GGally::ggpairs(
  h1LeeStartsEpi,
  columns = c("rho", "tau", "beta1", "nu", "loglik")
)
@

<<SimulateEpi, echo=FALSE, include=FALSE, message=FALSE, cache=TRUE>>=
h1_end <- haiti1(period = "endemic")

true_agg_cases <- haitiCholera %>%
  select(-report) %>%
  pivot_longer(
    data = .,
    cols = -date_saturday,
    values_to = 'cases',
    names_to = 'dep'
  ) %>%
  mutate(
    date = as.Date(date_saturday),
    dep = gsub("\\.", "_", dep)
  ) %>%
  mutate(
    dep = case_when(dep == "Grand_Anse" ~ "Grande_Anse", TRUE ~ dep)
  ) %>%
  tidyr::pivot_wider(
    data = .,
    id_cols = c(date),
    names_from = dep,
    values_from = cases,
    names_prefix = 'cases_'
  ) %>%
  mutate(
    ReportedAll = cases_Artibonite + cases_Centre +
      cases_Grande_Anse + cases_Nippes + cases_Nord +
      cases_Nord_Est + cases_Ouest + cases_Sud +
      cases_Sud_Est + cases_Nord_Ouest
  )
@

<<FitEnd, echo=FALSE, message=FALSE, include=FALSE>>=
foreach(
  i = 1:nrow(h1LeeStartsEnd),
  .combine = rbind
) %dopar% {
  r_params <- h1LeeStartsEnd[i, ][names(coef(h1_end))]
  coef(h1_end) <- r_params

  sims <- simulate(
    h1_end, nsim = N_LEE_SIMS,
    format = 'data.frame',
    params = r_params
  )
  sims$param_set <- i
  sims %>%
    dplyr::select(.id, week, cases, param_set)
} -> end_sims

@

<<plotEndParams, fig.width=7, fig.height=7, fig.align='center', fig.cap="Bivariate relationships between variables after fitting endemic period. Compare to S9 of Lee et al. (2020) supplement.", cache=TRUE, echo=FALSE>>=
GGally::ggpairs(
  h1LeeStartsEnd,
  columns = c("rho", "tau", "beta1", "nu", "loglik")
)
@

<<calcMod1LeeLikes, echo=FALSE, include=FALSE, message=FALSE>>=
h1_epi_evals <- readRDS("../model1/run_level_3/lee1_epi_evals.rds")
h1_end_evals <- readRDS("../model1/run_level_3/lee1_end_evals.rds")

mod1_all_likes <- dplyr::inner_join(
  x = h1_epi_evals,
  y = h1_end_evals,
  by = "parid"
  ) %>%
  dplyr::mutate(
    joint_ll = pfLL.x + pfLL.y
  )

# Approximate the likelihood of the Lee et al (2020) version of Model 1 by
# using the parameter sets that resulted in the maximum likelihood.
mod1_lee_ll <- max(mod1_all_likes$joint_ll, na.rm = TRUE)
@


<<plotMod1Sims, echo=FALSE, fig.cap=paste0("Simulations using parameter sets that were generated by running source code provided by Lee et al (2020). Compare to Figure S7 of \\cite{lee20sup}. The upper bound for the likelihood of this model is ", round(mod1_lee_ll), "."), cache=TRUE, fig.width=6.5, fig.height=3.75>>=

quants <- all_sims %>%
  filter(!param_set %in% c(1, 2)) %>%  # Remove parameter sets inconsistent with Lee et al (2020).
  group_by(week) %>%
  summarize(
    q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
    q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
    q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

quants_end <- end_sims %>%
  group_by(week) %>%
  summarize(
    q025 = quantile(cases, probs = 0.025, na.rm = TRUE),
    q50  = quantile(cases, probs = 0.500, na.rm = TRUE),
    q975 = quantile(cases, probs = 0.975, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  mutate(date = lubridate::ymd("2010-10-16") + lubridate::weeks(week))

ggplot() +
  geom_point(data = true_agg_cases, aes(x = date, y = ReportedAll)) +
  geom_line(data = quants, aes(x = date, y = q50), col = '#0b8a2b', size = 1) +
  geom_ribbon(
    data = quants,
    aes(x = date, ymin = q025, ymax = q975),
    alpha = 0.5, fill = '#0b9c30'
  ) +
  geom_line(data = quants_end, aes(x = date, y = q50), col = '#882871') +
  geom_ribbon(data = quants_end, aes(x = date, ymin = q025, ymax = q975), alpha = 0.5, fill = '#882871') +
  ylab("Reported, Symptomatic Incidence") +
  scale_x_date(date_breaks = "1 year") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(hjust = 1, angle = 45))
@

Because the model forecasts provided by \cite{lee20} come from various sets of parameters---which each correspond to a unique log-likelihood value---it is not obvious how one would obtain an estimate for the log-likelihood of the model that was used for simulations by the Model~1 authors.
One approach could be to calculate the logarithm of the weighted mean of the likelihoods for each parameter sets used to obtain the forecasts, where the weights are proportional to the number of times the parameter set was used.
However, in an effort to not underestimate the likelihood of the model of the Model~1 authors, we report the estimated log-likelihood as the log-likelihood value corresponding to the parameter set with the largest likelihood value, even though the majority of simulations were obtained using parameter sets with lower likelihood values.
In this sense, we consider the log-likelihood reported in Table~1 of the main text to be an upper-bound of the log-likelihood of the model used by \cite{lee20}.
For each parameter set, the log-likelihood was estimated using a particle filter, implemented as the \texttt{pfilter} function in the \texttt{pomp} package.

\subsection{Model~2 Replication}\label{sec:mod2rep}

Model~2 was developed by a team that consisted of members from the Fred Hutchinson Cancer Research Center and the University of Florida (hereafter referred to as the Model~2 authors).
While Model~2 is the only deterministic model we considered in our analysis, it contains perhaps the most complex descriptions of cholera in Haiti: Model~2 accounts for movement between spatial units; human-to-human and environment-to-human cholera infections; and transfer of water between spatial units based on elevation charts and river flows.

The source code that the Model~2 authors used to generate their results was written in the \code{Python} programming language, and is publicly available at \url{10.5281/zenodo.3360857} and its accompanying GitHub repository \url{https://github.com/lulelita/HaitiCholeraMultiModelingProject}.
In order to perform our analysis in a unified framework, we re-implemented this model in the \code{R} programming language using the \code{spatPomp} package \citep{asfaw23arxiv}, which facilitates the creation of meta-population models.
We note that the travel and water matrices used to implement the complex dynamics in Model~2 \citep{lee20sup} are not available in either the Zenodo archive or the GitHub repository;
instead, we obtained these matrices via personal correspondence with the Model~2 authors.
Using these matrices, and the point estimates for model parameters provided by \citep{lee20sup}, we created trajectories of the cholera dynamics and compared this to available data.
These trajectories, shown in Figure~\ref{fig:mod2rep}, are very similar to the trajectories shown in Figure~S15 of \cite{lee20sup}.

<<Model2Replication, cache=TRUE, echo=FALSE>>=
h2_epi <- haiti2()
h2_end <- haiti2(region = 'after')

h2_epi_traj <- trajectory(h2_epi, format = 'data.frame')
h2_end_traj <- trajectory(h2_end, format = 'data.frame')

h2_epi_traj$Ctotal <- rowSums(h2_epi_traj[, paste0("C", 1:10)]) * 0.2
h2_end_traj$Ctotal <- rowSums(h2_end_traj[, paste0("C", 1:10)]) * 0.2

dep_plot_df <- haitipkg::haitiCholera %>%
  select(-report) %>%
  tidyr::pivot_longer(
    data = .,
    cols = -date_saturday,
    values_to = 'cases',
    names_to = 'dep'
  ) %>%
  mutate(
    date = as.Date(date_saturday),
    dep = gsub("\\.", "_", dep)
  ) %>%
  mutate(
    dep = case_when(dep == "Grand_Anse" ~ "Grande_Anse", TRUE ~ dep)
  )

true_agg_cases <- dep_plot_df %>%
  tidyr::pivot_wider(
    data = .,
    id_cols = c(date),
    names_from = dep,
    values_from = cases,
    names_prefix = 'cases_'
  ) %>%
  mutate(
    ReportedAll = cases_Artibonite + cases_Centre +
      cases_Grande_Anse + cases_Nippes + cases_Nord +
      cases_Nord_Est + cases_Ouest + cases_Sud +
      cases_Sud_Est + cases_Nord_Ouest
  ) %>%
  select(date, ReportedAll)

h2_epi_traj$date <- as.Date(
  lubridate::round_date(
    lubridate::date_decimal(h2_epi_traj$year),
    unit = 'day'
  )
)

h2_end_traj$date <- as.Date(
  lubridate::round_date(
    lubridate::date_decimal(h2_end_traj$year),
    unit = 'day'
  )
)

all_traj <- bind_rows(
  select(h2_epi_traj, date, Ctotal),
  select(h2_end_traj, date, Ctotal)
)

projections <- left_join(
  x = true_agg_cases,
  y = all_traj,
  by = 'date'
)
@


\begin{figure}[!h]
<<plotModel2Rep, cache=TRUE, echo=FALSE, fig.width=6.5, fig.height=4.5>>=
gg_epi <- projections %>%
  filter(date < as.Date("03-01-2014", format = '%m-%d-%Y')) %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = ReportedAll), col = 'black', linetype = 'dashed') +
  geom_line(aes(y = Ctotal), col = 'red', size = 1) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 30000), breaks = seq(0, 30000, 5000)) +
  theme(axis.title.x = element_blank()) +
  ylab("Reported number of cases")

gg_end <- projections %>%
  filter(date >= as.Date("03-01-2014", format = '%m-%d-%Y')) %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = ReportedAll), col = 'black', linetype = 'dashed') +
  geom_line(aes(y = Ctotal), col = 'red', size = 1) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 7000), breaks = seq(0, 7000, 1000)) +
  theme(axis.title.x = element_blank()) +
  ylab("Reported number of cases") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")

cowplot::plot_grid(gg_epi, gg_end, ncol = 1)
@
\caption{\label{fig:mod2rep}
Model 2 trajectories using the \code{haitipkg}. Compare to Figure S15 of \cite{lee20sup}.
}
\end{figure}

There are minor differences between Figure~\ref{fig:mod2rep} and Figure~S15 of \cite{lee20sup}.
While the discrepancy appears minor, the deterministic nature of Model~2 implies that an exact replication of model trajectories should be possible.
In this case, these discrepancies may possibly be attributed to implementing the model and plotting the model trajectory in two different programming languages.
Another potential explanation for the discrepancy is that the parameters that we used are only approximately the same as those used by \citet{lee20sup}.
For example, the parameters $\transmission$, $\Wbeta{}$ (See Table~\ref{tab:translate}) had reported values of $9.9 \times 10^{-7}$ and $4.03 \times 10^{-2}$, respectively (Table~S13 of \cite{lee20sup}), but were actually fit to data and therefore likely these values have been rounded.
Additionally, our implementation of Model~2 used a time scale of years and many of the parameters were reported on a weekly scale, so small differences may result due to unit conversions.
The collective effect of these small differences in model parameters likely will result in small differences in model trajectories.

Some additional concerns about being able to accurately replicate the results of \cite{lee20} are valid.
Details about the measurement models and how latent states were initialized for the epidemic model were not provided by \citet{lee20sup} and therefore these details must be inferred by looking at the provided source code.
According to repository comments, the files \code{fit\allowbreak In\allowbreak Pieces\allowbreak 3params\allowbreak Clean\allowbreak May2019\allowbreak Public.py} and \code{fit\allowbreak In\allowbreak Pieces\allowbreak Mu\allowbreak With\allowbreak Frac\allowbreak Sus\allowbreak Fixed\allowbreak All\allowbreak Infections\allowbreak Public.py} were used to fit the epidemic and endemic phases of the model respectively, although it is apparent that these exact files were not used to obtain the reported results since the files contain some variable-naming errors that make it impossible to run the files without making modifications \footnote{One example of why the code cannot be run that the file loads functions from a non-extant file named \code{choleraEqs.py} in line 13 rather than \code{cholera\allowbreak Eqs\allowbreak Public.py}.}.
The inability to replicate the results by \citet{lee20} by running the provided source code makes checking whether or not a our numeric implementation faithfully represents their results very difficult.
Additionally, the script that was said to been used to obtain the results reported by \cite{lee20} appears to use a different measurement model than what was described in the supplemental material, again making it difficult to fully replicate the result of \cite{lee20} without being able to easily run the provided source code.
In this case, we chose to use measurement model described by Eq.~\ref{model2:measEpi} for both phases of the epidemic, as this seemed to visually match the results of \cite{lee20} most closely.

\subsection{Model~3 Replication}

Model~3 was developed by a team of researchers at the Laboratory of the Swiss Federal Institute of Technology in Lausanne, hereafter referred to as the Model~3 authors.
The code that was originally used to implement Model~3 is archived with the DOI: \url{10.5281/zenodo.3360723}, and also available in the public GitHub repository: \code{jcblemai/haiti-mass-ocv-campaign}.
Because the code was made publicly available, and final model parameters were reported in the supplementary material of \cite{lee20}, we were able to reproduce Model~3 by directly using the source code.
In Fig.~\ref{fig:mod3rep}, we plot simulations from this model.
This figure can be compared to Figure S18 of \cite{lee20}.
We note that slight differences may be accounted for due to variance in the model simulations and the difference in programming language used to produce the figure.
Overall, the high standard of reproducibility that was achieved by the Model~3 authors facilitated the ability to readily replicate their model and results.

<<Model3Replication, echo=FALSE, cache=TRUE>>=
NSIM <- 500

plot_order <- c(
  'Artibonite',
  'Sud_Est',
  'Nippes',
  'Nord_Est',
  'Ouest',
  'Centre',
  'Nord',
  'Sud',
  'Nord_Ouest',
  'Grande_Anse'
)

dep_labeller <- as_labeller(
  c(
    'Artibonite' = 'Artibonite',
    'Sud_Est' = 'Sud-Est',
    'Sud.Est' = 'Sud-Est',
    'Nippes' = 'Nippes',
    'Nord_Est' = 'Nord-Est',
    'Nord.Est' = 'Nord-Est',
    'Ouest' = 'Ouest',
    'Centre' = 'Centre',
    'Nord' = 'Nord',
    'Sud' = 'Sud',
    'Nord_Ouest' = 'Nord-Ouest',
    'Nord.Ouest' = 'Nord-Ouest',
    'Grande_Anse' = 'Grand\'Anse',
    'Grand.Anse' = 'Grand\'Anse'
  )
)

lee3 <- lee3_spatPomp()
lee3_sims_quants <- bake("../model3/si/lee3Sims.rds", {

  # Simulate from lee et al 2020 model
  sims_temp <- simulate(
    lee3, nsim = NSIM,
    seed = 321, format = 'data.frame'
  )

  # Get desired quantiles of reported cases from simulations
  quants <- sims_temp %>%
    dplyr::group_by(unitname, time) %>%
    dplyr::summarise(
      q05 = quantile(cases, 0.025, na.rm = T),
      mean = mean(cases, na.rm = T),
      q50 = quantile(cases, 0.5, na.rm = T),
      q95 = quantile(cases, 0.975, na.rm = T)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::mutate(
      date = lubridate::date_decimal(time),
      date = as.Date(lubridate::round_date(date))
    ) %>%
    dplyr::rename(dep = unitname)

  # Remove cases from Ouest that were not considered by Lee et al. (2020)
  quants %>%
    dplyr::filter(
      !(dep == "Ouest" & date < '2017-06-10')
    )

})

# Get the subset of cholera cases that match simulations from Lee et al. (2020) model.
sub_cases <- haitiCholera %>%
  dplyr::mutate(date = as.Date(date_saturday)) %>%
  dplyr::select(-report, -date_saturday) %>%
  dplyr::filter(lubridate::decimal_date(date) >= min(lee3@times)) %>%
  tidyr::pivot_longer(
    data = .,
    cols = -date,
    names_to = "dep",
    values_to = "cases"
  ) %>%
  dplyr::mutate(
    dep = dplyr::case_when(
      dep == "Grand.Anse" ~ "Grande_Anse",
      dep == "Nord.Est" ~ "Nord_Est",
      dep == "Nord.Ouest" ~ "Nord_Ouest",
      dep == "Sud.Est" ~ "Sud_Est",
      TRUE ~ dep
    )
  )
@

\begin{figure}[!h]
<<PlotMod3Rep, echo=FALSE, message=FALSE, fig.height=3.75>>=
ggplot() +
  # Region for upper and lower simulation bounds
  geom_ribbon(
    data = lee3_sims_quants,
    aes(x = date, ymin = q05, ymax = q95),
    fill = 'darkblue', alpha = 0.5
  ) +
  # Median line
  geom_line(
    data = lee3_sims_quants,
    aes(x = date, y = q50),
    col = 'darkblue'
  ) +
  # Observed data
  geom_point(
    data = dplyr::filter(sub_cases, cases < 500),
    aes(x = date, y = cases), size = 0.2
  ) +
  facet_wrap(
    ~factor(dep, levels = plot_order),
    nrow = 2, labeller = dep_labeller
  ) +
  labs(y = 'Reported Cholera Cases') +
  scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
  theme(
    axis.text.x = element_text(angle = 40, hjust = 1, vjust = 1, size = 7),
    axis.title.x = element_blank()
  )
@
\caption{\label{fig:mod3rep}
Simulations from Model~3.
Compare to Figure S18 of \cite{lee20}.
}
\end{figure}

<<lee3Cleanup, echo=FALSE, include=FALSE, message=FALSE>>=
rm(lee3, lee3_sims_quants, sub_cases)
gc()
@

\section{Forecasting with parameter uncertainty}

Let $f_{Y_{1:N}}(y_{1:N} | \theta)$ denote the pdf of the model under consideration, were $\theta$ is a parameter vector that indexes the model.
Furthermore, denote the observed data as $y_{1:N}^*$.
Because the uncertainty in just a single parameter can lead to drastically different forecasts \citep{saltelli20},
parameter uncertainty should be considered when obtaining model forecasts when the goal is to influencing policy.
In a Bayesian modelling paradigm, the most natural way to account for parameter uncertainty in model forecasts is to suppose that $\theta$ comes from a distribution $f_{\Theta}$, and then to obtain $J$ forecasts from the model where each forecast is obtained using parameters drawn from the posterior distribution $\Theta_{1:J} \mid Y_{1:N} = y_{1:N}^* \sim f_{\Theta}\big(\theta | Y_{1:N} = y_{1:N}^*\big)$.

When frequentist methods are used, however, there does not exist a posterior distribution from which one could sample.
A common approach could be to obtain a weighted average of the simulations from various models \citep{hoeting99}, but this can be problematic when forecasts from each model are very different from each other \citep{grueber11}.
A similar approach that has been taken \citep{king15} is to obtain model forecasts using multiple sets of parameter values and then sample from the resulting forecasts using weights proportional to the corresponding likelihoods of the parameter values.
This approach could be considered as empirical Bayes, as it is equivalent to using a discrete uniform prior where the set of values in the prior distribution is determined by a stochastic routine applied to the observed data, as discussed below.

For each $i \in 1:k$, let $\Theta_i$ be a random vector of model parameters.
Then, letting $\Theta$ denote the true model parameters, we endow the set $\{\Theta_1, \Theta_2, \ldots, \Theta_k\}$ with a discrete uniform distribution, such that $P\big(\Theta = \Theta_i\big) = \frac{1}{K}$ for all values $i \in \seq{1}{k}$.
Using this as a prior distribution, the posterior distribution of $\Theta | Y_{1:N} = y_{1:N}^*$ can be expressed as:
$P\big(\Theta = \Theta_k | Y_{1:N} = y_{1:N}^*\big) = \frac{f_{Y_{1:N}}(y_{1:N}^*| \Theta_k)}{\sum_{l = 1}^K f_{Y_{1:N}}(y_{1:N}^*| \theta_l)}$.
In a standard empirical Bayes analysis, the values $\Theta_1, \ldots, \Theta_k$ of the prior distribution would be chosen using the observed data, resulting in a posterior distribution that weighs the prior parameter vectors proportional to their corresponding likelihoods.
Instead, we choose $\Theta_i$ to be the output of a stochastic routine applied to the observed data by setting $\Theta_i$ to be the output of an iterated filtering algorithm.
In practice, because the likelihood maximization routines of iterated filtering methods are stochastic, it is common to run the iterated filtering method multiple times for each model in order to obtain a maximum likelihood estimate for model parameters.
This results in a natural set of parameters near the MLE that could be used as the discrete prior distribution.

\section{Translating to \citet{lee20} notation}

Since the models of \citet{lee20} were developed independently, the choice of notation varies inconsistently between models.
For our reanalysis, we rename parameters to provide a unified notation facilitating comparison between models.
Table~\ref{tab:translate} maps this notation back to the original notations, for reference.

\begin{table}
  \begin{center}
  \begin{tabular}{|c|c|c|c|c|}\hline
    \multirow{2}{*}{Parameter} & Our & \multicolumn{3}{c|}{Lee et al. (2020a)} \\\cline{3-5}
     & Notation & 1 & 2 & 3 \\
    \hline
    \hline
    % Table begins here
    Reporting Rate & $\reportRate$ & $\rho$ & $\rho$ & $\epsilon_1, \epsilon_2$ \\\hline
    Mixing Coefficient & $\mixExponent$ & $\nu$ & --- & --- \\\hline
    Measurement Over-Dispersion & $\obsOverdispersion$ & $\tau$ & --- & $p$ \\\hline
    Birth Rate & $\muBirth$ & $\mu$ & --- & --- \\\hline
    Natural Mortality Rate & $\muDeath$ & $\delta$ & --- & $\mu$ \\\hline
    Cholera Mortality Rate & $\choleraDeath$ & --- & --- & $\alpha$ \\\hline
    Latent Period & $1/\muEI$ & $1/\sigma$ & $1/\gamma_E$ & --- \\\hline
    Recovery Rate & $\muIR$ & $\gamma$ & $\gamma$ & $\gamma$ \\\hline
    Loss of Immunity & $\muRS$ & $\alpha$ & $\sigma$ & $\rho$ \\\hline
    Symptomatic Ratio & $\symptomFrac$ & $1 - \theta_0$ & $k$ & $\sigma$ \\\hline
    Asymptomatic Relative Infectiousness & $\asymptomRelativeInfect$ & $\kappa$ & $red_\beta$ & --- \\\hline
    Human-to-Water Shedding & $\Wshed$ & --- & $\mu$ & $\theta_I$ \\\hline
    Asymptomatic Relative Shedding & $\asymptomRelativeShed$ & --- & $red_\mu$ & $\theta_A/\theta_I$ \\\hline
    Seasonal Amplitude & $\seasAmplitude$ & --- & $\alpha_s$ & $\lambda$ \\\hline
    Transmission & $\transmission$ & $\beta$ & $\beta$ & $c$ \\\hline
    Water-to-Human & $\Wbeta{}$ & --- & $\beta_W$ & $\beta$ \\\hline
    Bacteria Mortality & $\Wremoval$ & --- & $\delta$ & $\mu_\beta$ \\\hline
    Vaccination Efficacy & $\vaccineEfficacy$ & $\theta_{vk}$ & $\theta_1, \theta_2, \theta_{1_5}, \theta_{2_5}$ & $\eta_{1d}, \eta_{2d}$ \\\hline
    Process Over-dispersion & $\sigmaProc$ & --- & --- & $\sigma_w$\\\hline
    % Half Saturation Constant & $\Wsat$ & --- & $Sat$ & $K$ \\\hline
  \end{tabular}
  \end{center}
  \caption{
  \label{tab:translate}Translations between our common notation and notation used by \cite{lee20}
  }
\end{table}

\clearpage

\bibliography{../bib-haiti}

\end{document}


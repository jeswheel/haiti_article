\documentclass[11pt]{article}

\input{inputs/header}

\setcounter{tocdepth}{1}

<<packages, include=FALSE>>=
library(tidyverse)
library(pomp)
library(haitipkg)
library(spatPomp)
library(doParallel)
library(doRNG)
library(knitr)

options(
  scipen = 2,
  help_type = "html",
  stringsAsFactors = FALSE,
  continue = "+  ",
  width = 70,
  useFancyQuotes = FALSE,
  reindent.spaces = 2,
  xtable.comment = FALSE
)

opts_chunk$set(
  tidy.opts=list(
    keep.blank.line=FALSE
  ),
  comment="",
  fig.path = 'figure/',
  # warning=FALSE,
  # message=FALSE,
  echo=FALSE,
  fig.align="center",
  dev="pdf",
  dev.args=list(
    bg="transparent",
    pointsize=9
  )
)

cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset = NA))
if(is.na(cores)) cores <- detectCores()
# cores <- 20
registerDoParallel(cores)
theme_set(theme_bw())

if (!dir.exists("../model3/si")) {
  dir.create("../model3/si", recursive = TRUE)
}
@

\begin{document}

%%%%%%%%%% START

\section{Calibrating Model~3 to observed cases}\label{sec:mod3Cal}

<<CalibrateOrig3>>=
# Create vectors for the unit and shared parameters
unit_specific_names <- c("betaB", "foi_add")
shared_param_names <- c(
  "mu_B", "XthetaA", "thetaI", "lambdaR", "r", "std_W",
  "epsilon", "k"
)
est_param_names <- c(
  unit_specific_names, shared_param_names
)

# Add unit numbers to each parameter
est_param_names_expanded <- paste0(rep(est_param_names, each = 10), 1:10)

# Simple function that is used to set the rw_sd for the first search
# This function is convenient so that we don't have to type out the rw_sd
# for each unit parameter.
set_rw_1 <- function(x) {
  if (gsub("[[:digit:]]+$", "", x) %in% shared_param_names) {
    return(0.01)
  # } else if (x %in% paste0(rep(c("aHur", 'hHur'), each = 2), c(3, 9))) {
  #   return(expression(ifelse(time >= 2016.754 & time <= 2017, 0.015, 0)))
  # } else if (x %in% c("Iinit3", "Iinit4")) {
  #   return(expression(ivp(0.25)))
  } else if (grepl('^foi_add[[:digit:]]+$', x)) {
    return(0.015)
  } else if (grepl('^betaB[[:digit:]]+$', x)) {
    return(0.01)
  } else {
    return(0)
  }
}

set_rw_2 <- function(x) {
  if (gsub("[[:digit:]]+$", "", x) %in% shared_param_names) {
    return(0.003)
  # } else if (x %in% paste0(rep(c("aHur", 'hHur'), each = 2), c(3, 9))) {
  #   return(expression(ifelse(time >= 2016.754 & time <= 2017, 0.005, 0)))
  # } else if (x %in% c("Iinit3", "Iinit4")) {
  #   return(expression(ivp(0.125)))
  } else if (grepl('^foi_add[[:digit:]]+$', x)) {
    return(0.005)
  } else if (grepl('^betaB[[:digit:]]+$', x)) {
    return(0.003)
  } else {
    return(0)
  }
}

# First Search RW
reg_rw_1.sd <- lapply(est_param_names_expanded, set_rw_1)
names(reg_rw_1.sd) <- est_param_names_expanded
chol_rw_1.sd <- do.call(rw_sd, reg_rw_1.sd)

# Second Search RW
reg_rw_2.sd <- lapply(est_param_names_expanded, set_rw_2)
names(reg_rw_2.sd) <- est_param_names_expanded
chol_rw_2.sd <- do.call(rw_sd, reg_rw_2.sd)

SEARCH1 = list(
  NBPF = 100,
  NP = 2500,
  SPAT_REGRESSION = 0.05,
  NREPS = 144,
  NP_EVAL = 2500,
  NREPS_EVAL = 18,
  RW_SD = chol_rw_1.sd,
  COOLING = 0.5,
  KEEP_TRACES = FALSE,
  KEEP_LIKE_MAT = FALSE
)

SEARCH2 = list(
  TOP_N = 12,
  NBPF = 100,
  NP = 2500,
  SPAT_REGRESSION = 0.05,
  NREPS = 6,
  NP_EVAL = 2500,
  NREPS_EVAL = 18,
  RW_SD = chol_rw_2.sd,
  COOLING = 0.5,
  KEEP_TRACES = FALSE,
  KEEP_LIK_MAT = TRUE
)

haiti3_sub_fit <- bake(
  file = "../model3/si/subset_noparams.rds", {
    fit_haiti3(
      nsearches = 2L,
      search1 = SEARCH1,
      search2 = SEARCH2,
      search_rho = FALSE,
      search_gamma = FALSE,
      search_Iinit = FALSE,
      search_hur = FALSE,
      ncores = cores
    )
  }
)

# Find model index with best log-likelihood
best_m <- which.max(haiti3_sub_fit$search2$logLiks$logLik)

# Find parameters using best_m index
sub_params <- haiti3_sub_fit$search2$params[best_m, ]

# Get unit log-likelihoods for the best model.
all_liks <- as.data.frame(haiti3_sub_fit$search2$likMat)
# all_liks$which <- rep(1:(SEARCH3$TOP_N * SEARCH3$NREPS), each = SEARCH3$NREPS_EVAL)
all_liks$which <- rep(1:(12 * 6), each = 18)
best_mod <- filter(all_liks, which == best_m)

# Pivot longer to get estimate of log-likelihood for each unit.
unitLikesM3 <- best_mod %>%
  pivot_longer(
    cols = -which,
    names_to = "unit",
    names_prefix = "V",
    values_to = "logLik"
  ) %>%
  select(-which) %>%
  group_by(unit) %>%
  summarize(
    ll_est = logmeanexp(logLik),
    ll_se = logmeanexp(logLik, se = TRUE)[2]
  ) %>%
  ungroup() %>%
  mutate(unit = as.numeric(unit)) %>%
  arrange(unit)
@

<<M3_bench, echo=FALSE, message=FALSE, include=FALSE, cache=TRUE>>=
h3 <- haiti3_spatPomp()
deps <- h3@unit_names

mod3_cases <- as.data.frame(t(h3@data))
colnames(mod3_cases) <- deps
m3_inits <- c(7362, 1914, 22, 8, 2818, 82, 2499, 6953, 44, 7)  # fit by hand
names(m3_inits) <- deps

bench_cond_lls <- list()
for (dep in deps) {

  # Get department cases
  cases <- as.numeric(mod3_cases[, dep])
  mod <- ar1_NegBinom(data = cases, init = m3_inits[dep])

  bench_cond_lls[[dep]] <- haitipkg:::.ar1_NegBinom_cond_ll(
    theta = mod$theta, init = m3_inits[dep], data = cases, transform = FALSE
  )
}

bench_cond_lls <- as.data.frame(bench_cond_lls)

bench_lls <- apply(bench_cond_lls, 2, sum)

unitLikes_bench <- data.frame(
  unit = 1:10,
  # dep = names(bench_lls),
  ll = bench_lls
) %>%
  as_tibble()

bench_cond_lls$date <- lubridate::round_date(lubridate::date_decimal(h3@times), unit = 'day')
@

<<combine_bench_M3, echo=FALSE>>=

# Combine tables with model 3 and benchmark likelihoods.
bench_M3_joined <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  select(-ll_se) %>%
  mutate(unit = as.character(unit)) %>%
  pivot_longer(
    cols = -unit,
    names_to = "type",
    values_to = "ll"
  ) %>%
  mutate(unit = paste0(unit, ": ", deps[as.numeric(unit)]))
@

<<M3bpf, echo=FALSE, message=FALSE, include=FALSE>>=

# Change model parameters to the best set of parameters from previous
# search
coef(h3)[names(sub_params)] <- sub_params

registerDoRNG(40765101)  # for modular, reproducible results

# Perform block partical filter with fitted model, to get conditional
# log-likelihoods.
bpf_sub3_out <- bake(
  file = "../model3/si/bpf_sub_cond_LL.rds", {

    results <- list()

    bpf_out <- h3 %>% bpfilter(
      Np = 5000,
      block_size = 1,
      save_states = TRUE
    )

    results$block.cond.loglik <- bpf_out@block.cond.loglik
    results$times <- bpf_out@times  # Saved for convenience
    results
  },
  seed = 432235
)
@

In this section, we provide more detail on the process that was used to estimate the coefficients of Model~3.
In particular, we discuss why we decided to include additional model parameters---those that are associated with the behavior of the system during Hurricane Matthew---that were not considered by \cite{lee20}.
To calibrate this model, we used the iterated block particle filter (IBPF) method of \cite{ionides22}.
Due to the novelty of this algorithm, there does not exist published examples the IBPF algorithm used for data analysis outside of the papers in which the algorithm was introduced \citep{ning21ibpf,ionides22}, which is one motivation of the inclusion additional details related to fitting and diagnosing the model fit provided here.

\cite{lee20} were only able to estimate model parameters to a simplified version of Model~3 on a subset of the available data, as no method existed at the time of their publication to fit a fully coupled meta-population model to disease incidence data.
Building on their results, we fit the fully coupled version of Model~3 to nearly all available data, reserving only a few observations to use to calibrate the initial conditions of the model (see the previous section for more details).
In order to properly maximize the model likelihood, we found that it was necessary to use multiple searches for the MLE, periodically pruning away less successful searches.
The first collection of searches was performed by obtaining initial values for the parameters by uniformly sampling values from a predefined hypercube.
A subsequent refinement search used parameter values corresponding to the largest model likelihoods as starting parameter values.
The need for multiple searches does not appear to be uncommon, as a similar approach was used by \cite{ionides22}.

We use the technique described above to fit the fully coupled version of Model~3 proposed by \cite{lee20}.
The maximum likelihood we obtained after two rounds of searching was $\Sexpr{round(max(haiti3_sub_fit$search2$logLiks$logLik))}$, which is higher than the benchmark model ($\Sexpr{round(sum(unitLikes_bench$ll))}$).
While beating a simple associative benchmark is promising, this does not immediately rule out the possibility that the model is a poor description of the system.
Additional investigation of parameters estimates and their corresponding implications on model based conclusions should always be conducted.
For meta-population models, it is worth considering how well the model fits the data to each spatial unit.
% This can be done by looking at conditional log-likelihoods, which is part of the output of the \texttt{bpfilter} algorithm in the \texttt{spatPomp} package.
The likelihoods for each department, compared to the corresponding benchmark model, are displayed in Fig.~\ref{fig:h3UnitLikes}.
The figure demonstrates that while the log-likelihood of the fitted model outperforms the auto-regressive negative binomial benchmark model, Model~3 has lower likelihoods than the benchmark for some departments.

\begin{figure}[!ht]
<<h3UnitLikes, fig.height=3, fig.width=5.5, echo=FALSE>>=
ggplot(bench_M3_joined, aes(x = ll, y = reorder(unit, ll))) +
  geom_point(aes(col = type)) +
  ylab("Unit") +
  xlab("Log Likelihood") +
  scale_color_manual(
    labels = c("ll" = "Benchmark", "ll_est" = "Model 3"),
    values = c("ll" = "#1b9e77", "ll_est" = "#d95f02")
  ) +
  theme(legend.title = element_blank(),
        axis.title.y = element_blank())
@
\caption{\label{fig:h3UnitLikes}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model prior to the inclusion of parameters that account for Hurricane Matthew.}
\end{figure}

In addition to considering the conditional log-likelihoods of each unit, one can consider conditional log-likelihoods of each observation.
When compared to a benchmark, this level of detail can provide useful information about which observations are well described by the model and which are not.
In Fig.~\ref{fig:condLL}, we plot the conditional log-likelihoods of Model~3 for each observation.
Typically it is most useful to compare the conditional log-likelihoods of the model under consideration to a benchmark, as plotting only conditional log-likelihoods without a comparison may not be helpful.
In this case, however, the same insight can be drawn using a figure without a benchmark comparison, so we do not include the benchmark in order to avoid the issue of over-plotting.

\begin{figure}[!ht]
<<CondLLFig, echo=FALSE, message=FALSE, fig.height=3>>=

# Changing labels to human-readable labels.
deps[grepl("_", deps)] <- gsub("_", "-", deps[grepl("_", deps)])
deps[deps == "Grande-Anse"] <- "Grand\'Anse"
names(deps) <- as.character(1:10)
myLabeller <- as_labeller(deps)

# Plot conditional log-likelihoods for each unit
t(bpf_sub3_out$block.cond.loglik) %>%
  as.data.frame() %>%
  mutate(t = bpf_sub3_out$times) %>%
  pivot_longer(
    cols = -t,
    values_to = 'likelihood',
    names_prefix = "V",
    names_to = 'Unit'
  ) %>%
  mutate(date = as.Date(lubridate::date_decimal(t))) %>%
  ggplot(aes(x = date, y = likelihood)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Unit, labeller = myLabeller, scales = 'free_y',
             nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Conditional log-likelihoods")
@
\caption{\label{fig:condLL}Conditional log-likelihoods of Model~3 prior to the inclusion of the Hurricane Matthew related parameters.}
\end{figure}

Fig.~\ref{fig:condLL} reveals that the fitted model poorly describes certain features of the data.
For example, many departments (in particular Sud) have observations with lower conditional log-likelihoods near October 2016 than at other time points.
Further investigation of the model output reveals that the model is struggling to explain the sudden surge in cholera cases that occurred at this time, which coincides with the time that Hurricane Matthew struck Haiti.
While the model does include a mechanism to account for increased cholera transmission due to large rainfall events, the mechanism does not appear to be sufficient to capture the damaging effects of the hurricane, which had the greatest impact in the the Sud and Grand'Anse departments \citep{ferreirai16}.
This result led us to include parameters $\Whur{u}$ and $\hHur{u}$ in Eq.~23 of the main text, which allows for an increase in the transmission rate between environmental cholera and humans for in Sud and Grand'Anse during and after the hurricane.
The effect of the hurricane on cholera transmission is assumed to have an exponential decay, where the magnitude is determined by $\Whur{u}$ and the duration of the effect determined by $\hHur{u}$.

<<loadFinalModel3, echo=FALSE, include=FALSE, message=FALSE>>=

# This result was used in the manuscript, so it doesn't need to be recalculated.
h3_final_results <- readRDS("../model3/run_level_3/haiti3_fit.rds")

# Save best_m as index for the model parameters with the best likelihood.
# Then, save the model parameter values and the likelihood evaluations
best_m <- which.max(h3_final_results$search2$logLiks$logLik)
best_params <- h3_final_results$search2$params[best_m, ]
all_liks <- as.data.frame(h3_final_results$search2$likMat)
all_liks$which <- rep(1:72, each = 18)
best_mod <- dplyr::filter(all_liks, which == best_m)

# Get repeated estimates of unit log-likelihoods
unitLikesM3 <- best_mod %>%
  pivot_longer(
    cols = -which,
    names_to = "unit",
    names_prefix = "V",
    values_to = "logLik"
  ) %>%
  select(-which) %>%
  group_by(unit) %>%
  summarize(
    ll_est = pomp::logmeanexp(logLik),
    ll_se = pomp::logmeanexp(logLik, se = TRUE)[2]
  ) %>%
  ungroup() %>%
  mutate(unit = as.numeric(unit)) %>%
  arrange(unit)

# Combine model likelihood table to benchmarks
bench_M3_joined <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  select(-ll_se) %>%
  mutate(unit = as.character(unit)) %>%
  pivot_longer(
    cols = -unit,
    names_to = "type",
    values_to = "ll"
  ) %>%
  mutate(unit = paste0(unit, ": ", deps[as.numeric(unit)]))

bench_diffs <- dplyr::left_join(
  x = unitLikesM3,
  y = unitLikes_bench,
  by = "unit"
) %>%
  rename(ll_mod3 = ll_est, ll_bench = ll) %>%
  mutate(unit = unlist(myLabeller(unit)),
         ll_diff = ll_mod3 - ll_bench) %>%
  arrange(ll_diff)

floor(100 * sum(mod3_cases[, c(bench_diffs$unit[1:4])], na.rm = TRUE) / sum(mod3_cases, na.rm = TRUE))
@

We refit Model~3 after introducing these hurricane-related parameters using the same successive search scheme described above.
The resulting model has a log-likelihood value of $\Sexpr{round(max(h3_final_results$search2$logLiks$logLik), 1)}$.
The addition of the Hurricane parameters seems to have resulted in a dramatic increase in conditional likelihoods around and after October 2016, as seen in Fig.~\ref{fig:finalCondLL}.
The inclusion of these parameters resulted in an overall increase of $\Sexpr{round(max(h3_final_results$search2$logLiks$logLik) - max(haiti3_sub_fit$search2$logLiks$logLik), 1)}$ log-likelihood units, which is a highly statistically significant difference.

<<finalModel3BPF, echo=FALSE, message=FALSE, include=FALSE>>=

# Set the model parameters to the best results from previous search
coef(h3)[names(best_params)] <- best_params

# block particle filter to get conditional log-likelihoods.
bpf_final3_out <- bake(
  file = "../model3/si/h3_final_bpf_cond_ll.rds", {

    results <- list()

    bpf_out <- h3 %>% bpfilter(
      Np = 5000,
      block_size = 1,
      save_states = TRUE
    )

    results$Sus_df <- foreach(i=1:length(h3@times), .combine = bind_rows) %do% {
      get_h3_S_quants(bpf_out, i)
    }

    results$block.cond.loglik <- bpf_out@block.cond.loglik
    results$times <- bpf_out@times  # saved for convenience
    results
  },
  seed = 786321
)
@

\begin{figure}[!ht]
<<finalCondLLFig, echo=FALSE, message=FALSE, fig.height=3>>=
t(bpf_final3_out$block.cond.loglik) %>%
  as.data.frame() %>%
  mutate(t = bpf_final3_out$times) %>%
  pivot_longer(
    cols = -t,
    values_to = 'likelihood',
    names_prefix = "V",
    names_to = 'Unit'
  ) %>%
  mutate(date = as.Date(lubridate::date_decimal(t))) %>%
  ggplot(aes(x = date, y = likelihood)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Unit, labeller = myLabeller, scales = 'free_y',
             nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Conditional log-likelihoods")
@
\caption{\label{fig:finalCondLL}Conditional log-likelihoods of Model~3 after adding and estimating the parameters related to Hurricane Matthew.}
\end{figure}

Now that the model with additional parameters has been calibrated to the incidence data, we plot the conditional log-likelihood of each department compared to a benchmark model in Fig.~\ref{fig:finalUnitLL}.
Interestingly, the difference in log-likelihoods between Model~3 and the benchmark model is smallest in the departments \Sexpr{bench_diffs$unit[1]}, \Sexpr{bench_diffs$unit[2]}, \Sexpr{bench_diffs$unit[3]} and \Sexpr{bench_diffs$unit[4]}.
Each of these departments also exhibited the most sustained cholera transmission, defined by having the fewest number of weeks with no recorded cholera cases.
Specifically, these four departments have zero cholera cases recorded in less than $4\%$ of the available data, and all remaining departments---except for Nord-Ouest, which has approximately $9.5\%$ of cases that are zeros and also exhibits the next smallest difference in log-likelihoods---have zero cases recorded in at least $14\%$ of the available weekly data.
This result suggests that the quantitative advantage Model~3 has over its respective benchmark is primarily due to the model's ability to describe a resurgence of cases after a department records a week of zero cholera cases.
This result may be unsurprising in the context of the models that we are comparing.
Because the cholera transmission in individual departments likely depends on the national prevalence of cholera and the Vibrio cholerae bacteria, our benchmark model that relies exclusively on the previous number of case within any given unit has a difficult time predicting a resurgence of cases.

\begin{figure}[!ht]
<<finalUnitLLFig, fig.height=3, fig.width=5.5, echo=FALSE>>=
ggplot(bench_M3_joined, aes(x = ll, y = reorder(unit, ll))) +
  geom_point(aes(col = type)) +
  ylab("Unit") +
  xlab("Log Likelihood") +
  scale_color_manual(
    labels = c("ll" = "Benchmark", "ll_est" = "Model 3"),
    values = c("ll" = "#1b9e77", "ll_est" = "#d95f02")
  ) +
  theme(legend.title = element_blank(),
        axis.title.y = element_blank())
@
\caption{\label{fig:finalUnitLL}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model after adding and estimating parameters related to Hurricane Matthew.}
\end{figure}

The difference in log-likelihoods between Model~3 and its benchmark model for each individual units suggests that Model~3 has a relatively poor fit for the four units with the most sustained cholera transmission.
The simple four parameter benchmark has a higher likelihood than Model~3 for the \Sexpr{bench_diffs$unit[1]} and \Sexpr{bench_diffs$unit[2]} departments, and also has log-likelihoods only a few units smaller than Model~3 for units \Sexpr{bench_diffs$unit[3]} and \Sexpr{bench_diffs$unit[4]}.
This is particularly worrisome given that these four departments account for more than $\Sexpr{floor(100 * sum(mod3_cases[, c(bench_diffs$unit[1:4])], na.rm = TRUE) / sum(mod3_cases, na.rm = TRUE))}\%$ of the total number of reported cholera cases.

\subsection{Examining the Hidden States of the Calibrated Model}

For mechanistic models, beating a suitable statistical benchmark does not alone guarantee that the model provides an accurate description of a dynamic process.
Indeed, a good statistical fit does not require the model to assert a causal explanation.
For example, reconstructed latent variables should make sense in the context of alternative measurements of these variables \citep{grad12}.
We demonstrate this principle by examining the latent state of the calibrated model.
In particular, we examine the compartment of susceptible individuals under various scenarios.

Recall that the filtering distribution for the calibrated version of Model~3 at time $t_k$ is defined as the distribution of the latent state at time $t_k$ given the data from times $t_{1}:t_{k}$, i.e. $f^{(3)}_{\bm{X}_k|\bm{Y}_{1:k}}(\bm{x}_{k} | \bm{y}^*_{1:k} ; \hat\theta)$.
In general, one may expect simulations from the filtering distribution of a model with a good statistical fit to result in hidden states that are highly consistent with the observed data because the filtering distribution is conditioned on the observed data.
Fig.~\ref{fig:h3Sus} shows the percentage of individuals that are in the susceptible compartment from various simulations of the model:
simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
This figure shows that simulations from initial conditions tends to result in a much more rapid depletion of susceptible individuals at the start of the epidemic than simulations from the filtering distribution, suggesting the calibrated model has a propensity to predict larger outbreaks than what is typically seen in the data.
This also appears true in later stages of the time period of observed data, where the data suggests that there is a replenishment of the susceptible compartment, but the Model simulations typically retain a smaller proportion of susceptible individuals.
This result demonstrates that the calibrated model favors a more rapid growth in cholera cases than what is typically seen in the observed data, which explains why the model fails to predict the absence of cases between February, 2019 and October, 2022.

<<h3SusFigSetup, echo=FALSE, message=FALSE, include=FALSE>>=

# This bake perfoms the following:
#  (1) simulates the model from initial conditions
#  (2) get the values of the hidden states at the end of each simulation, and
#      saves these for later use.
#  (3) Obtain quantiles for the Susceptible pool from the simulations.
init_conditions_results <- bake(
  file = "../model3/si/InitSimsQuants.rds", {

    results <- list()  # To return final output of bake

    # Simulate from the fitted model
    h3Spat_sims <- simulate(
      h3, nsim = 2000, format = 'data.frame'
    )

    # Get states at the end of the simulations
    h3_spat_end <- h3Spat_sims %>%
      filter(time == max(time))

    # Save hidden state values at the end of the simulation. Saved as a matrix
    # in the same format as the output from a block-particle filter, so that
    # it can be used as the input to project_from_filter.
    results[["end_conditions"]] <- h3_spat_end %>%
      select(-time, -cases) %>%
      mutate(
        unit = case_when(
          unitname == "Artibonite" ~ 1,
          unitname == "Centre" ~ 2,
          unitname == "Grande_Anse" ~ 3,
          unitname == "Nippes" ~ 4,
          unitname == "Nord" ~ 5,
          unitname == "Nord_Est" ~ 6,
          unitname == "Nord_Ouest" ~ 7,
          unitname == "Ouest" ~ 8,
          unitname == "Sud" ~ 9,
          unitname == "Sud_Est" ~ 10
        )
      ) %>%
      select(-unitname) %>%
      pivot_wider(
        data = .,
        names_from = unit,
        values_from = c(S, I, A, R_one, R_two, R_three, VSd, VR1d, VR2d,
                        VR3d, VSdd, VR1dd, VR2dd, VR3dd, VSd_alt, VR1d_alt,
                        VR2d_alt, VR3d_alt, VSdd_alt, VR1dd_alt, VR2dd_alt,
                        VR3dd_alt, C, B, Doses, totInc),
        names_sep = ""
      ) %>%
      select(-.id) %>%
      as.matrix() %>%
      t()

    # Get the quantiles for the Susceptible pool from the simulations
    results[["SQuants"]] <- h3Spat_sims %>%
      rename(dep = unitname) %>%
      group_by(dep, time) %>%
      summarise(
        Q025 = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.025),
        Q50  = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.5),
        Q975 = quantile(S + VSd + VSdd + VSd_alt + VSdd_alt, probs = 0.975)
      ) %>%
      ungroup() %>%
      rowwise() %>%
      mutate(
        unit_num = which(h3@unit_names == dep),
        date = lubridate::date_decimal(time),
        Q025 = Q025 / h3@params[paste0("H", unit_num)],
        Q50  = Q50  / h3@params[paste0("H", unit_num)],
        Q975 = Q975 / h3@params[paste0("H", unit_num)]
      ) %>%
      filter(
        date >= as.Date(
          lubridate::round_date(lubridate::date_decimal(h3@t0), unit = 'day')
        )
      ) %>%
      mutate(date = as.Date(lubridate::round_date(date, unit = 'day')))

    results
  },
  seed = 834687
)

# Get the output of the "bake" call for plotting.
h3Spat_quants <- init_conditions_results[["SQuants"]]

# This bake call uses the ending hidden states from above to forecast cholera
# cases in the future under the V0 vaccination scenario
mod3_V0_initcond_res <- bake(
  file = '../model3/si/vacc_sim_from_init.rds', {
    h3_V0_res <- list()

    # Get "noVacc" scenario parameters
    h3_S0_par <- get_model3_vacc_scenario_params(scenario = "noVacc")
    h3Spat <- h3
    p3 <- best_params
    coef(h3Spat) <- p3
    coef(h3Spat)[names(h3_S0_par)] <- h3_S0_par

    # store the end states that will be used as initial values
    h3_bpf_end_states <- init_conditions_results$end_conditions

    # Using parameters and end state values, obtain projections
    h3_V0_sims <- project_from_filter(
      h3Spat, end_states = h3_bpf_end_states, covarGen = project_rain,
      nsims = 1000, seed = 184697
    )

    # Store nationally aggregated results
    h3_V0_res$mod3_V0_sims <- agg_mod3_sims(h3_V0_sims)
    h3_V0_res$mod3_V0_probs <- get_elimProbs(h3_V0_sims, model = 3)

    # Get quantiles of observed cases for each department and time
    h3_V0_res$UnitQuants <- h3_V0_sims %>%
      mutate(
        date = as.Date(  # Get date object from decimal time
          lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
        )
      ) %>%
      group_by(unitname, date) %>%
      summarize(
        Q025 = quantile(cases, probs = 0.025),
        Q50 = quantile(cases, probs = 0.5),
        Q975 = quantile(cases, probs = 0.975)
      ) %>%
      ungroup() %>%
      rename(dep = unitname)

    # Get quantiles of susceptible individuals for each department and time
    h3_V0_res$SQuants <- h3_V0_sims %>%
      mutate(
        date = as.Date(
          lubridate::round_date(lubridate::date_decimal(time), unit = 'day')
        )
      ) %>%
      group_by(unitname, date) %>%
      mutate(
        # Get the population for each unit
        pop = h3@params[paste0("H", which(first(unitname) == unit_names(h3)))]
      ) %>%
      summarize(  # Calculate the quantiles
        Q025 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.025
        ),
        Q50 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.5
        ),
        Q975 = quantile(
          (S + VSd + VSdd + VSdd_alt + VSd_alt) / pop,
          probs = 0.975
        )
      ) %>%
      ungroup() %>%
      rename(dep = unitname)

    h3_V0_res
  }
)

# This result was already used in the manuscript; it doesn't need to
# be recalculated.
mod3_V0_res <- readRDS("../model3/run_level_3/mod3_V0_res.rds")
@

\begin{figure}[!ht]
<<h3SusFig, echo=FALSE, message=FALSE, fig.height=3>>=
ggplot() +
  # Sims from initial condition
  geom_line(data = h3Spat_quants, aes(x = date, y = Q50), col = 'red') +
  geom_ribbon(
    data = h3Spat_quants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'red', alpha = 0.5
  ) +
  # projections from initial condition, using filtered sims function
  geom_line(data = mod3_V0_initcond_res$SQuants, aes(x = date, y = Q50), col = 'red') +
  geom_ribbon(
    data = mod3_V0_initcond_res$SQuants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'red', alpha = 0.5
  ) +
  # Sims from filtered distribution
  geom_line(
    data = bpf_final3_out$Sus_df,
    aes(x = as.Date(lubridate::date_decimal(time)), y = Q50),
    col = 'blue'
  ) +
  geom_ribbon(
    data = bpf_final3_out$Sus_df,
    aes(x = as.Date(lubridate::date_decimal(time)), ymin = Q025, ymax = Q975),
    fill = 'blue', alpha = 0.5
  ) +
  # Projections from filtered distribution
  geom_line(data = mod3_V0_res$SQuants, aes(x = date, y = Q50), col = 'blue') +
  geom_ribbon(
    data = mod3_V0_res$SQuants,
    aes(x = date, ymin = Q025, ymax = Q975),
    fill = 'blue', alpha = 0.5
  ) +
  geom_vline(xintercept = max(h3Spat_quants$date), linetype = 'dashed') +
  facet_wrap(~dep, scales = 'free_y', nrow = 2) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -45, hjust = 0)) +
  ylab("Total Susceptible (%)")
@
\caption{\label{fig:h3Sus}Percentage of individuals that are in the susceptible compartment.
Simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
The dashed line represents the end of the observed data.}
\end{figure}

%%%%%%%%%% END

\bibliography{../bib-haiti}

\end{document}

